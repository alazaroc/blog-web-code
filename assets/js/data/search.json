[ { "title": "Challenge: 'AWS Certified Data Analytics Specialty' in 30 days", "url": "/posts/challenge-data-certification-in-30-days/", "categories": "General", "tags": "data, certification", "date": "2023-10-25 22:59:00 +0200", "snippet": "TLDR Updated: The AWS Certified Data Analytics Specialty is going to be retired on April 9, 2024. You still have time to do it! However, you have another option, AWS presented a new Data certific...", "content": "TLDR Updated: The AWS Certified Data Analytics Specialty is going to be retired on April 9, 2024. You still have time to do it! However, you have another option, AWS presented a new Data certification of the Associate level, AWS Certified Data Engineer, you have more information here.If you are here, it’s likely because you’re eager to obtain the certification, or perhaps you’re curious about the insights I’ve shared in this guide!In either case, rest assured that this guide is a firsthand account of my successful journey to achieve the AWS Certified Data Analytics Specialty certification. I conquered the exam on October 15th.If I could do it, you certainly can too.I didn’t have experience in data analytics (not enough) and neither specific knowledge, but I wanted to prepare the certification because I wanted to get more knowledge in this area. I am a Cloud &amp; Solutions Architect who wanted to know how to design, build, secure, and maintain analytics solutions using AWS tools and services. However, this certification is not easy and I had to dedicate a lot of hours to be prepared to pass the exam. I will use in this article [30 days] to prepare the certification’s journey. However, remember, everyone's learning pace is different. Adjust the suggested plan based on your comfort level and progress. You could need more or less than 30 days.IntroductionWelcome to the challenge of achieving the AWS Certified Data Analytics Specialty certification in just 30 days!In this article, we will embark on a journey together to prepare for and conquer this certification. Whether you are an AWS Solutions Architect or a data enthusiast looking to expand your knowledge and credentials, this challenge will provide you with the guidance and resources needed to succeed.Good luck with your certification journey!Challenge: ‘AWS Certified Data Analytics Specialty’ in 30 daysI wrote this other article about 10 steps to prepare any AWS Certification, and I will follow those 10 steps in this challenge. This is the summary: Find the proper certification for you Find your WHY: Motivation is the key Create a study plan Take the official AWS Exam Readiness course Choose a main training course Practice using the AWS console to gain hands-on experience Read the recommended whitepapers Read the recommended FAQs Take your own notes (and study them) Practice with exam-style questions (tests)Day 1: validate that the Data analytics certification is right for you and know what is your motivation (WHY)Validate that the certification is right for you (step 1) Read exam guideIn this challenge, our goal is to obtain the AWS Certified Data Analytics Specialty certification, so it is important to carefully read the exam guide and ensure that this certification aligns with your goals and aspirations: Data Analytics exam landing page Data Analytics Exam guide Decide if you want to achieve the certificationI am far from being the target candidate described in the exam guide: The target candidate should have 5 years of experience with common data analytics technologies. The target candidate also should have at least 2 years of hands-on experience and expertise working with AWS services to design, build, secure, and maintain analytics solutions.It means that I need to intensify my efforts and commence my journey with a solid grasp of the essentials.Motivation (step 2) WHY?Before starting, you must have a very good reason to do it, good enough to not quit in the middle. What are your reasons for getting this new AWS certification?In my case, I need the Data certification because I don’t have as much experience in data projects as I want, and I truly wanted to learn how to design, build, secure, and maintain analytics solutions using AWS tools and services so that I can collaborate in a helpful way on data projects. The second reason is because the data certification is the nearest one to an event driven solution, and I want to be involved in this solutions in the future. Your motivation is your WHY. How powerful to you is your WHY to you? If it is not good enough, probably you are going to give up…Day 2: Target date, buy the exam and create the study plan (step 3) Purchase the examMy target date was one month away, so I purchased the exam to do it even more real. I usually set a target date and buy the exam to motivate myself. Don’t worry, you can always reschedule the exam twice for free with at least a minimum of 24 hours prior notice. I had to reschedule exams several times, but it is a good idea have a date in mind from the beginning. I strongly recommend you request the time extension of 30 minutes if English is not your mother tongue. Create a study planLike I said, we will follow these 10 steps to prepare any AWS Certification.You have to allocate time to prepare the exam. When creating your study plan, it is important to be realistic about the amount of time you can commit to studying. Be sure to factor in other commitments, such as work, family, and social obligations.Probably your plan will change and you need to be flexible, but it is a good idea try to plan all the things you want to do to prepare the exam.These are all the tasks that I had to do to prepare the exam, with an estimated amount of time [updated: I had to spend more hours in practically all the tasks]: 8h: Learn the basics of data analytics 4h of online content (so I will spend at least 8 hours) Xh: Take a main training course. I will left here different options for you (there are many many more). 4h (my estimation) - Whitepapers: 5h (my estimation) - FAQs 20h (my estimation) - Practice with the main services (list above) 8h (my estimation) - Take my own notes and study it 8h (my estimation) - Practice exam questionsDay 3 - Starting with the basics Introduction to AWS Data AnalyticsIn my case, I had to start with the basics, so I decided to spend one full day and also writing this article about the Introduction to Data Analytics. Initially I have included all the information in the same article but it was too long so I decided separate, also because maybe you are not interested in the introduction to the Data Analytics!Days 4-6 - Exam readiness (step 4) Exam readinessTake the time to go through the Exam Readiness: AWS Certified Data Analytics Specialty course provided by AWS. This course is specifically designed to help you prepare for the exam and covers important topics, including domain areas, question formats, and exam strategies. AWS always show one message similar to this one: This is NOT training. This is to complement your training….These are the five domains for this certification. Four of them are the same that the Process of Data Analytics mentioned above + Security domain. Collection (18%) Storage and data management (22%) Processing (24%) Analysis and visualization (18%) Security (18%)Let me show the main AWS Services for each domain, and let me show you the more important ones.Domain 1: CollectionFocuses on ingesting raw data from transactions, logs, and Internet of Things (IoT) devices.Main AWS Services Amazon Kinesis Data Streams AWS Glue Amazon Kinesis Data Firehose AWS DMS Amazon SQS AWS IoT AWS Snowball Amazon MSKDomain 2: Storage and Data ManagementMain AWS Services: Amazon RDS Amazon Neptune Amazon DynamoDB Amazon Redshift Amazon ElastiCache Amazon S3 AWS Lake Formation Amazon AuroraDomain 3: ProcessingThe goal of the processing system is to transform data and make it more consumable by analytics and visualization toolsMain AWS Services: Amazon EMR Amazon Kinesis Data Analytics AWS Glue AWS Lambda AWS Step Functions AWS Data PipelineDomain 4: Analysis and VisualizationThe analysis and visualization domain is about using the data you’ve collected, processed and transformed to generate actionable insights. Amazon Athena Amazon Kinesis Video Streams Amazon OpenSearch Amazon EMR Amazon Redshift Amazon kinesis Data Analytics Amazon SageMaker Amazon QuickSightDomain 5: SecurityMain AWS Services: AWS IAM AWS KMS Check encryption of the following services: S3, Redshift, Kinesis, EMR and Glue Take my own notesI take notes of all the videos/material I think it is useful so after that I can check it again. Sometimes is just do an screenshot of something important and sometimes I take my own notes.Days 7-17 - Main training course &amp; Practice resources (step 5 &amp; 6)First of all, I think you have to check this AWS resource: Ramp-up data analytics. This is a recompilation of many other courses and AWS content related with Data Analytics. Main training courseYou have to select your own training course. Here, if you select one course that covers all the certification content you will save a lot of time searching for additional resources.Some examples: 17h of content: Udemy [not free course] 42h of content: Cloud Academy [not free course] AWS SkillBuilder courses [not free courses]In my case, I selected SkillBuilder resources because I wanted to test this platform. Also, I don’t wanted to watch training videos… I preferred choose all downloadable content and online content, and take my own notes to check them later. However, there is no one official preparation course in the SkillBuilder platform, so I had to select a few resources to try to cover all the content (and definitely I had to invest more time). Disclaimer: these are not free courses. If you are only looking for free content, you can search yourself a free main training course or you can omit this step. However, probably you have to compensate with MANY more hours of preparation to fill all the gaps you have and to cover all the certification contentThese are a few courses I did in the SkillBuilder platform: Data Analytics on AWS (Technical), 4h. Digital content (no videos) AWS Partner Certification Readiness: Data Analytics - Specialty (18h 27m). It includes pdf material that you can download, very useful! I didn’t check the videos… only the pdf content! Also, this is a readiness course… Data &amp; Analytics Tech Talk (Partner Learning Plan), 10h 32m. Again, it includes pdf content that you can download, very useful! It contains 8 different courses Transform your data approach: Develop a modern data strategy - Technical Better, faster, and lower-cost storage : Optimizing Amazon S3 &amp; FSx/EFS storage - Technical Analytics Readiness for BFSI - Technical Transactional data lakes on AWS - Technical Serverless Data Integration for a Modern Data Infrastructure with AWS Glue - Technical Analytics on SAP Data - Technical Redshift Migrations &amp; POCs - Technical Right Data Streaming Architecture for your Streaming Analytics AWS Cloud Quest Data Analytics. AWS Cloud Quest is the only role-playing game to help you build practical AWS Cloud skills. The challenges were easy and you have to complete many very un-useful challenges related with other services not related with the Data Analytics certification, but it was funny! I completed all the challenges and I achieved the AWS Cloud Quest Data Analytics badge PracticeFollow a training course is highly recommended but practice with the AWS console is even more useful to retain information. And much more if you don’t have much experience with some of the AWS services.I experiment by myself directly with some of the AWS services but I also get some internet examples.I used these Skill Builder labs to practice with the main AWS resources in a provided AWS account.Again, there is A LOT of free content you can check… Here are a few examples: Build and automate a serverless data lake using an AWS Glue trigger for the Data Catalog and ETL jobs Game Analytics Pipeline Serverless Analytics for Games Create business intelligence dashboards with Amazon QuickSight: https://aws.amazon.com/getting-started/hands-on/create-business-intelligence-dashboards-using-amazon-quicksight/?p=ft&amp;c=aa&amp;z=4){:target=”_blank”} Orchestrate Amazon EMR Serverless jobs with AWS Step functions Modernize a legacy real-time analytics application with Amazon Managed Service for Apache Flink Using AWS AppSync and AWS Lake Formation to access a secure data lake through a GraphQL API Build event-driven architectures with Amazon MSK and Amazon EventBridge Securely process near-real-time data from Amazon MSK Serverless using an AWS Glue streaming ETL job with IAM authentication Introducing Amazon MSK as a source for Amazon OpenSearch Ingestion Generate security insights from Amazon Security Lake data using Amazon OpenSearch Ingestion End-to-end development lifecycle for data engineers to build a data integration pipeline using AWS Glue Build and share a business capability model with Amazon QuickSightDays 18-19 - Whitepapers (step 7)The link of the official AWS Whitepapers is included in the documentation landing page: Amazon EMR Migration Guide: How to Move Apache Spark and Apache Hadoop From On-Premises to AWS Big Data Options on AWS Streaming Data Solutions on AWS with Amazon Kinesis Teaching Big Data Skills with Amazon EMR Reference Architecture: SQL Based Data Processing in Amazon ECSDays 20-21 - FAQs of the main services (step 8)The link of the official FAQs is included (again) in the same documentation landing page: Amazon Athena Amazon EMR Amazon Redshift Amazon CloudSearch Amazon Kinesis Amazon Kinesis Data Streams Amazon Kinesis Data Firehose Amazon Kinesis Data Analytics Amazon ElasticSearch Service Amazon Managed Service for Kafka (MSK) Amazon QuickSight AWS Data Exchange AWS Glue AWS Lake Formation AWS Data PipelineDays 22-24 - Study my notes (step 9)Unfortunately I cannot share my own notes, but I used this time to check everything again and again.Days 25-29 - Practice with exam-style questions (step 10)You have many options on internet.The 2 AWS official test resources are the following: 10 questions - AWS Certified Data Analytics Specialty Sample Questions 20 questions - AWS Certified Data Analytics Specialty Official Practice Question SetDay 30 - EXAMA few advices that you already know… so, reminders!BEFORE the exam: Sleep well the previous night to be rest in the exam. It will be a long exam with long questions…DURING the exam: Identify keywords in the questions Eliminate wrong answers If you have doubts, answer the question, write in the comments your possible answers and everything that will help you in your next revision (example: A or B doubt in xxxxx), flag it, and go to the next one Don’t spend to much time on each question. 3 minutes if you have requested the additional 30 minutes for non-native english speakers Review all the flagged questions and un-flag the ones that you are more confident now Review againWith all the work already done, you only have to do the exam and pass it!One last thing! If you followed this guide or found it helpful, let me know in the comments!" }, { "title": "Mastering the Basics of Data Analytics", "url": "/posts/introduction-to-data-analytics/", "categories": "General", "tags": "data, certification", "date": "2023-10-23 00:11:00 +0200", "snippet": "TLDREmbarking on the journey to attain the AWS Certified Data Analytics – Specialty certification requires a solid grasp of the fundamentals. This article is a comprehensive guide covering essentia...", "content": "TLDREmbarking on the journey to attain the AWS Certified Data Analytics – Specialty certification requires a solid grasp of the fundamentals. This article is a comprehensive guide covering essential information to kickstart your preparation.I wanted to prepare the AWS Certified Data Analytics – Specialty certification, but I had to start with the basics. I wrote this article to try to cover all the important information you should know before start to study. Let’s cover the fundamentals of data analytics.Understanding Data AnalyticsData analytics is the practice of examining and interpreting raw data to discover meaningful patterns, extract valuable insights, and make informed business decisions. It involves the use of statistical analysis, data mining techniques, and visualization tools to gain a deeper understanding of data and its significance. For children: Data analytics means looking at lots of information very carefully to find important clues and answers to questions. It’s like being a detective who uses special tools to solve mysteries by studying all the evidence and putting the pieces together.Decoding Big DataBig Data refers to extremely large and complex datasets (generated from various sources) that are challenging to process and analyze using traditional methods. For children: Big Data means having so much information from lots of places that it’s hard to handle and understand without special tools. It’s like having a giant puzzle with many pieces that we need to put together to learn new things.AWS Certification Evolution: From “Big Data” to “Data Analytics”Wondering why the AWS certification transitioned from AWS Certified Big Data - Specialty to AWS Certified Data Analytics - Specialty? This change mirrors the evolving landscape of data analytics, expanding beyond big data processing to encompass a broader range of analytics techniques and services offered by AWS.Knowing When to Use Data AnalyticsWhen to use analytics depends on the three Vs: volume, velocity, and variety of data. Volume: Analytics is useful when dealing with a large amount of data that is too vast to process manually. By using analytics tools, you can efficiently analyze and gain insights from massive data sets that would be otherwise overwhelming. Velocity: Analytics is beneficial when dealing with data that is generated at high speeds and requires real-time or near-real-time analysis. For example, in industries like finance or e-commerce, where transactions occur rapidly, analytics helps monitor and analyze data in real-time to detect anomalies, make quick decisions, or respond to changing conditions promptly. Variety: Analytics is valuable when dealing with diverse data types and formats. It involves processing and analysing structured data (e.g., spreadsheets), semi-structured data (e.g., XML files), and unstructured data (e.g., social media posts, emails). Analytics tools can handle this variety of data, allowing you to extract insights from different sources and formats.Analytics is then used when there is a large volume of data, data is generated at high speeds, and data comes in various types and formats.Here are some examples: Understanding customer behavior: Businesses can use data analytics to analyze customer preferences, purchasing patterns, and behavior to better understand their needs and provide personalized experiences. Improving operational efficiency: Data analytics can be applied to optimize processes, identify bottlenecks, and streamline operations, leading to cost savings and increased productivity. Enhancing marketing strategies: By analysing data on customer demographics, interests, and online behavior, companies can create targeted marketing campaigns, tailor advertisements, and improve customer engagement. Predicting trends and forecasting: Data analytics can help forecast market trends, demand for products or services, and emerging opportunities, enabling organizations to make proactive decisions and stay ahead of the competition. Fraud detection and risk assessment: Data analytics plays a crucial role in detecting fraudulent activities, identifying anomalies, and assessing risks across various industries, such as finance, insurance, and cybersecurity. Healthcare and medical research: Data analytics is used to analyze large volumes of medical data, patient records, and genetic information to improve diagnoses, develop new treatments, and advance medical research.Classifying DataUnderstanding the type of data is essential for determining the appropriate storage, analysis, and processing methods. Each data type requires different tools and techniques to make sense of the information it contains.Data can be classified into different types based on its structure and organization. Here are three main data types: Structured Data: Structured data refers to well-organized and formatted data that fits into predefined schemas or tables. It has a fixed format with clearly defined fields and relationships. Examples of structured data include data stored in relational databases like Amazon RDS (Relational Database Service). Structured data is easily searchable and can be queried using specific languages like SQL. Semi-structured Data: Semi-structured data does not conform to a rigid schema but has some organizational elements. It contains both structured and unstructured elements, allowing flexibility in data representation. Examples of semi-structured data formats include non-SQL databases, XML (eXtensible Markup Language), JSON (JavaScript Object Notation), and CSV (Comma-Separated Values) files. Semi-structured data can be queried using specialized query languages or transformed into structured formats for analysis. Unstructured Data: Unstructured data refers to data that does not have a predefined structure or organization. It does not fit into traditional rows and columns like structured data. Unstructured data includes text documents, images, videos, social media posts, emails, and sensor data. Analysing unstructured data requires advanced techniques like natural language processing (NLP), computer vision, and machine learning algorithms to extract valuable insights. Types of AnalyticsThere are three main types of analytics: Descriptive Analytics: focuses on summarizing historical data to understand what has happened in the past. It involves aggregating and analysing data to uncover patterns, trends, and key performance indicators (KPIs). Descriptive analytics helps in gaining insights into past events and provides a foundation for further analysis. Predictive Analytics: involves using historical and current data to make predictions or forecasts about future events or outcomes. It leverages statistical models, machine learning algorithms, and data mining techniques to identify patterns and trends and make data-driven predictions. Predictive analytics helps organizations anticipate customer behavior, optimize operations, mitigate risks, and improve decision-making processes. Prescriptive Analytics: goes beyond predicting future outcomes and recommends actions to optimize decision-making. It utilizes advanced analytics techniques, optimization algorithms, and simulation models to provide insights on what actions should be taken to achieve desired outcomes. Prescriptive analytics helps in making informed decisions and taking proactive measures to drive desired results.Two Approaches to Data Processing Batch Analytics: Batch analytics involves processing large volumes of historical data in batches or groups. It focuses on analysing data accumulated over a specific period, such as hours, days, or weeks. Batch analytics allows organizations to gain insights from past data trends and patterns. It is commonly used for tasks like generating reports, identifying long-term trends, and performing retrospective analysis. Real-time Analytics: Real-time analytics refers to the analysis of data as it is generated or received, providing immediate insights and responses. It enables businesses to make instant decisions and take immediate actions based on incoming data. Real-time analytics is valuable for scenarios that require timely information and quick responses.The Data Analytics ProcessThe data analytics process involves four steps: Collect/Ingest: In this step, data is gathered from various sources and ingested into a storage system, such as Amazon S3 or Amazon Redshift. Store: The collected data needs to be stored in a scalable and secure manner. AWS provides several services for data storage, including Amazon S3, Amazon Redshift, and Amazon DynamoDB. Analyze/Process: Once the data is stored, it can be processed and analysed using various AWS services like Amazon EMR, AWS Glue, or Amazon Athena. These services offer tools and frameworks for data transformation, exploration, and running advanced analytics algorithms. Visualize: The final step involves visualizing the analysed data to derive meaningful insights. AWS services like Amazon QuickSight and Amazon Kinesis Data Analytics provide visualization capabilities to create interactive dashboards, reports, and real-time data visualizations. If you want to know more you can review this official link about What Is Data Analytics?" }, { "title": "10 steps to prepare any AWS certification", "url": "/posts/10-steps-to-prepare-any-aws-certification/", "categories": "General", "tags": "certification", "date": "2023-10-20 19:37:00 +0200", "snippet": "IntroductionThe process to prepare for an AWS certification is always the same, but yes, logically the content and difficulty of the exams will be different. And of course, there will always be nua...", "content": "IntroductionThe process to prepare for an AWS certification is always the same, but yes, logically the content and difficulty of the exams will be different. And of course, there will always be nuances, but let’s keep an open mind.More than one year ago, I wrote this other article in Spanish, about How to pass any AWS certification. At this moment I had 5 AWS certifications and I had passed 6/6 AWS exams (1 renewal exam). There you will find much more information, but I want to have here in my blog the updated version with the most useful information.Now, 22 of October of 2023, I have 2 more AWS certifications, and 8/8 in AWS exams.These are just numbers, but I know how to prepare the AWS certification exams, and if you are reading this article you probably want to know why I wrote something like this and why you should trust me.My 10 steps to achieve any AWS certificationThis is the process I always follow to prepare any AWS certification: 1/10: Find the proper certification for youThe first step is choose your certification. Probably you already did it.Access to the AWS certification website, and check all of them. You have to review: Level of the certification Area of the certificationNext step is access to the specific certification and read all the information included in the official page. It includes the exam guide to validate if this certification is suitable for you and whether pursuing it aligns with your goals. 2/10: Find your WHY: Motivation is the keyDo you truly want this specific certification? Do you need it? Why?If you don’t have a very good reason, you might find it challenging to stay committed. 3/10: Create a study planSet a target date for the exam and allocate study hours accordingly to create a structured plan. When creating your study plan, it is important to be realistic about the amount of time you can commit to studying. Be sure to factor in other commitments, such as work, family, and social obligations.It’s important to stick to this plan as much as possible. Don’t worry, if necessary, you can reschedule the exam for a later date, but it’s crucial to have a target date in mind. 4/10: Take the official AWS Exam Readiness courseThis course provides an overview of the certification and highlights the key topics you should study.You can do it either through on-demand digital training or by participating in a webinar.Note that this course only provides a high-level understanding, so additional study is required. However, AWS explains the most important aspects of the certification, making it essential!You should try to study everything included there. 5/10: Choose the main training course that will prepare you for the certificationIt must be specific to your certification.There are many options available, including official courses, online video courses, book courses/preparations (not available for all certifications), and in-person training.In my last exam, I skipped this step because I wanted to experiment if I could prepare for one certification without any specific course. My conclusion is: if you have limited time and don’t want to invest a significant amount of time searching for various resources to cover all exam content, yes, the specific certification course is necessary. It is going to save to you a lot of time. 6/10: Practice using the AWS console to gain hands-on experiencePractice is the best way to learn and gain real experience.Avoid relying solely on studying tests or theory—you must know how to use the theory in practice!Yes, you can pass an AWS exam without the practice, but then, what is the point? You won’t know how to do anything, and you’ll likely forget everything… 7/10: Read the recommended whitepapers for your certificationAWS Whitepapers are in-depth technical documents that provide detailed information about specific AWS services.These whitepapers may be extensive, but it’s crucial to read the ones recommended on the certification’s official page. 8/10: Read the recommended FAQsThe services listed in the FAQs section of the certification are the most important services for this certification selected by AWS. You should check it.The AWS FAQs are a great resource for getting clarification on any topics that are confusing. The FAQs are also a good way to learn about common misconceptions about AWS services. You will fill many gaps you might have.Is this step required to prepare an AWS certification? No, but it is highly recommended. 9/10: Take your own notes (and study them)Taking your own notes is a great way to retain information. Be sure to review your notes regularly, and add to them as you learn new things.Another option is to find comprehensive summaries online, but be cautious not to rely solely on them and avoid taking shortcuts, though it may be tempting. 10/10: Practice with exam-style questions (tests)Practice exams are a great way to get used to the format of the certification exam and the types of questions that are typically asked. Be sure to take practice exams under timed conditions to simulate the real exam experience.You have a lot of options.ConclusionSo, you’re thinking about getting AWS certified? Good for you! It’s a great way to boost your career and learn some cool stuff along the way. Just remember, the journey is just as important as the destination. Don’t just focus on passing the exam. Take the time to really understand the material and how it applies to the real world.And don’t be afraid to ask for help. There are tons of resources available to help you prepare, from online forums to study groups.With dedication and hard work, you’ll be a certified AWS professional in no time!" }, { "title": "AWS WAF (Web Application Firewall): Deep Dive", "url": "/posts/aws-waf-web-application-firewall-deep-dive/", "categories": "Security", "tags": "security, waf, deep dive", "date": "2023-05-19 19:41:00 +0200", "snippet": "IntroductionA Web Application Firewall (WAF) is a security solution that protects web applications from malicious attacks, such as cross-site scripting, SQL injection, and malicious bot traffic. WA...", "content": "IntroductionA Web Application Firewall (WAF) is a security solution that protects web applications from malicious attacks, such as cross-site scripting, SQL injection, and malicious bot traffic. WAF is typically deployed as a reverse proxy, sitting between the internet and the web application, to inspect and filter incoming requests before they reach the web server.How WAF WorksWAF works by analyzing incoming HTTP and HTTPS requests to a web application and allows or blocks requests based on pre-defined security rules. Security rules can be based on IP addresses, headers, parameters, and other attributes of the request. WAF can also perform Deep Packet Inspection (DPI) to inspect the contents of the request payload and determine if the request contains malicious content.If a request violates a security rule, the WAF blocks the request and returns an error response to the client.AWS WAFAWS WAF is a popular choice for cloud-based WAF solutions, providing a comprehensive set of security rules to protect web applications.AWS WAF (Web Application Firewall) is a cloud-based service that protects your web applications, defending against common web exploits that could impact availability, compromise security, or consume excessive resources. It enables you to control access to your web content and provides customizable security rules to filter traffic based on IP addresses, HTTP headers, HTTP body content, or URI strings.Here’s a visual representation of the basic architecture of AWS WAF:Protected resourcesYou can protect the following resource types: Amazon CloudFront distribution Amazon API Gateway REST API Application Load Balancer AWS AppSync GraphQL API Amazon Cognito user pool AWS App Runner service AWS Verified Access instanceMain componentsThe main components of AWS WAF include the following: Rules: AWS WAF allows you to create rules that define the types of traffic you want to allow or block from reaching your web applications. You can create rules based on various conditions such as IP addresses, HTTP headers, URI strings, and HTTP body content. Managed Rule Groups: AWS WAF provides pre-built managed rule groups that offer protection against common web attacks such as SQL injection, cross-site scripting (XSS), and more. These rule groups are created and maintained by AWS and updated regularly to ensure they provide up-to-date protection against the latest threats. Web ACLs: AWS WAF uses web ACLs (Web Access Control Lists) to group together rules that you can then apply to one or more web applications. Web ACLs allow you to apply a set of rules across multiple web applications, making it easier to manage and apply security policies consistently. Benefits and featuresAWS WAF offers a wide range of benefits and features, empowering you to secure your web applications effectively. Here are some key advantages of using AWS WAF: Agile protection against web attacks: AWS WAF rule propagation and updates take just under a minute, enabling you to react faster when you are under an attack or when security issues arise. Ease of deployment and maintenance: AWS WAF is easy to deploy and protects application(s) deployed on either Amazon CloudFront, the Application Load Balancer, or Amazon API Gateway. There is no additional software to deploy, DNS configuration, or SSL/TLS certificate to manage. Cost-effective: AWS WAF is a pay-as-you-go service, which means you only pay for the resources you use. This makes it a cost-effective solution for securing your web applications. Improved web traffic visibility: AWS WAF gives near real-time visibility into your web traffic, which you can use to create new rules or alerts in Amazon CloudWatch. Scalability: AWS WAF is designed to handle high volumes of traffic and can scale automatically to meet the demands of your web applications. Flexibility: AWS WAF provides a wide range of options for creating custom rules to filter traffic, giving you greater flexibility in defining your security policies. Hands-on with AWS WAFFirst of all, you must know that AWS WAF is a regional service. However, it seems a global service when you access it, but you have to change between regions using this option:Getting started with WAF is relatively straightforward, simply log in to the AWS Management Console and navigate to the AWS WAF service. From there, you can create a new WAF policy and configure the policy with the desired security rules. Step 1: Describe web ACL and associate it to AWS resources Step 2: Add rules and rule groups Step 3:Set rule priority Step 4: Configure metrics Step 5: Review and create web ACL Enabling logsLogs are disabled by default, so we have to enable it. We have to enter the Web ACL created and access to Logging and Metrics tab: As you can check in the following image We will have 3 options as Logging Destination (CloudWatch Logs Groups, Kinesis Data Firehose stream and S3 bucket): After enabling CloudWatch logs, we can access the CloudWatch Log Insights tab and execute sample queries or create custom queries to analyze the logs. We can see the results as logs or in a diagram (Visualization): MetricsAWS WAF reports metrics to CloudWatch in one-minute intervals by default, and the metrics are retained for 2 weeks.The metrics monitored are the following: AllowedRequests BlockedRequests CountedRequests PassedRequestsThese metrics provide a SUM count of web requests that hit a specific Rule or Web ACL.Create custom alarmsUsually is a good idea to create some alarms to be notified if some block requests appear on our web, so we can configure a CloudWatch Alarm.The easy way to create the alarm is through the AWS WAF Metric for BlockeRequests. There we have a direct link to create one alarm with this associated metric if we access Graphed Metrics` and select the indicated icon in red color:After the creation, we will have something like that:Estimating CostsThe cost of AWS WAF can vary depending on the scale of your deployment, ranging from a few dollars per month for small deployments to several thousand dollars per month for large-scale deployments. AWS WAF pricing is based on the number of web requests processed and the number of security rules that are used.Example of cost for our example (1 Web ACL with a few managed rules): $5.00 per web ACL per month (prorated hourly) * 1 web ACL = $5.00 $1.00 per rule per month (prorated hourly) * 5 rules = $1.00 $0.60 per million requests processed * 1 (we will assume 1 million request) = $0.60 $0.10 per alarm metric * 1 alarm = $0.10Total: $6.70 per monthMore Information about AWS WAFFor more information about AWS WAF, you can visit the following links AWS WAF (Developer Guide): https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html Security Automations for AWS WAF: https://aws.amazon.com/solutions/implementations/security-automations-for-aws-waf/ WAF FAQs: https://aws.amazon.com/waf/faqs/" }, { "title": "How to deploy a serverless website with Terraform", "url": "/posts/how-to-deploy-serverless-website-with-terraform/", "categories": "IaC", "tags": "how-to, iac, terraform, github, serverless", "date": "2023-05-05 20:53:00 +0200", "snippet": "IntroductionAs you already know, creating and managing infrastructure can be a complex and time-consuming process, and fortunately, tools like Terraform can simplify this process by allowing you to...", "content": "IntroductionAs you already know, creating and managing infrastructure can be a complex and time-consuming process, and fortunately, tools like Terraform can simplify this process by allowing you to define your Infrastructure as Code (IaC). In this article, we will explore the basics of Terraform and walk through how to create infrastructure on AWS using it.However, this is not my first article about Infrastructure as Code tools. If you want to know how to create infrastructure using familiar programming languages you can review this article about CDK: How to create infrastructure with CDK If you want to create serverless applications on AWS, you can review this other article about SAM: How to create serverless applications with SAM I have uploaded the source code used in this article in the following GitHub repository https://github.com/alazaroc/aws-terraform-serverless-websiteWhat is Terraform?Terraform is an open-source Infrastructure as Code (IaC) software tool created by HashiCorp. It allows you to define and manage your infrastructure using human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform supports a wide range of cloud providers, including AWS, Microsoft Azure, and Google Cloud Platform.How Terraform WorksTerraform uses a declarative configuration language called HashiCorp Configuration Language (HCL).In a nutshell: Write: You define your infrastructure in a series of .tf files, which contain the configuration for your resources Plan: Terraform then uses this configuration to generate an execution plan, which outlines the changes that will be made to your infrastructure. Apply: Once you have reviewed the execution plan, you can apply it to your infrastructure using the “terraform apply” command. Terraform will then provision your resources according to the configuration you defined.Getting Started with TerraformTo get started with Terraform, you will need to install it on your machine. You can download the latest version of Terraform from the official website here, and if you want to deploy on AWS, you also need AWS CLI.Once you have installed Terraform, you can start creating your infrastructure.In this section, we will review how Terraform works creating a first example to deploy a simple S3 bucket: Create a new directory for your Terraform configuration file: mkdir my-serverless-websitecd my-serverless-website Create a new file called “main.tf” and there we are going to include the AWS provider information with the “us-east-1” region, and one resource of type aws_s3_bucket to create an S3 bucket with all the default configuration. Remember that the S3 bucket name must be unique globally. provider \"aws\" { region = \"us-east-1\"}resource \"aws_s3_bucket\" \"website_bucket\" { bucket = \"my-unique-bucket-name-12y398y13489148h\"} Initialize your Terraform configuration. terraform init When you run terraform init, Terraform downloads and installs the necessary provider plugins and modules required for your configuration. It also initializes the backend, which is the storage location for your Terraform state file. The state file is used to store the current state of your infrastructure, and it is used by Terraform to plan and apply changes to your infrastructure. Generate an execution plan (optional): terraform plan This command generates an execution plan based on the configuration in your main.tf file, and let you check what is going to be deployed. Apply the execution plan to deploy on AWS: terraform apply When you run terraform apply, Terraform compares the current state of your infrastructure with the desired state defined in your configuration. It then creates, modifies, or deletes resources as necessary to bring your infrastructure into the desired state. In our example, a new S3 bucket will be deployed. Now, the AWS resources have been created. Let’s check it accessing the AWS S3 service to access the new resource created by Terraform: We already know how to deploy AWS resources with Terraform, so in the following section, we will evolve the initial example to deploy three serverless websites using S3 bucket.How to deploy a serverless website with TerraformWe have an AWS S3 bucket deployed, and now we are going to use it to host a serverless website. There are different ways to do it, and we are going to evolve the S3 bucket to create a serverless static website.This is what we are going to build: v1.1: public S3 bucket Advantage: easy to implement Disadvantages: no custom domain, no aligned with security best practices (public bucket), no cache for static files v1.2: public S3 as Static website hosting Advantages: easy to implement, index document and error page, redirection rules Disadvantages: not aligned with security best practices (public bucket), no cache for static files, Amazon S3 website endpoints do not support HTTPS (if you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3) v2: CloudFront distribution + private S3 bucket Advantage: easy to implement, private s3 bucket, cache for static files Disadvantages: auto-generated domain name v3: Route53 + ACM + CloudFront Distribution + private S3 bucket + optionally Lambda Edge Advantages: custom domain name using AWS managed certificates, private s3 bucket, cache for static files Disadvantages: more complex to implement In these examples, we are going to deploy a static website based on an index.html file with this content:This is my serverless websitev1.1: public S3 bucketIn this version 1.1 we are going to expose the S3 bucket publicly so any person can access the index.html file using the public S3 endpoint. Advantage: easy to implement Disadvantages: no custom domain no aligned with security best practices (public bucket) no cache for static files This solution is not aligned with security best practices because it expose an S3 bucket publicly.Update the file “main.tf” with the following content:provider \"aws\" { region = \"us-east-1\"}resource \"aws_s3_bucket\" \"website_bucket\" { bucket = \"my-unique-bucket-name-12y398y13489148h\"}resource \"aws_s3_object\" \"website_bucket\" { bucket = aws_s3_bucket.website_bucket.id key = \"index.html\" source = \"index.html\" content_type = \"text/html\"}resource \"aws_s3_account_public_access_block\" \"website_bucket\" { block_public_acls = false block_public_policy = false}resource \"aws_s3_bucket_public_access_block\" \"website_bucket\" { bucket = aws_s3_bucket.website_bucket.id block_public_acls = false block_public_policy = false ignore_public_acls = false restrict_public_buckets = false}resource \"aws_s3_bucket_policy\" \"website_bucket\" { bucket = aws_s3_bucket.website_bucket.id policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Sid = \"PublicReadGetObject\" Effect = \"Allow\" Principal = \"*\" Action = [ \"s3:GetObject\", \"s3:ListBucket\", ] Resource = [ \"${aws_s3_bucket.website_bucket.arn}\", \"${aws_s3_bucket.website_bucket.arn}/*\" ] } ] })}Now, execute the terraform apply command: terraform apply --auto-approveThen, open a private window in your browser and access the index.html content using the public endpoint of our S3 bucket. If you have executed the code before, it will be the following URL: https://my-unique-bucket-name-12y398y13489148h.s3.amazonaws.com/index.htmlv1.2: Static website hosting using S3In this version, similar to the previous one, we still have exposed the S3 bucket publicly but we will enable the static website hosting feature of S3. Advantages: easy to implement static website includes the configuration of an index document, an error page and also redirection rules Disadvantages: not aligned with security best practices (public bucket) no cache for static files Amazon S3 website endpoints do not support HTTPS (if you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3) This solution is not aligned with security best practices because it expose an S3 bucket publicly.Using the content of the “main.tf” showed in the previous v1.1 example, add at the end of the document the following lines:resource \"aws_s3_bucket_website_configuration\" \"website_bucket\" { bucket = aws_s3_bucket.website_bucket.id index_document { suffix = \"index.html\" }}Now, execute the terraform apply command: terraform apply --auto-approveThen, open a private window in your browser and access through the static website to the index.html content: http://my-unique-bucket-name-12y398y13489148h.s3-website-us-east-1.amazonaws.com/ If you didn’t realize before, look that we are using HTTP, not HTTPS. S3 static website don’t support HTTPS.v2: CloudFront Distribution + private S3 bucketIn this version, we are going to change the bucket to private again (and also, we are going to enable again the Block Public Access settings for this account feature of S3), and we are going to create a CloudFront Distribution connected with the private S3 bucket. So, we are going to access the S3 bucket using the CloudFront distribution. Advantage: easy to implement private s3 bucket (aligned with the security best practices) includes cache for static files Disadvantages: auto-generated domain name (by CloudFront) Replace the “main.tf” content with the following lines:provider \"aws\" { region = \"us-east-1\"}resource \"aws_s3_bucket\" \"website_bucket\" { bucket = \"my-unique-bucket-name-12y398y13489148h\"}resource \"aws_s3_account_public_access_block\" \"website_bucket\" { block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true}resource \"aws_s3_object\" \"website_bucket\" { bucket = aws_s3_bucket.website_bucket.id key = \"index.html\" source = \"index.html\" content_type = \"text/html\"}resource \"aws_cloudfront_distribution\" \"cdn_static_site\" { enabled = true is_ipv6_enabled = true default_root_object = \"index.html\" comment = \"my cloudfront in front of the s3 bucket\" origin { domain_name = aws_s3_bucket.website_bucket.bucket_regional_domain_name origin_id = \"my-s3-origin\" origin_access_control_id = aws_cloudfront_origin_access_control.default.id } default_cache_behavior { min_ttl = 0 default_ttl = 0 max_ttl = 0 viewer_protocol_policy = \"redirect-to-https\" allowed_methods = [\"GET\", \"HEAD\", \"OPTIONS\"] cached_methods = [\"GET\", \"HEAD\"] target_origin_id = \"my-s3-origin\" forwarded_values { query_string = false cookies { forward = \"none\" } } } restrictions { geo_restriction { locations = [] restriction_type = \"none\" } } viewer_certificate { cloudfront_default_certificate = true }}resource \"aws_cloudfront_origin_access_control\" \"default\" { name = \"cloudfront OAC\" description = \"description of OAC\" origin_access_control_origin_type = \"s3\" signing_behavior = \"always\" signing_protocol = \"sigv4\"}output \"cloudfront_url\" { value = aws_cloudfront_distribution.cdn_static_site.domain_name}data \"aws_iam_policy_document\" \"website_bucket\" { statement { actions = [\"s3:GetObject\"] resources = [\"${aws_s3_bucket.website_bucket.arn}/*\"] principals { type = \"Service\" identifiers = [\"cloudfront.amazonaws.com\"] } condition { test = \"StringEquals\" variable = \"aws:SourceArn\" values = [aws_cloudfront_distribution.cdn_static_site.arn] } }}resource \"aws_s3_bucket_policy\" \"website_bucket_policy\" { bucket = aws_s3_bucket.website_bucket.id policy = data.aws_iam_policy_document.website_bucket.json}Now, execute the terraform apply command: terraform apply --auto-approveThen, open a private window in your browser and access the index.html content using the CloudFront DNS. The execution of the terraform template has as output the URL of the CloudFront Distribution. Now, the S3 bucket is private, so if you access to the public endpoint of S3 trying to get the index.html file you will receive an errorv3: Route53 + ACM + CloudFront Distribution + private S3 bucketIn the last example, we will use our domain (registered in Route53) and we will create a certificate using ACM. Requirement: Required a custom domain Advantages: custom domain name using AWS managed certificates private s3 bucket (aligned with the security best practices) includes cache for static files Disadvantages: more complex to implement To be able to run this example you need your own domain (example.com) and you have to register it in Route53 service. If you don’t have it you can buy a new domain in the Route53 server but it will cost you about 10 dollars per year.These are the changes that you have to do in the previous “main.tf” file: Update in your CloudFront Distribution resource aws_cloudfront_distribution the following, (where ${var.domain_name_simple} is for example example.com): viewer_certificate { cloudfront_default_certificate = true } replacing it for this: viewer_certificate { acm_certificate_arn = aws_acm_certificate.cert.arn ssl_support_method = \"sni-only\" minimum_protocol_version = \"TLSv1.2_2021\" } aliases = [ var.domain_name_simple, var.domain_name ] Next, add the following lines: resource \"aws_acm_certificate\" \"cert\" { provider = aws.use_default_region domain_name = \"*.${var.domain_name_simple}\" validation_method = \"DNS\" subject_alternative_names = [var.domain_name_simple] lifecycle { create_before_destroy = true }}data \"aws_route53_zone\" \"zone\" { provider = aws.use_default_region name = var.domain_name_simple private_zone = false}resource \"aws_route53_record\" \"cert_validation\" { provider = aws.use_default_region for_each = { for dvo in aws_acm_certificate.cert.domain_validation_options : dvo.domain_name =&gt; { name = dvo.resource_record_name record = dvo.resource_record_value type = dvo.resource_record_type } } allow_overwrite = true name = each.value.name records = [each.value.record] type = each.value.type zone_id = data.aws_route53_zone.zone.zone_id ttl = 60}resource \"aws_acm_certificate_validation\" \"cert\" { provider = aws.use_default_region certificate_arn = aws_acm_certificate.cert.arn validation_record_fqdns = [for record in aws_route53_record.cert_validation : record.fqdn]}resource \"aws_route53_record\" \"www\" { zone_id = data.aws_route53_zone.zone.id name = \"www.${var.domain_name_simple}\" type = \"A\" alias { name = aws_cloudfront_distribution.cdn_static_site.domain_name zone_id = aws_cloudfront_distribution.cdn_static_site.hosted_zone_id evaluate_target_health = false }}resource \"aws_route53_record\" \"apex\" { zone_id = data.aws_route53_zone.zone.id name = var.domain_name_simple type = \"A\" alias { name = aws_cloudfront_distribution.cdn_static_site.domain_name zone_id = aws_cloudfront_distribution.cdn_static_site.hosted_zone_id evaluate_target_health = false }} Now, execute the terraform apply command: terraform apply --auto-approve Finally, open a private window in your browser and access your registered domain: https://example.com ConclusionIn this article, we have explored the basics of Terraform and walked through how to create infrastructure with different examples. With Terraform, you can define your infrastructure as code and automate the process of creating and updating your resources.This example is based on the code I have created to deploy my own blog https://playingaws.com using Route53 + ACM + CloudFront Distribution + private S3 bucket." }, { "title": "How open source tools can help you with your code", "url": "/posts/aws-open-source-tools-code/", "categories": "General", "tags": "open-source, security, github", "date": "2023-02-11 15:41:00 +0100", "snippet": "IntroductionThis is the last article of a series of 3 about open source tools and AWS. If you haven’t read them I recommend you to do it. Getting started with AWS open-source tools: here Open sou...", "content": "IntroductionThis is the last article of a series of 3 about open source tools and AWS. If you haven’t read them I recommend you to do it. Getting started with AWS open-source tools: here Open source tools to analyze your AWS environment: hereThe current article is about how open source tools can help you with your code: to generate it (IaC), validate it (policy as code and compliance) and analyze it (static analysis/credentials)I have created the following sections: IaC tools Analysis code: credentials and static analysis Policy as Code ComplianceGenerate IaCThis section is about open source tools which will help you to generate Infrastructure as Code directly from your existing resources. former2: generate IaC in many different languages (CloudFormation, CDK, Terraform, ...) terracognita: generates your infrastructure as code on Terraform configuration AirIAM: scans existing IAM usage patterns and provides a simple method to migrate IAM configurations into a right-sized Terraform planformer2Former2 allows you to generate Infrastructure-as-Code outputs from your existing resources within your AWS account. By making the relevant calls using the AWS JavaScript SDK, Former2 will scan across your infrastructure and present you with the list of resources for you to choose which to generate outputs for.Supported IaC output: CloudFormation Terraform Troposphere CDK v1 CDK v2 CDK (Terraform) Pulumi Diagram Raw Output (Debug) Why use it: Generate IaC from AWS account resources created manually using an extension for your browser. You will configure it through the former2.com website.Is it popular?: On github: Watch 36; Fork 190; Stars 1.6KRecently updated?: Yes, two month ago (on Dec 5, 2022)URL: https://github.com/iann0036/former2More information: https://former2.com/#section-dashboardterracognitaReads from existing public and private cloud providers (reverse Terraform) and generates your infrastructure as code on Terraform configuration.Terracognita currently imports AWS, GCP, AzureRM and VMware vSphere cloud providers as Terraform (v1.1.9) resource/state. Why use it: yo are able to create the terraform templates of all your AWS infrastructure with zero effort and this is something wonderful, isn’t it?Is it popular?: On github: Watch 33; Fork 119; Stars 1.5kRecently updated? No. Last commit on Aug 25, 2022. In total, 625 commitsURL: https://github.com/cycloidio/terracognita/AirIAMAirIAM scans existing IAM usage patterns and provides a simple method to migrate IAM configurations into a right-sized Terraform plan. It identifies unused users, roles, groups, policies and policy attachments and replaces them with a Least Privileges Terraform code modelled to manage AWS IAM.AirIAM was created to promote immutable and version-controlled IAM management to replace today’s manual and error prone methods. Why use it: IAM scan tool to detect unused resources (based on Amazon Access Advisor APIs) and the creation of terraform templates of your IAM resources. Is it popular?: On github: Watch 15; Fork 68; Stars 677Recently updated? No. Last commit the Aug 2, 2022. In total, 426 commitsURL: https://github.com/bridgecrewio/AirIAM First time I used this tool the terraform template generation functionality works fine, now now I receive an error and I was not able to use it. However, I think that this tool is useful to find this iam unused resourcesSummary: Which tool should I use to generate IaC? CDK, CloudFormation, Troposphere, Pulumi: former2 Terraform IaC: former2 or terracognita Specific IAM resources: former2, terracognita or AirIAMAnalyze IaC code: static analysis Static Code Analysis commonly refers to the running of Static Code Analysis tools that attempt to highlight possible vulnerabilities within ‘static’ (non-running) source code. Information extracted from OWASP website checkov: Scans cloud infrastructure provisioned to detects security and compliance misconfigurations using graph-based scanning. KICKS: Find security vulnerabilities, compliance issues, and infrastructure misconfigurations early in the development cycle of your infrastructure-as-code terrascan: static code analyzer for Infrastructure as Code to scan infrastructure as code for misconfigurations, detect security vulnerabilities and compliance violations tfsec: static analysis code for Terraform cfn-nag: looks for patterns in CloudFormation templates that may indicate insecure infrastructure. All the tools in this section allow you to create custom rulescheckovCheckov is a static code analysis tool for infrastructure as code (IaC) and also a software composition analysis (SCA) tool for images and open source packages.It scans cloud infrastructure provisioned to detects security and compliance misconfigurations using graph-based scanning.Checkov scans these IaC file types: Terraform (for AWS, GCP, Azure and OCI) CloudFormation (including AWS SAM) Azure Resource Manager (ARM) Serverless framework Helm charts Kubernetes DockerSome Features: Over 1000 built-in policies cover security and compliance best practices for AWS, Azure and Google Cloud. In AWS there are 177 controls in the framework CKV_AWS. Checkov scans for compliance with common industry standards such as the Center for Internet Security (CIS) and Amazon Web Services (AWS) Foundations Benchmark. Detects AWS credentials in EC2 user-data, Lambda environment variables and Terraform providers. Identifies secrets using regular expressions, keywords, and entropy based detection. Plugins for popular IDEs available (JetBrains, VSCode and Vim). However, activating the extension requires submission of one-time Bridgecrew API Token that can be obtained by creating a new Bridgecrew platform account.You can create custom policies here more information using Python opr YAML. Why use it: Includes 177 AWS controls including the Center for Internet Security (CIS) and Amazon Web Services (AWS) Foundations Benchmark, and is able to detect secrets and AWS credentials in the code. You also can visualize checkov scan output using Bridgecrew platform (free to use with the Community plan - up to 50 resources and small projects - here)Is it popular?: On github: Watch: 55; Fork: 799; Stars: 5.2kRecently updated?: Today (in the time I am writing this post)URL: https://github.com/bridgecrewio/checkovMore information: https://www.checkov.io/1.Welcome/Quick%20Start.htmlKICKSFind security vulnerabilities, compliance issues, and infrastructure misconfigurations early in the development cycle of your infrastructure-as-code.KICS stands for Keeping Infrastructure as Code Secure and support the following IaC solutions: Terraform, AWS CloudFormation, AWS SAM, AWS CDK, Kubernetes, Docker, Ansible, Helm, Google Deployment Manager, Microsoft ARM, Microsoft Azure Blueprints, OpenAPI 2.0 and 3.0, Pulumi, Crossplane, Knative and Serverless Framework.KICS is 100% open source is written in Golang using Open Policy Agent (OPA) and it is possible create custom queries to create custom rules (using REGO language). Why use it: Evaluate IaC to detect vulnerabilities Is it popular?: On github: Watch: 22; Fork: 224; Stars: 1.4kURL: https://github.com/Checkmarx/kicsMore information: https://www.kics.io/terrascanTerrascan is a static code analyzer for Infrastructure as Code. Terrascan allows you to: Seamlessly scan infrastructure as code for misconfigurations. Monitor provisioned cloud infrastructure for configuration changes that introduce posture drift, and enables reverting to a secure posture. Detect security vulnerabilities and compliance violations. Mitigate risks before provisioning cloud native infrastructure. Offers flexibility to run locally or integrate with your CI\\CD.Key features 500+ Policies for security best practices Scanning of Terraform (HCL2), AWS CloudFormation Templates (CFT), Azure Resource Manager (ARM), Kubernetes (JSON/YAML), Helm v3, and Kustomize and Dockerfiles Integrates with docker image vulnerability scanning for AWS, Azure, GCP, Harbor container registries.Terrascan policies are written using the Rego policy language, and you can create your own policies Why use it: More than 300 AWS rules and support CloudFormation and Terraform. Is it popular?: On github: Watch: 70; Fork: 459; Stars: 3.9kRecently updated?: Today (in the time I am writing this post)URL: https://github.com/tenable/terrascanMore information: https://runterrascan.io/docs/tfsecThe tfsec open source tool provides security analysis of Terraform code and detects potential security issues based on AWS best practices.The tool, contain checks for more than 30 AWS resources, and can be found here: https://aquasecurity.github.io/tfsec/v1.28.1/checks/aws/tfsec has the capability to apply user-defined Rego policies. This is a useful feature if your organisation needs to implement custom security policies on top of avoid other misconfigurations and enforcing best practice guidelines. More information here.Some Features: Checks for misconfigurations across all major (and some minor) cloud providers Hundreds of built-in rules Applies (and embellishes) user-defined Rego policies Very fast, capable of quickly scanning huge repositories Plugins for popular IDEs available (JetBrains, VSCode and Vim) Why use it: static analysis code for terraform with checks in more than 30 AWS resources. Integration with Github Security alerts: Is it popular?: On github: Watch: 69; Fork: 485; Stars: 5.6kRecently updated?: Yes, last month (1318 commits)URL: https://github.com/aquasecurity/tfsecMore information: https://aquasecurity.github.io/tfsec/v1.28.1/cfn-nagThe cfn-nag tool looks for patterns in CloudFormation templates that may indicate insecure infrastructure. It will look for: IAM rules that are too permissive (wildcards) Security group rules that are too permissive (wildcards) Access logs that aren’t enabled Encryption that isn’t enabled Password literalsThe tool contains more than 150 AWS controls. Why use it: Analyze CloudFormation templates to detect insecure infrastructureIs it popular?: On github: Watch 35; Fork 199; Stars 1.1kRecently updated? No. Last commit the Jun 7, 2022. In total, 664 commitsURL: https://github.com/stelligent/cfn_nagSummary: Which tool should I use to perform a static analysis? CloudFormation code: checkov, KICKS terrascan, cfn-nag Terraform code: checkov, KICKS, terrascan or tfsec Integrate with IDE: checkov or tfsec Allow create custom rules: checkov, KICKS, terrascan, tfsecIn my blog-backend-infrastructure code available here I had the following errors using these tools: KICKS: 36 (3 high, 20 medium, 13 low) terrascan: 9 (1 high, 7 medium, 1 low) tfsec: N/A (only Terraform code) cfn-nag: 33Analyze IaC code: Policy as Code Policy-as-code is the use of code to define and manage rules and conditions to assure that your Infrastructure will be compliance with that.This is a way to apply preventative governance and compliance (shift left), validating Infrastructure-as-code (IaC) against your organizational best practices for security and compliance. CloudFormation Guard: policy-as-code evaluation tool for general purpose OPA: general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack Regula: evaluates infrastructure as code files for potential security and compliance violations prior to deployment.CloudFormation GuardAWS CloudFormation Guard is an open-source general-purpose policy-as-code evaluation tool. It provides developers with a simple-to-use, yet powerful and expressive domain-specific language (DSL) to define policies and enables developers to validate JSON or YAML formatted structured data with those policies.Supported: CloudFormation Templates, CloudFormation ChangeSets, Terraform JSON configuration files, Kubernetes configurations, and more. Why use it: Simple to use, compatible with CloudFormation, Terraform and kubernetes configurations.Is it popular?: On github: Watch: 30; Fork: 145; Stars: 1.1kRecently updated?: Last week (in the time I am writing this post)URL: https://github.com/aws-cloudformation/cloudformation-guard#installationMore information: https://docs.aws.amazon.com/cfn-guard/latest/ug/what-is-guard.html This is an official aws-cloudformation tool (yet open-source), but you have to create your own rules. I didn’t find the “security best practices” included in the control AWS best practices of Security Hub as rules. However, you have here some examples: https://github.com/aws-cloudformation/cloudformation-guard/tree/main/guard-examplesOpen Policy Agent (OPA)Open Policy Agent (OPA) is an open source, general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack.The Open Policy Agent is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language (REGO) that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.OPA ecosystem is hugh here, but in AWS we can use it to analyze our JSON/YAML file in CloudFormation or our Terraform template by creating custom rules for these languages.OPA is a project of the Cloud Native Computing Foundation (CNCF) landscape. Why use it: unify policy enforcement across the stack. This example is using Terraform. You have to convert your output Terraform to JSON file and then analyze it with OPA rules: Is it popular?: On github: Watch: 119; Fork: 1.1k; Stars: 7.6kRecently updated?: 4 days ago (in the time I am writing this post)URL: https://github.com/open-policy-agent/opaMore information: https://www.openpolicyagent.org/ You have to create your own OPA rules with REGO language (easy to learn). You have an online playground to test your REGO policies hereRegulaRegula is a tool that evaluates infrastructure as code files for potential AWS, Azure, Google Cloud, and Kubernetes security and compliance violations prior to deployment. Based on Open Policy Agent (OPA) and written in REGORegula supports the following file types: CloudFormation JSON/YAML templates Terraform source code (.tf or .tf.json format) Terraform JSON plans Kubernetes YAML manifests Azure Resource Manager (ARM) JSON templates (in preview)This is the list of rules applied (in AWS there are rules for Terraform and CloudFormation): https://regula.dev/rules.html. You must to know that at this time CloudFormation support 23 controls, and Terraform 114.Here you can find more information about Writing custom rules. Why use it: Evaluate IaC to detect vulnerabilitiesIs it popular?: On github: Watch: 54; Fork: 776; Stars: 5.1kURL: https://github.com/fugue/regulaMore information: https://regula.dev/Summary: Which tool should I use to create my custom Policy as Code? AWS solution: CloudFormation Guard, OPA or regula Cross-provider solution: OPA or regula (both using REGO language)Or you can use any of the tools in the static analysis category, because all of them allow you to create custom rules.Detect credentials in codeThese tools can be apply to any git code: General for git repository: git-secrets: Prevents you from committing passwords and other sensitive information to a git repository. gitleaks: tool for detecting and preventing hardcoded secrets like passwords, api keys, and tokens in git repos. Specific of IaC code: checkov: static code analysis tool for infrastructure as code (included secrets) This tool is already explained a few lines before so you can get the information from there git-secretsPrevents you from committing passwords and other sensitive information to a git repository. Why use it: Find api keys, passwords, AWS keys in the code.Is it popular?: On github: Watch: 198; Fork: 1.1k; Stars: 10.8kRecently updated?: Today (in the time I am writing this post). Commit in total 110URL: https://github.com/awslabs/git-secretsgitleaksGitleaks is a SAST tool for detecting and preventing hardcoded secrets like passwords, api keys, and tokens in git repos. Gitleaks is an easy-to-use, all-in-one solution for detecting secrets, past or present, in your code. Why use it: Analyze the code of your IaC infrastructure to detect security and compliance misconfigurations.Is it popular?: On github: Watch: 142; Fork: 1.1k; Stars: 11.5kRecently updated?: Today (in the time I am writing this post). Commit in total 896URL: https://github.com/zricethezav/gitleaksOthers Compliance cloud custodian: rules engine to define policies to enable a well managed, secure and cost optimized cloud infrastructure Cost of Terraform templates: Infracost: estimate cost for Terraform before to deploy Cloud custodianCloud Custodian is a rules engine to define policies to enable a well managed, secure and cost optimized cloud infrastructure in yaml format.Custodian can be used to manage AWS, Azure, and GCP environments by ensuring real time compliance to security policies (like encryption and access requirements), tag policies, and cost management via garbage collection of unused resources and off-hours resource management.Custodian policies are written in simple YAML configuration files that enable users to specify policies on a resource type (EC2, ASG, Redshift, CosmosDB, PubSub Topic) and are constructed from a vocabulary of filters and actions.You can apply to AWS to here Why use it: manage your rules with yaml policies Is it popular?: On github: Watch 1.3k; Starts 4.6kRecently updated?: Yes. The last commit was 3 days ago (4054 commits in total)URL: https://github.com/cloud-custodian/cloud-custodian/ Many examples available in https://www.cloudcustodian.io/docs/aws/examples/index.htmlInfracostInfracost shows cloud cost estimates for Terraform. It lets DevOps, SRE and engineers see a cost breakdown and understand costs before making changes, either in the terminal or pull requests.Infracost also has many CI/CD integrations so you can easily post cost estimates in pull requests. This provides your team with a safety net as people can discuss costs as part of the workflow.The CDK for Terraform is also supported as it can generate Terraform Why use it: If you are using Terraform, this tool will estimate the cost before to deploy. Do you want to try? (images taken from github)Output of infracost breakdown infracost diff shows diff of monthly costs between current and planned state Post cost estimates in pull requests Is it popular?: On github: Watch: 63; Fork: 418; Stars: 8.5kRecently updated?: Yes. Last commit yesterday (in the time I am writing this post). In total 2196 commitsURL: https://github.com/infracost/infracostMore information: https://www.infracost.io/docs/Next Steps My arsenal of security tools: https://github.com/toniblyx/my-arsenal-of-aws-security-tools Improve your handsome security skills: https://github.com/RhinoSecurityLabs/cloudgoat AWS Security primer: https://cloudonaut.io/aws-security-primer/ AWS Lambda Power tuning (optimize Lambda): https://github.com/alexcasalboni/aws-lambda-power-tuning" }, { "title": "Open source tools to analyze your AWS environment", "url": "/posts/aws-open-source-tools-environment/", "categories": "General", "tags": "open-source, github, security", "date": "2023-01-30 18:02:00 +0100", "snippet": "IntroductionThis is the second part of the series Open source tools. If this is the first article of this series that you are reading, I recommend you to review the first one Getting started with A...", "content": "IntroductionThis is the second part of the series Open source tools. If this is the first article of this series that you are reading, I recommend you to review the first one Getting started with AWS open-source tools here Some open source tool could appear in several categories (in this same article or among the 3 open source tools articles). I thought it was better not to mix information between categories when a tool could be part of several of them.Inventory and analyze your environmentThese open source tools are useful to analyze all your AWS environment: General cloudmapper: Analyze your AWS environment prowler: Quick analysis of your AWS environment IAM (specific) AirIAM: Detect IAM unused resources: users, access keys, roles, groups, policies and policy attachments cloudmapperCloudMapper helps you analyze your Amazon Web Services (AWS) environments.Based on Python components. Why use it: Analyze your AWS environment: review status of the account, resource inventory (all resources in all the regions and region usage), IAM resources (active/inactive), public resources (ec2, elb, elbv2, rds, redshift, ecs, autoscaling, cloudfront and apigateway) and finally findings of security (a few). List of the commands of this tool: The shortcut is: Run the collect command to get your resources (you can filter the regions that you want using the --regions 'eu-west-1,eu-west-2'… us-east-1 will be always added) Then run the report command (an html page will be generated with all the information) A specific command that I like is the iam_report because it allows you review unused permissions in your IAM resources: Is it popular?: On github: Watch 135; Fork 759; Stars 5.3KRecently updated? No. The last commit was on Jul 25, 2022 (956 commits). Also, some original functionality of this tools is now deprecated (generation of networking diagrams).URL: https://github.com/duo-labs/cloudmapper There is a CDK project to run in Fargate service the CloudMapper’s collection and audit capabilities nightly, across multiple accounts, sending any audit findings to a Slack channel and keeping a copy of the collected metadata in an S3 bucket. This is the diagram of the solution https://github.com/duo-labs/cloudmapper/blob/main/auditor/README.mdprowlerProwler is an Open Source security tool… and you will find all this information in the following section (this is for inventory and analysis of the environment).prowler has a functionality to perform a quick inventory check. It will give you in the console information about the number of each resource that you have, and in json/csv generated files you will find information about what is the specific resource (region, aws service, resource type, resource id and ARN) Why use it: Perform a quick analysis of your AWS environment (a few seconds). And this is the CSV file report: Is it popular?: On github: Watch 122; Fork 1.1k; Stars 7.6KRecently updated? Last commit yesterday (in the time I am writing this post). Total commits 2271URL: https://github.com/prowler-cloud/prowlerAirIAMAirIAM scans existing IAM usage patterns and provides a simple method to migrate IAM configurations into a right-sized Terraform plan. It identifies unused users, roles, groups, policies and policy attachments and replaces them with a Least Privileges Terraform code modelled to manage AWS IAM.AirIAM was created to promote immutable and version-controlled IAM management to replace today’s manual and error prone methods. Why use it: IAM scan tool to detect unused resources (based on Amazon Access Advisor APIs) and the creation of terraform templates of your IAM resources. This is another example analyzing other AWS account and exporting the results in a txt file: Is it popular?: On github: Watch 15; Fork 68; Stars 673Recently updated? No. Last commit the Aug 2, 2022. In total, 426 commitsURL: https://github.com/bridgecrewio/AirIAMUpdated: First time I used this tool the terraform template generation functionality works fine, now now I receive an error and I was not able to use it. However, I think that this tool is useful to find this iam unused resourcesSummary: Which tool should I use to analyze my environment? This is my personal opinion. If you have a different one, let me know in the comments at the end of this article!It depends what you want to do: Full analysis of the AWS environment: cloudmapper Get a quick inventory of the AWS environment: prowler Fast IAM unused resources analysis: AirIAM Apply least-privilege to IAM roles (review which permissions you are not using): cloudmapperSecurity AssessmentThere are many different tools to realize a security assessment of your environment. General prowler: security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness Scoutsuite: multi-cloud security-auditing tool, which enables security posture assessment of cloud environments cloudsploit: Cloud Security Scans to detect potential misconfigurations and security risks steampipe yes, AGAIN. Ensure that cloud resources comply with security benchmarks such as CIS, NIST, and SOC2. IAM cloudsplaining: IAM Security Assessment tool prowlerThis is the second time I mention prowler in this article in two different categories.Prowler is an Open Source security tool to perform AWS and Azure security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness.It contains more than 240 controls covering CIS, PCI-DSS, ISO27001, GDPR, HIPAA, FFIEC, SOC2, AWS FTR, ENS and custom security frameworks.There are 4 available categories: secrets trust-boundaries internet-exposed forensics-readyThere are 3 available Compliance Frameworks: cis_1.4_aws cis_1.5_aws ens_rd2022_awsseverity: informational low medium high critical Why use it: Security tool to perform security best practices assessments, audits, incident response, continuous monitoring, hardening and forensics readiness. By default, prowler will scan all AWS regions. If you want analyze only a few regions you can use the following command -f us-east-1 eu-west-1 ... And this is the HTML file report: Is it popular?: On github: Watch 122; Fork 1.1k; Stars 7.6KRecently updated? Last commit yesterday (in the time I am writing this post). Total commits 2271URL: https://github.com/prowler-cloud/prowlerScoutsuiteScout Suite is an open source multi-cloud security-auditing tool, which enables security posture assessment of cloud environments. Using the APIs exposed by cloud providers, Scout Suite gathers configuration data for manual inspection and highlights risk areas. Rather than going through dozens of pages on the web consoles, Scout Suite presents a clear view of the attack surface automatically.Scout Suite was designed by security consultants/auditors. It is meant to provide a point-in-time security-oriented view of the cloud account it was run in. Once the data has been gathered, all usage may be performed offline. Why use it: Enable security posture assessment of cloud environments. By default, scoutsuite will scan all AWS regions. If you want analyze only a few regions you can use the following command -r us-east-1 eu-west-1 ... Is it popular?: On github: Watch 113; Fork 816; Stars 5KRecently updated?: No. Last commit was on Sep 5, 2022 (6101 commits in total)URL: https://github.com/nccgroup/ScoutSuite I didn’t found information about what security frameworks is using or more information about the security controls. However, the findings in the audit report are useful and you have to review them!cloudsploitCloudSploit by Aqua is an open-source project designed to allow detection of security risks in cloud infrastructure accounts, including: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Oracle Cloud Infrastructure (OCI), and GitHub. These scripts are designed to return a series of potential misconfigurations and security risks.Compliance: hipaa PCI CIS (level 1 and 2) Why use it: detect potential misconfigurations and security risks in your AWS account However, in the console the result is not clear: A better solution is generate the report in a csv file adding the following command --csv=file.csv. After convert the csv into a table it will look like this: Is it popular?: On github: Watch 70; Fork 575; Stars 2.5kRecently updated?: Yes. Last commit was 3 weeks ago (3449 commits in total)URL: https://github.com/aquasecurity/cloudsploit I didn’t found how to scan only a few regions. In the official documentation only is included how to suppress all one region results ./index.js --suppress *:us-east-1:*. And to exclude more than one region you can use something like this --suppress \"*:ap-*-*:*\" --suppress \"*:af-*-*:*\" ...steampipeI mention this tool in my first article about open source, when I talked about Extend CLI capabilities hereSteampipe is the universal interface to APIs, and we can use SQL to query cloud infrastructure, SaaS, code, logs, and more.Besides, I am going to show you two new capabilities: Check: Ensure that cloud resources comply with security benchmarks such as CIS, NIST, and SOC2. AWS security best practices contains 180 controls CIS v1.50 contains 63 controls Visualize: View prebuilt dashboards or build your own. Why use it: for me, the benchmark of AWS security best practices with 180 controls and the CIS level 1.5 with 63 are enough to consider use this tool to perform a security analysis. Is it popular?: On github: Watch 32; Fork 171; Stars 4.6kRecently updated? Yes, last commit 2 days ago. Total commits 2007URL: https://github.com/turbot/steampipeMore information (checks and dashboard): https://steampipe.io/docs/check/overview https://steampipe.io/docs/dashboard/overview https://aws.amazon.com/blogs/opensource/compliance-auditing-with-steampipe-and-sql/cloudsplainingCloudsplaining is an AWS IAM Security Assessment tool that identifies violations of least privilege and generates a risk-prioritized HTML report.The assessment identifies where resource ARN constraints are not used and identifies other risks in IAM policies: Privilege Escalation Resource Exposure Infrastructure Modificationº Data ExfiltrationCloudMapper helps you analyze your Amazon Web Services (AWS) environments (using python) Why use it: The tool provides you a report with some risk in IAM policies (customer/inline/AWS) and the IAM principals (IAM Users, Groups, and Roles). Is it popular?: On github: Watch 29; Fork 143; Stars 1.6KRecently updated? Yes. Last commit 3 days ago (347 commits in total)URL: https://github.com/salesforce/cloudsplainingSummary: Which tool should I use to perform security assessments on my account? This is my personal opinion. If you have a different one, let me know in the comments at the end of this article! General security assessment: prowler or steampipe Security assessment based on CIS 1.5: steampipe, prowler or cloudsploit Richer tool security assessment based on frameworks: steampipe IAM specific security assessment: cloudsplainingNext stepsThe next article related with open source tools will be: How to help you with your code: to generate it (IaC), validate it (policy as code and compliance) and analyze it (static analysis/credentials) - here" }, { "title": "Getting started with AWS open-source tools", "url": "/posts/aws-open-source-tools/", "categories": "General", "tags": "getting-started, open-source, github", "date": "2023-01-24 20:15:00 +0100", "snippet": "IntroductionThis is the first part of a series of articles about open source software and AWS, in which I will share those tools that I have tested and that I think you may find useful.The complete...", "content": "IntroductionThis is the first part of a series of articles about open source software and AWS, in which I will share those tools that I have tested and that I think you may find useful.The complete series contains the following: Getting started: main open source tools and extend AWS CLI (this article) Analyze your AWS environment: focus on the inventory, analysis and security assessment of the AWS environment - here How to help you with your code: to generate it (IaC), validate it (policy as code and compliance) and analyze it (static analysis/credentials) - hereWhat is open-source?The Open Source Initiative defines here the following criteria for considering software as open source: Free Redistribution Source code must be accessible The license must allow modifications and derived works Integrity of The Author’s Source Code No Discrimination Against Persons or Groups No Discrimination Against Fields of Endeavour Distribution of License License Must Not Be Specific to a Product License Must Not Restrict Other Software License Must Be Technology-Neutral Open source software is code designed to be accessible to the public: anyone can view, modify and distribute the code as they wish.By the way, both terms are correct: open source and open-source. I could use them interchangeably in this article.What is the relationship with AWS?I’m sure you already know that there are many open-source projects related to AWS, too many. This is because AWS is very popular, but also because AWS is very committed to the open source community.AWS claims that open source is good for everyone and regularly develops open source software and contributes to thousands of open-source communities on GitHub, Apache, and the Linux Foundation. More information can be found hereIn this post, I will try to show you some open source projects, since you probably don’t know all of them and you can surely start using some of them. So, let’s start sharing public code!Main AWS open-source toolsLet’s start with the most popular AWS open source projects.You are probably already using some of them and did not realize they are open source. However, I am not going to explain them or give more information, I will just name them here: AWS CLI: Amazon Web Services Command Line Interface https://github.com/aws/aws-cli CDK (Cloud Development Kit): Define cloud infrastructure using familiar programming languages https://github.com/aws/aws-cdk SAM (Serverless Application Model): Framework for building serverless applications https://github.com/aws/aws-sam-cli AWS Amplify: framework and tools for developing mobile and web applications https://github.com/aws-amplify EKS distro: Certified Kubernetes distribution based on and used by Amazon Elastic Kubernetes Service (EKS) to create reliable and secure Kubernetes clusters-https://github.com/aws/eks-distro Karpenter: node provisioning project built for Kubernetes https://github.com/aws/karpenter OpenSearch: A community-driven, search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 &amp; Kibana 7.10.2 https://github.com/opensearch-project Bottlerocket: Linux-based operating system meant for hosting containers https://github.com/bottlerocket-os/bottlerocket Firecracker: Virtual machine monitor (VMM) to create and manage microVMs. Firecracker powers the AWS Lambda service. https://firecracker-microvm.github.io/ Extend AWS CLIMy first idea was named this section CLI tools but all the open source tools listed here are CLI (Command Line Interface) tools, so this section is for the tools that you can use to improve/extend/replace your AWS CLI tool. Security aws-vault: tool to securely store and access AWS credentials in a development environment Extend AWS CLI aws-shell: interactive productivity booster for the AWS CLI awsls: a list command for AWS resources steampipe: Use SQL to query cloud infrastructure, SaaS, code, logs, and more ohmyzsh with the aws plugin: provides completion support for AWS CLI and a few utilities to manage AWS profiles and display them in the prompt Logs awslogs: a simple command line tool for querying groups, streams and events from Amazon CloudWatch logs aws-vaultaws-vault securely store and access AWS credentials in a development environment.AWS Vault stores IAM credentials in your operating system’s secure keystore and then generates temporary credentials from those to expose to your shell and applications. It’s designed to be complementary to the AWS CLI tools, and is aware of your profiles and configuration in ~/.aws/config. Why use it: Complementary tool for AWS CLI tools, for secure your connections protecting your credentials. With the last command, the AWS Console will be open and you will be logged!Is it popular? Yes, GitHub statistics: Watch 119; Fork 725; Stars 7.1kRecently updated? Yes, 1096 commits, last 2 weeks agoURL: https://github.com/99designs/aws-vault In my opinion, this tool is a must-have for securing your credentials.aws-shellThe interactive productivity booster for the AWS CLI Why use it: AWS CLI is awesome but maybe you don’t know the commands. With aws-shell, you have a helper and as you type you can visually see the available options: Is it popular? Yes, GitHub statistics: Watch 230; Fork 755; Stars 6.8kRecently updated? No, the project seems abandoned (last commit Oct 7, 2020). Total commits 235URL: https://github.com/awslabs/aws-shell The tool is not working with AWS CLI v2 (here is the official Issue in github), and the project seems to have been abandoned (last commit July 10, 2020). However, if you use v1, the tool is worth it because it contains all the core AWS services.awslsA list command for AWS resources. More than 100 AWS resource supported. The goal is to code-generate a list function for every AWS resource that is covered by the Terraform AWS Provider (currently over 500) Why use it: If you want to search for resources across multiple regions and/or accounts and filter by any value using GREP, this is the tool for you! Is it popular? Yes, GitHub statistics: Watch 10; Fork 51; Stars 763Recently updated? No, one year from the last update (Feb 13, 2022), with 91 commits in total.URL: https://github.com/jckuester/awsls Although it has not been updated recently, it is worth using for its ability to search multiple accounts and filter using the GREP command.steampipeSteampipe is the universal interface to APIs. Use SQL to query cloud infrastructure, SaaS, code, logs, and more. Why use it: Using SQL you can query AWS resources, perform join queries (same account, several accounts, between different sources), and you have a helper to perform the queries. All in one! Is it popular?: On github: Watch 32; Fork 171; Stars 4.6kRecently updated? Yes, last commit 2 days ago. Total commits 2007URL: https://github.com/turbot/steampipeMore information (querying aws resources): https://steampipe.io/docs/query/overview https://aws.amazon.com/blogs/opensource/querying-aws-at-scale-across-apis-regions-and-accounts/ I like this tool! If I want to get specific information in AWS using a CLI tool this is my first choice to do so, I recommend you to try it!awslogsawslogs is a simple command line tool for querying groups, streams and events from Amazon CloudWatch logs. Why use it: If you want to review your logs with console this is your tool. You can filter start/end and also using GREP! An example: Is it popular?: On github: Watch 61; Fork 326; Stars 4.5kRecently updated? No, the project seems to have been abandoned. The last commit was July 10, 2020. Total commits 326URL: https://github.com/jorgebastida/awslogs The tool is not updated but if you want to query the CloudWatch logs, this is your tool!ohmyzsh with the aws pluginIf you are using ohmyzsh (framework for managing your zsh configuration), you can add this aws plugin to extend your CLI.This plugin provides completion support for AWS CLI and a few utilities to manage AWS profiles and display them in the prompt. Why use it: Useful if you are using different profiles and ohmyzsh. Is it popular?: ohmyzsh is a very popular framework for zsh (155k stars), and the AWS plugin is part of this framework.Recently updated? Yes. The aws plugin was updated 2 weeks agoURL: https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/awsNext stepsAs I said in the introduction, there are 2 more articles in this series of open source tools: Analyze your AWS environment: focus on the inventory, analysis and security assessment of the AWS environment - here How to help you with your code: to generate it (IaC), validate it (policy as code and compliance) and analyze it (static analysis/credentials) - here" }, { "title": "DeepRacer - First steps with Machine Learning", "url": "/posts/deepracer-first-steps-with-machine-learning/", "categories": "Machine learning", "tags": "getting-started, deepracer, personal", "date": "2022-12-06 19:33:00 +0100", "snippet": "IntroductionYes! Now that artificial intelligence bots are here to help everyone create content (and much more)… and since I like to do just the opposite, I’ve written this article from a personal ...", "content": "IntroductionYes! Now that artificial intelligence bots are here to help everyone create content (and much more)… and since I like to do just the opposite, I’ve written this article from a personal point of view based on my experience with DeepRacer at the last re:Invent 2022.In short, in DeepRacer you have to train your car to stay on track… and it won’t be something you’ll achieve quickly.But eventually, you will be able to complete a lap!Why did I decide to learn how DeepRacer works?At the AWS Summit Madrid 2022, my first AWS in-person event, I discovered DeepRacer, the service, the car and the track, but I didn’t have enough time to try it out or know how it worked, and I told myself I would take the next opportunity to learn more about it.And this opportunity came early! I had the pleasure of going to re:Invent this very 2022, and I took some time to do a DeepRacer Level 200 Introductory Workshop. I created my first model and ran my first race!Learn, practice and have fun!!ConceptsAWS DeepRacer is a fully autonomous 1/18th scale racing car that can be used to learn about reinforcement learning (RL) and participate in global racing leagues. “A fun way to learn machine learning” said AWS in their webpageIt was announced at AWS re:Invent 2018, as part of a suite of new Amazon Web Services (AWS) machine learning (ML) products and services, including Amazon SageMaker, Amazon Lex and Amazon Rekognition.Machine learning is a field of artificial intelligence (AI) that focuses on the development of algorithms that allow computers to learn and make decisions based on data, rather than being explicitly programmed. Machine learning algorithms are designed to improve their performance over time as they are exposed to more data.Exists different types of machine learning: Supervised: labelled data, where the correct output is provided for each example of the training set (dataset). The model uses this supervision to learn how to make predictions on its own. Unsupervised: unlabelled data, where the correct output is not provided and the model must find patterns or relationships in the data on its own. Reinforcement: train an agent (car) to make decisions based on the current state (camera images) to achieve specified goals and maximize a reward in an environment (circuit). The model is trained through trial and error, to learn how to take actions that maximize the reward Reward positive behavior Don’t reward negative behavior Probably the most important thing in reinforcement learning is to design the reward function so that the agent (car) makes better decisions.This is the association with the reinforcement learning key terms in AWS DeepRacer: Agent: car State: camera images Environment: circuit Action: move at a particular speed and steering angle Reward (number, +100) –&gt; CORE of the reinforcement learning Episode: journeyHow to train your model?To get started with AWS DeepRacer, you need an AWS account and access to the DeepRacer service. Once there, you can access the DeepRacer console and begin training your ML model. As always, the AWS console contains a lot of good documentation and direct links to learn much more.Before starting: you have to create your racer profile - just complete the forms and choose your avatar. then, in the Garage, you will create and customize the virtual cars that you will then use to train models. finally, you can start to configure a new model.This image represents the general concept of how the training model works Create a model Train Evaluate Model iteration and UpskillBut of course, first of all, you have to learn the basics! The 10-minute course inside the console is highly recommended and very visual! You will learn how reinforcement learning works and you will be ready to start “playing” with the DeepRacer console.Your first model should be the easy one, the one with all the default options. Only with that, you should be able to complete the race. By the way, this is also the first option recommended by AWS. If you try to change default options without knowledge it would possibly be worse than leaving the default options.These are the 5 steps to create a new model: 1: Specify the model name and environment 2: Choose race type and training algorithm race types: Time trial, Object avoidance and Head-to-head racing 3: Define action space 4: Choose a vehicle 5: Customize the reward function The recommended time to complete the training is 1 hour, but in general, more time means that the agent (car) will gain more experience and learn as you have defined in the reward function. The last step will be to define the reward function, but fortunately, there are 4 different options you can use in the console: Time trial - follow the center line (Default) Time trial - stay inside the two borders Time trial - prevent zig-zag Object avoidance and head-to-head - stay on one lane and not crash (default for OA and h2h) You should test which options are best for your specific circuit, but my recommendation is…. trial and error! You have the limitation of training 4 models at the same time: create one model with the default options and 3 more with some other changes… then when train ends, analyze the results, finally, use the better model (in graph and in times) to clone it and try again with other 4 different options How do you know if one model is better than the previous one?When a model finishes a training session, it has to be evaluated, and you can do it several times because the lap times are not always the same!You can also review the training graph. Performance improvement should increase over time:But sometimes you will see something like this:It means: Over-training: the model does not improve over time Or not a good combination of optionsIn any case, if you see that your graph decreases and the “best model” remains at the beginning, you can stop this training and try a different combination, because this one is not working well! Be careful! Costs can rise quickly when you’re having fun. I know… but since I get a voucher (credits) it was not a problem…. However, with the free level you have 10 hours and it’s enough to test the service. If you want to save money, you should consider train your models in your local environment and then import to the DeepRacer console to evaluateSome of my testsMy option 1: The default training modelMy option 2: using SAC option (huge difference)Many more trials and errors…And the final result, after many hours and different attempts:This is not a big deal, but for me, it was worth it!And just for participating, I received this car!More information and the next steps What is DeepRacer More information about the car device Wiki with concepts AWS DeepRacer Community Train models in local Use logs to do analysis" }, { "title": "AWS Control Tower Deep Dive", "url": "/posts/aws-control-tower-deep-dive/", "categories": "Multi-account", "tags": "multi-account, deep dive, control tower", "date": "2022-10-09 00:30:00 +0200", "snippet": "Introduction This is the second article of AWS Control Tower and the AWS multi-account approach. In the first article I had explained why you need a multi-account approach, what are the best pract...", "content": "Introduction This is the second article of AWS Control Tower and the AWS multi-account approach. In the first article I had explained why you need a multi-account approach, what are the best practices and recommendations and how to create the landing zone using AWS Control Tower.In this article, we will continue to add new AWS accounts to our multi-account approach and review all options in the AWS Control Tower service.Once the creation of the landing zone by the AWS control tower is completed, it will have the following organizational structure:Now, we will review all the options available in AWS Control Tower.Single Sign-OnWe have more than one account and need/want a central place to access all accounts. Fortunately, AWS Control Tower provides us with a default configuration of the AWS Single Sign-On solution (IAM Identity Center, previously named AWS SSO). IAM Identity Center expands the capabilities of AWS Identity and Access Management (IAM) to provide a central place that brings together administration of users and their access to AWS accounts and cloud applications.The default configuration used as identity source is Identity Center directory, that is, an internal directory where we can add users directly to give them access to the organization.SettingsThis is the default configuration: You can update the identity source of the IAM Identity Center configuration to use your Active Directory or an external identity provider.UsersBy default, only the email used for the root account is included as a user, but you need to confirm it and set a new password. This is a new user different from the root user of the AWS management account.To add a new user with this configuration the following information is mandatory: Username (required for this user to sign in to the AWS access portal. The username can’t be changed later.) Email address First name Last name Display name (typically the full name of the workforce user, is searchable, and appears in the user’s list)GroupsBy default, the following groups are created: AWSAccountFactory: Read-only access to account factory in AWS Service Catalog for end users AWSSecurityAuditors: Read-only access to all accounts for security audits AWSSecurityAuditPowerUsers: Power user access to all accounts for security audits AWSServiceCatalogAdmins: Admin rights to account factory in AWS Service Catalog AWSControlTowerAdmins: Admin rights to AWS Control Tower core and provisioned accounts AWSLogArchiveAdmins: Admin rights to log archive account AWSAuditAccountAdmins: Admin rights to cross-account audit account AWSLogArchiveViewers: Read-only access to log archive accountUse of SSOYou will access through one link similar to this “https://xxxxxxxxxxxxx.awsapps.com/start” and you will access to this portal:When you successfully log in, you will see the following:Clicking on the AWS account menu (number) will display information about these accounts:New Organizational Units (OUs)Creating a new Organizational Unit (OU) is very easy from the AWS Control Tower or AWS Organizations. From AWS Control Tower: automatically will be registered in Control Tower service From AWS Organizations: you need to register the OU in Control Tower So, if you are using AWS Control Tower, better use it instead of AWS Organizations to create new OUs (Organizational Units) or AWS accounts.Account Factory With the account factory you can provision new accounts and enrols existing accounts, and you can standardize your account and network configurations for creating multiple accounts.You can update the AWS Control Tower service to the Network configuration to define how VPCs will be created. When you save the changes, the defined configurations will be published to AWS Service Catalog as a product.To deploy new AWS accounts with Account Factory you have 2 options: Use the product in AWS Service Catalog: Use Create accounts in the Account Factory feature of AWS Control Tower In both cases, the mandatory fields are: Account email Display Name Identity Center user email IAM Identity Center user name Organizational unitAlso, in both cases, the Service Catalog is used behind the scenes.More information: Automate the creation of multiple accounts with Control TowerGuardrails (Controls)AWS Control Tower applies high-level rules, called guardrails, that help enforce your policies using service control policies (SCPs), and detect policy violations using AWS Config rules. 07 May 2023: AWS Control Tower has updated recently the controls.AWS Control Tower has now 358 controls (governance rules for your AWS environment) in 3 different categories of guidance. Mandatory: always enforced 20 preventive guardrails to enforce policies 3 detective to detect configuration violations Strongly recommended: designed to enforce some common best practices for well-architected, multi-account environments. Example: Detect whether public write access to Amazon S3 buckets is allowed Elective: enable you to track or lock down actions that are commonly restricted in an AWS enterprise environment.Now, is also important distinct the controls by behaviour: Proactive These controls only are available if you deploy CloudFormation templates in the accounts and Regions where the control has been activated. It is implemented through AWS CloudFormation hooks and guard rules, and enforced through the deployment of a CloudFormation template. A guard rule is a policy-as-code rule that expresses the compliance requirements for an AWS resource. Hooks proactively inspect these resource configurations by comparing AWS resources against the guard rule, before the resources are provisioned. Detective These controls are owned by AWS Security Hub or AWS Control Tower and implemented using AWS Config rules Controls owned by AWS Security Hub are not aggregated in the compliance status of accounts and OUs in AWS Control Towe Preventive These controls are implemented with Service control policy (SCP). When activated, preventive controls are enforced at the OU level. I recommend to enable these controls in all the OUs:All: Disallow actions as a root user Control objective: Enforce least privilege Guidance: Strongly recommended Behaviour: Preventive All: Disallow creation of access keys for the root user Control objective: Enforce least privilege Guidance: Strongly recommended Behaviour: Preventive If possible: Deny access to AWS based on the requested AWS Region Control objective: Protect configurations Guidance: Elective Behaviour: Preventive Enable controlsSo, there are 23 mandatory guardrails, but there many others that you can enable. To enable one of them you select, click on Enable control on OU and then select one of them.Infrastructure as Code (IaC)There are at least 2 solutions to define using Infrastructure as Code (IaC) your AWS resources or SCPs and deploy it on each account (new or existing). Customizations for Control Tower (CfCT): use CloudFormation Use the Terraform module of Control Tower Account Factory for TerraformCustomizations for Control Tower (CfCT) Here is the official documentation about CfCT and here is the public template that you need to install in the CloudFormation service.Customizations for AWS Control Tower (CfCT) helps you customize your AWS Control Tower landing zone and stay aligned with AWS best practices. Customizations are implemented with AWS CloudFormation templates and service control policies (SCPs).Deploying CfCT builds the following environment in the AWS Cloud:CfCT deploys two workflows: an AWS CodePipeline workflow (executed when changes appear) an AWS Control Tower lifecycle event workflow (executed when a new account is launched)The customizations-for-aws-control-tower.template deploys the following: An AWS CodeBuild project An AWS CodePipeline project An Amazon EventBridge rule AWS Lambda functions An Amazon Simple Queue Service queue An Amazon Simple Storage Service bucket with a sample configuration package AWS Step FunctionsTo deploy CfCT: Step 1: Launch the stack in CloudFormation here. You can choose S3 or CodeCommit as the source Step 2: connect to the source and perform some changes (CodePipeline will be deployed automatically when you upload the new changes) Manage AWS Accounts Using Control Tower Account Factory for TerraformThere is a Terraform module that makes it easy to create and customize new accounts that comply with your organization’s security guidelines.Here is the official link that explains how to do it step by step." }, { "title": "Getting started with AWS Multi-account approach", "url": "/posts/multi-account-approach/", "categories": "Multi-account", "tags": "getting-started, multi-account", "date": "2022-09-17 19:22:00 +0200", "snippet": "IntroductionThis is the first article related to the multi-account approach, and the first thing to review is to know when and why I need a multi-account solution.When do you need a multi-account s...", "content": "IntroductionThis is the first article related to the multi-account approach, and the first thing to review is to know when and why I need a multi-account solution.When do you need a multi-account solution?If you are using AWS for your personal projects and you have a simple solution without many resources, it is not worth it. Or maybe your solution is not so simple but you don’t want to worry about the additional operational overhead to manage it. I understand it. This was exactly my case but in this article, I want to show you when, why and how to deploy a multi-account solution.However, in enterprise solutions, you should have a multi-account solution, without a doubt, so you need to know about it.Then, for me, it is very simple: Personal account with simple solution - not necessary multi-account solution Personal account with no simple solution or many workloads - depends on you Small enterprise solution - depends on you, but even if your company is small is a good idea to take advantage of the multi-account approach Medium/Big enterprise solution - mandatoryWhat AWS says? While you may begin your AWS journey with a single account, AWS recommends that you “set up multiple accounts, as your workloads grow in size and complexity”. Extracted from here Also, “If you have a single account today and either have production workloads in place or are considering deploying production workloads, we recommend that you transition to the use of multiple accounts so that you can gain the Benefits of using multiple AWS accounts.” Extracted from hereWhy do you need a multi-account solution?The main reason to do it is isolation. An AWS account serves as a resource container and the easiest way to isolate resources is by using different accounts.There are many reasons to use isolations: Environment isolation (testing/production): Apply distinct security controls by environment Business units/workloads/applications/products isolation: Group workloads based on business purpose and ownership Functional team isolation Data isolation: Constrain access to sensitive data. This category includes data stores, which may be necessary because of legal restrictionsSome other advantages of a multi-account solution are: to manage security by using policy-based controls to simplify billing - You can use a tagging strategy and cost allocation tags to track your AWS costs on a detailed level in the same account but is much better to use multiple accounts to do the same. Promote innovation and agilityThe disadvantage of a multi-account solutionThe main disadvantage if you want a multi-account solution is the management of all accounts (do it in the same way, create new accounts, duplicate efforts,… this is, mismanagement).Multi-account approach: AWS OrganizationsAWS provides a resource to help you centrally manage and govern your environment: AWS OrganizationsCore concepts Organization: an entity to administer accounts as a single unit Account: standard AWS account that contains your AWS resources Organizational Units (OUs): a container that helps you to organize your accounts into a hierarchy and make it easier to apply management controls Secure Control Policies (SCPs): policies are used to limit what the principals in member accounts can doFeatures / Advantages Free to use Centralized management of your AWS accounts Hierarchical grouping of your accounts (in Organizational Units - OUs) Access control (IAM Identity Center, successor to AWS Single Sign-On) Permission Management (apply Service Control Policies - SCPs) Consolidated billing for all member accounts Configure supported AWS services to perform actions in your organization.AWS Services integrationThis is the full list of supported service integration with AWS Organizations service. I have included all the services because is important to know that AWS Organizations can help with many integrations and automatization in the new accounts. Amazon Detective (makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities) Amazon DevOps Guru (analyze operational data and identify behaviors that deviate from normal operating patterns) Amazon GuardDuty (help to identify unexpected and potentially unauthorized or malicious activity in your AWS environment) Amazon Inspector (automated vulnerability management service that continually scans Amazon EC2 and container workloads for software vulnerabilities and unintended network exposure) Amazon Macie (discover, classify, and help you protect your sensitive data in Amazon S3) Amazon VPC IP Address Manager (makes it easier for you to plan, track, and monitor IP addresses for your AWS workloads) Artifact (provides on-demand downloads of AWS security reports) AWS Account Management (allow you to programmatically modify your account information and metadata) AWS Audit Manager (helps you continuously audit your AWS usage) AWS Backup (set policies to enforce automatic backups) AWS Control Tower (straightforward way to set up and govern an AWS multi-account environment, following prescriptive best practices) AWS Health (visibility into your resource performance and the availability of your AWS services and accounts) AWS Marketplace - License Management AWS Network Manager (centrally manage your AWS Cloud WAN core network and your AWS Transit Gateway network) AWS Trusted Advisor (inspects your AWS environment and makes recommendations) CloudFormation StackSets (enables you to create, update, or delete stacks across multiple accounts and regions with a single operation) CloudTrail (enable governance, compliance, and operational and risk auditing of your AWS account) Compute Optimizer (recommends optimal AWS compute resources based on an analysis of the historical utilization metrics of your resources) Config (enables you to assess, audit, and evaluate the configurations of your AWS resources) Directory Service (share your AWS Managed Microsoft AD directories between other trusted AWS accounts in your organization) Firewall Manager (simplify AWS WAF administration and maintenance) IAM Access Analyzer (helps you set, verify, and refine permissions to grant the right fine-grained permissions) License Manager (brings software vendor licenses to the cloud) RAM (share AWS resources) S3 Storage Lens (aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console) Security Hub (provides you with a comprehensive view of the security state of your AWS resources) Service Catalog (enables you to create and manage catalogs of IT services that are approved for use on AWS) Service Quotas (enables you to view and manage your quotas from a central location) Single Sign-On (centrally provide and manage single sign-on access to all your AWS accounts) Systems Manager (enables visibility and control of your AWS resources) Tag policies (help you standardize tags across resources)Recommended organization structure (best practices) The following recommendations need to be adapted to each Organization. Each case is different and you have to think about how grouping accounts and how many OUs you need.AWS recommends that you start with the central services in mind: Security: Used for centralized security tooling and audit resources. Usually with at least 2 accounts (one for each of mentioned purposes) Infrastructure: You can use this level to share infrastructure with the other accounts: i.e. networking and IT servicesOnce you have your foundational Organizational Units, is time to think about the environments, and separate SDLC (non-production) from production because usually have different requirements and you want each of them isolated. Although it can be a new OU categorization, is common to use it as a sub-categorization inside another OU level (for example Security and Infrastructure would have another 2 levels: SDLC and Prod). EDIT: SDLC (Software Development Life Cycle) is used here because this article is based in the AWS recommendations and they use it for the name of one OU to include all the non-productive environments, for example here and here. Btw, I “introduce” the concept of SDLC when I explain how to add CI/CD to my CDK project in this other article. Please, if you found some error there, please let me know!Now, we have to think about all the additional OUs that you need. AWS recommends creating OUs that directly relate to building or running your products or services: Sandbox Workloads Deployments Suspended Transitional Exceptions …More information in the official AWS links: https://aws.amazon.com/organizations/getting-started/best-practices/?org_console https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/recommended-ous-and-accounts.html Best practices for management account and for member accounts Basic organizationGet Started with AWS OrganizationsAWS Organizations is the service to help us with central management for multiple AWS accounts.To start you only need an AWS account, access to the AWS Organizations and then click “Create an organization”:When you do it, you could view the Organizational Structure of your Organization, and you will have in the left section of the screen the available options: send Invitations, configure supported services, enable policies, the Organization settings and a Get started option.The Get started option is interesting as you can view the 4 steps to “Build your organization”: Create accounts or invite existing accounts Organize your organization’s member accounts into OUs Create policies Enable AWS services that support AWS OrganizationsAnd also, a simplified recommended organizational structureSo perfect, now we know how to create an AWS Organization, we can create the different OUs we need, we can create new Accounts and send them an invitation to join our organization, and then configure the enabled services and apply the policies.Easy? Not really… you can do it all manually if you need heavy customization, but it seems necessary to apply automation for efficient governance and management, and probably we need help to set up our landing zone. A landing zone is a well-architected, multi-account AWS environment that is a starting point from which you can deploy workloads and applications.Fortunately, AWS has another service to help us: AWS Control Tower.AWS Control Tower AWS Control Tower offers the easiest way to set up and govern a new, secure, compliant, multi-account AWS environment. It establishes a landing zone, which is a well-architected, multi-account environment based on best-practice blueprints, and enables governance using guardrails you can choose. Guardrails are SCPs and AWS Config rules that implement governance for security, compliance, and operations. Q: What is the difference between AWS Control Tower and AWS Organizations? AWS Control Tower offers an abstracted, automated, and prescriptive experience on top of AWS Organizations. It automatically sets up AWS Organizations as the underlying AWS service to organize accounts and implements preventive guardrails using SCPs. Control Tower and Organizations work well together. You can use Control Tower to set up your environment and set guardrails, then using AWS Organizations, you can further create custom policies (such as tag, backup or SCPs) that centrally control the use of AWS services and resources across multiple AWS accounts. Extracted from the FAQS of AWS OrganizationsNow, we are going to create our Landing Zone using AWS Control Tower.First, access the service and click on Set up landing Zone:Then, review the first step, set the Home region, if you want to deny access to any Region in which AWS Control Tower is not available and your additional Region for governance:Now set a name for the Foundational Organizational Unit (Security, with 2 accounts: log archive account and security audit account) and the Additional OU (Sandbox):Then configure the shared accounts (Foundational OU - security), set an email associated with the new accounts and a name for these 2 accounts: Log archive: The log archive account is a repository of immutable logs of API activities and resource configurations from all accounts. Security audit: The audit account is a restricted account. It allows your security and compliance teams to gain access to all accounts in the organization.Step 4 is to configure the CloudTrail and the encryption (disabled by default)Finally, you need to review all the information and confirm. Next, a new screen will be shown with the progress of the creation of the landing zone. The estimated time is 60 minutes. This is the summary of what is being set up: 2 organizational units, one for your shared accounts and one for accounts that will be provisioned by your users 2 new accounts, for log archive and security audit A native cloud directory with preconfigured groups and single sign-on access 20 preventive guardrails to enforce policies 3 detective guardrails to detect configuration violations With a little patience, you will receive the confirmation of everything was ok, and the setup of the landing zone will be complete:The new structure created is very simple:And a new AWS Identity Center configuration (previously AWS SSO) has been configured for you and now you can access this portal to access all your AWS accounts:Clean upTo clean up the AWS Control Tower you need to decommission it and then wait until 2 hours to remove all resources.Next steps Next post: AWS Control Tower Deep Dive Comment this post More information Practice: Workshop of multi-account security governance workshop Practice: AWS Control Tower Immersion day Example of SCPs AWS blog article: migrate AWS Accounts to AWS Control Tower AWS blog article: Migrate resources between accounts Landing Zone accelerator on AWS Dependency checker to migrate between AWS Organizations " }, { "title": "Amazon GuardDuty: Deep dive", "url": "/posts/amazon-guardduty-deep-dive/", "categories": "Security", "tags": "security, guardduty, deep dive", "date": "2022-06-19 15:30:00 +0200", "snippet": "IntroductionAmazon GuardDuty is a continuous security monitoring service that analyzes and processes the following data sources: AWS CloudTrail management event logs, AWS CloudTrail data events f...", "content": "IntroductionAmazon GuardDuty is a continuous security monitoring service that analyzes and processes the following data sources: AWS CloudTrail management event logs, AWS CloudTrail data events for S3, DNS logs, EKS audit logs, and VPC flow logs.It uses threat intelligence feeds, such as lists of malicious IP addresses and domains, and machine learning to identify unexpected and potentially unauthorized and malicious activity within your AWS environment. Enabling this service is a MUST and a quick win to improve your security posture. With one-click Amazon GuardDuty reduces risk using intelligent and continuous threat detection of your AWS accounts, data, and workloads.Getting started GuardDuty is a Regional service, meaning any of the configuration procedures you follow must be repeated in each region that you want to monitor with GuardDuty.AWS highly recommend that you enable GuardDuty in all supported AWS Regions. This enables GuardDuty to generate findings about unauthorized or unusual activity even in Regions that you are not actively using.The first step to using GuardDuty is to enable it in your account. Once enabled, GuardDuty will immediately begin to monitor for security threats in the current region.Multi-account environmentWhen you use GuardDuty with AWS Organizations, you can designate any account within the organization to be the GuardDuty delegated administrator. Only the organization management account can designate GuardDuty delegated administrators.An account that is designated as a delegated administrator becomes a GuardDuty administrator account has GuardDuty automatically enabled in the designated Region and is granted permission to enable and manage GuardDuty for all accounts in the organization within that Region.More informationStandalone account environmentEnable GuardDuty is a one-click action:Estimating costsPricing (USD), Per account, per month, per Region AWS CloudTrail Management Event Analysis   Per one million events / month $4.00 per one million events AWS CloudTrail S3 Data Event Analysis   First 500 million events / month $0.80 per one million events Next 4,500 million events / month $0.40 per one million events Over 5,500 million events / month $0.20 per one million events Amazon EKS Audit Logs   There is no data in the official page There is no data in the official page VPC Flow Log and DNS Query Log Analysis   First 500 GB / month $1.00 per GB Next 2,000 GB / month $0.50 per GB Next 7,500 GB / month $0.25 per GB Over 10,000 GB / month $0.15 per GB You can use the GuardDuty console and API operations to estimate how much GuardDuty will cost you. During the 30-day free trial period, the cost estimation projects what your estimated costs will be after the free trial period.More informationHands-on with Amazon GuardDutySample findingsGuardDuty supports generating sample findings with placeholder values, which can be used to test GuardDuty functionality and familiarize yourself with findings before needing to respond to a real security issue discovered by GuardDuty.These are the generated GuardDuty findings:Filtering findingsWe can filter the results by many criteria using the filter bar or we can filter using the colored buttons in the upper right section.You could also save your custom filters and using later.Suppression rulesIf you are receiving findings for expected behavior in your environment, you can automatically archive findings based on criteria you define with suppression rules. Suppression rules are rules which automatically send matched findings to archive.There are some cases where it is desirable to use suppression rules, to facilitate the recognition of security threats with the greatest impact on your environment: low-value findings false-positive findings threats you do not intend to act on Suppressed findings are not sent to AWS Security Hub, Amazon S3, Detective, or CloudWatch, reducing finding noise level if you consume GuardDuty findings via Security Hub, a third-party SIEM, or other alerting and ticketing applications.More information about suppression rulesFinding informationSelect a finding from the table to view its details. In the finding details pane you can see all the information associated with the finding:It contains the information related to the finding: overview section, resource affected, action, actor and additional information.Amazon GuardDuty integrates with Amazon Detective and if you click on the Investigate with Detective link you can view more information and the links to the Detective service to investigate it.More information about finding detailsCustom responses with EventBridgeGuardDuty creates an event for EventBridge when any change in findings takes place (newly generated findings or newly aggregated findings).By using EventBridge events with GuardDuty, you can automate tasks to help you respond to security issues revealed by GuardDuty findings.More informationSubscribing to Amazon SNS GuardDuty announcementsAn Amazon SNS topic is created by GuardDuty (you cannot view it from the SNS console) and you can subscribe to receive notifications about newly released finding types, updates to the existing finding types, and other functionality changes.More information about how to do itRemediating security issuesThese are the recommended actions to remediate these scenarios.Remediating a compromised EC2 instanceThis is the information found in the official AWS documentation: Investigate the potentially compromised instance for malware and remove any discovered malware If you are unable to identify and stop unauthorized activity on your EC2 instance, AWS recommends that you terminate the compromised EC2 instance and replace it with a new instance as neededHowever, I think that this is more appropriate to deal with compromised EC2 instances: Lock the instance down Take the EBS snapshot Perform a memory dump Perform Forensic Analysis Terminate the instanceRemediating a compromised S3 Bucket Identify the affected S3 resource Identify the source of the suspicious activity and the API call used Determine whether the call source was authorized to access the identified resource. If the access was authorized, you can ignore the findingRemediating compromised AWS credentials Identify the affected IAM entity and the API call used Review permissions for the IAM entity Determine whether the IAM entity credentials were used legitimately. If you confirm that the activity is a legitimate use of the AWS credentials, you can ignore the GuardDuty findingRemediating Kubernetes security issues discovered by GuardDutyMore informationGood to know Regional service 30 days free trial when you enable it the first time, and during the free trial GuardDuty provides an estimate of what the spend would be, so you can assess your spending beyond the free trial. All findings are stored in GuardDuty for 90 days. GuardDuty recommends setting up findings export, which allows you to export your findings to an S3 bucket for indefinite storage. The events are delivered to EventBridge in near-real-time and on a best-effort basis. Amazon GuardDuty consumes CloudTrail management and S3 data events directly from CloudTrail through an independent and duplicative stream of events (no additional cost) Global events in CloudTrail (IAM, AWS Security Token Service, Amazon S3, Amazon CloudFront, and Route 53) are delivered to any trail that includes global services and are logged as occurring in the US East (N. Virginia) Region. When you enable GuardDuty, it immediately starts analyzing your VPC flow logs data. It consumes VPC flow log events directly from the VPC flow logs feature through an independent and duplicative stream of flow logs, so Flow logs for VPCs do not need to be turned on to generate findings. All findings are dynamic, meaning that, if GuardDuty detects new activity related to the same security issue it will update the original finding with the new information, instead of generating a new finding Finding types (4): EC2, IAM, S3 and Kubernetes. The official documentation has a full explanation about each category and each finding inside, you can find it here GuardDuty lists allow you to customize the publicly routable IP addresses that GuardDuty generates findings for. You can create a Trusted IP list and a Threat list.More information about GuardDuty Official documentation Amazon GuardDuty Workshop" }, { "title": "AWS Security Hub: Deep dive", "url": "/posts/aws-security-hub-deep-dive/", "categories": "Security", "tags": "security, security hub, deep dive", "date": "2022-06-17 23:20:00 +0200", "snippet": "IntroductionAWS Security Hub is a cloud security posture management service that: Reduced effort to collect and prioritize findings performs automatic security checks against best practices and s...", "content": "IntroductionAWS Security Hub is a cloud security posture management service that: Reduced effort to collect and prioritize findings performs automatic security checks against best practices and standards aggregates your security alerts in a consolidated view of findings across accounts and providers enables automated remediationSecurity Hub enables you to understand your overall security posture via a consolidated security score across all of your AWS accounts, and automatically assesses the security of your AWS accounts resources via the security standards available: AWS Foundational Security Best Practices CIS AWS Foundations Benchmark PCI DSS Enabling this service is a MUST and a quick win to improve your security posture: just by enabling some security standard you will receive security alerts (findings), and the service will aggregate your security alerts automatically. Before you can enable Security Hub standards and controls, you must first enable resource recording in AWS Config. You must enable resource recording for all of the accounts and in all of the Regions where you plan to enable Security Hub standards and controls,How it worksTo maintain a complete view of your security posture on AWS, you need to integrate multiple tools and services including Threat detections from Amazon GuardDuty, vulnerabilities scan from Amazon Inspector, publicly accessible and cross-account resources from IAM Access Analyzer, sensitive data classifications from Amazon Macie, resources lacking WAF coverage from AWS Firewall Manager, resource configuration issues from AWS Config, and AWS Partner Network Products.More informationGetting startedWhether an account needs to enable AWS Security Hub manually depends on how the accounts are managed.Multi-account environmentIf you use the integration with AWS Organizations: The organization management account chooses a Security Hub administrator account. Security Hub is enabled automatically for the chosen account. See Designating a Security Hub administrator account. The Security Hub administrator account enables organization accounts as member accounts. Those organization accounts also have Security Hub enabled automatically. See Managing member accounts that belong to an organization. The only organization account for which Security Hub is not enabled automatically is the organization management account. The organization management account does not need to enable Security Hub before it designates the Security Hub administrator account. The organization management account must enable Security Hub before it is enabled as a member account.Standalone account environment Accounts that are not managed using the Organizations integration must enable Security Hub manually.To start using AWS Security Hub, it only takes a few clicks from the Management Console to start adding findings and performing security checks.As you can see in the image, you only need to check the Security Standard you want to enable and then enable de service. Additionally, you can download a CloudFormation template to deploy it as StackSet. After you enable a security standard, AWS Security Hub begins to run all checks within two hours but it can take up to 24 hours for the Security standards pane to populateEstimating costsPricing (USD), Per account, per month, per Region Security checks   First 100,000 $0.0010/check 100,001 – 500,000 $0.0008/check 500,001 + $0.0005/check Finding ingestion events   First 10,000 Free 10,001 + $0.00003/finding It includes ingestion of updates to existing findings. Finding ingestions for Security Hub security checks is free.Within the Security Hub service, under the Settings Usage tab, you can find your monthly estimate:Hands-on with AWS Security HubSummaryOn the Summary page: Security standards show you your security score (total and by standard), and the resources with the most failed security checks Findings by Region summarize the number of active findings for each severity across Regions. The counts only include findings that have a workflow status of NEW or NOTIFIED. Insights: An AWS Security Hub insight is a collection of related findings. It identifies a security area that requires attention and intervention.Security StandardsThis page shows you the 3 security standards and whether you have them enabled (with their security score) or not (you can enable them with a one-click action).If you access one of them, you will be able to see more details about how many security controls are enabled, failed, passed, disabled… and a table with information about compliance status, severity, ID, title and Failed checks:FindingsThis page shows all the findings, the ones generated by AWS Security Hub with Security Standards and all the rest of the other tools and services.In the following image, you can view Security Hub and GuardDuty findings:Finding aggregation (cross-account &amp; cross-region)With finding aggregation, you can use a single aggregation account &amp; Region to view and update findings from multiple linked accounts &amp; Regions. Administrator accounts configure the aggregation. Security Hub replicates findings and finds updates for all of the member accounts in the linked Regions. You can connect multiple AWS accounts and consolidate findings across those accounts You can also designate an aggregator Region and link some or all Regions to that aggregator Region to give you a centralized view of all your findings across all your accounts and all your linked RegionsInteresting article in AWS Security Blog: Best practices for cross-Region aggregation of security findings. Best practices mentioned: Enable cross-Region aggregation Consolidating downstream SIEM and ticketing integrations Auto-archive GuardDuty findings associated with global resources Reduce AWS Config cost by recording global resources in one Region Disable AWS Security Hub AWS Foundational Best Practices periodic controls associated with global resources Implement automatic remediation from a central RegionYou also can show your AWS Security Hub findings in a view of data to enable decision-makers to assess the health and status of an organization’s IT infrastructure at a glance. This article contains how to do it.Disable controls When you enable a standard, all the controls for that standard are enabled by default. You can then disable and enable specific controls within an enabled standard.There are several reasons why you may choose to disable the controls: Controls for unused services Controls using global resources Controls with compensating controlsIt can be useful to turn off security checks for controls that are not relevant to your environment. Disabling irrelevant controls reduces the number of irrelevant findings. It also removes the failed check from the security score for the associated standard.When you disable a control, the following occurs: The check for the control is no longer performed. No additional findings are generated for that control. Existing findings are archived automatically after three to five days (note that this is the best effort and not guaranteed). The related AWS Config rules that Security Hub created are removed. Remember that Security Hub is Regional. When you disable or enable a control, it is disabled only in the current Region or in the Region that you specify in the API request.This is the fastest way to disable one control, with the CLI:aws securityhub update-standards-control --standards-control-arn \"arn:aws:securityhub:eu-west-1:1234567890:control/aws-foundational-security-best-practices/v/1.0.0/GuardDuty.1\" --control-status \"DISABLED\" --disabled-reason \"testing functionality\"With the AWS Console, you can easily disable a control. Access to a security standard:And then:In the following links, you will find more information on how to do it in depth, how to do it in a multi-account environment (several options), and more relevant information! How to disable controls How to disable in a multi-account environment AWS Foundational Best Practices controls that you might want to disable how to Create Auto-Suppression Rules in AWS Security HubIntegrationsSecurity Hub provides the ability to integrate security findings from AWS services and third-party products. For AWS services Security Hub automatically enables the integration, and you can optionally disable each integration. For third-party products Security Hub gives you the ability to selectively enable the integrations and provides a link to the configuration instructions related to the third-party product.Some examples of custom integrations: Integrate with Jira Service Management Integrate with Slack Summary emailAutomated response, remediation, and enrichment actionsYou can create custom automated response, remediation, and enrichment workflows using Security Hub’s integration with Amazon EventBridge. All Security Hub findings are automatically sent to EventBridge, and you can create EventBridge rules that have AWS Lambda functions, AWS Step Function functions, or AWS Systems Manager Automation runbooks as their targets.Security Hub also supports sending findings to EventBridge on-demand via custom actions, so that you can have an analyst decide when to trigger an automated response or remediation action.The Security Hub Automated Response and Remediation (SHARR) solution provides you with pre-packaged EventBridge rules for you to deploy via AWS CloudFormation.More info about automated response and remediation hereGood to know Regional service. You must enable it in all the regions you want to review. 30 days free trial when you enable it the first time, and during the free trial Security Hub provides an estimate of what the spend would be, so you can assess your spending beyond the free trial. All findings are stored in Security Hub for 90 days after the last update date Periodic checks run automatically within 12 hours after the most recent run. You cannot change the periodicity. Change-triggered checks run when the associated resource changes state. Even if the resource does not change state, the updated time for change-triggered checks is refreshed every 18 hours. After control statuses are generated for the first time, Security Hub updates the control status every 24 hours based on the findings from the previous 24 hours Security Hub processes the findings using a standard findings format called the AWS Security Finding Format (ASFF), which eliminates the need for time-consuming data conversion efforts. With the ASFF, all of Security Hub’s integration partners (including both AWS services and external partners) send their findings to Security Hub in a well-typed JSON format consisting of over 1,000 available fields. This means that all of your security findings are normalized before they are ingested into Security Hub, and you don’t need to do any parsing and normalization yourself. The findings identify resources, severities, and timestamps consistently so that you can more easily search and take action on them. The events are delivered to EventBridge in near-real-time and on a guaranteed basis When you enabled a security standard, AWS Config rules are created automatically (and you cannot delete them) You can create Custom Findings with AWS Config Rules and send them to Security Hub with EventBridgeMore information about Security Hub AWS official documentation Security Hub Workshop Automatically resolve findings for resources that no longer exist Enrich findings with account metadata Correlate findings" }, { "title": "How to improve your account security", "url": "/posts/how-to-improve-your-account-security/", "categories": "Security", "tags": "how-to, security, getting-started", "date": "2022-06-03 01:17:00 +0200", "snippet": "TLDRYou already have one or multiple AWS accounts and you want to improve your security approach, the Well-Architected Framework (security pillar) contains a lot of information, I did a full summar...", "content": "TLDRYou already have one or multiple AWS accounts and you want to improve your security approach, the Well-Architected Framework (security pillar) contains a lot of information, I did a full summary here, and you may want to learn about a plan to improve your account.I will share with you two resources to do it: AWS Security Maturity Model will allow you to know the recommended actions to strengthen your security posture at every stage of your journey to the cloud Contains 4 phases. The first one “quick wins” allow you fast security improvements Article in the AWS security blog: The top 10 most important cloud security tips that Stephen Schmidt, Chief Information Security Officer for AWS, laid out at AWS re:Invent 2019AWS Security Maturity ModelIt is a valuable resource for reviewing the current status and improving the security of your solutions.The classification of the different recommendations into categories depends on the cost and difficulty of implementing the security control, and the positive impact that it will achieve. This model is updated monthly by AWS. In fact, a month after writing this article I had to update it because the model had changed and all areas of the organization had changed, although the content is the same.The official documentation is located hereIntroductionSecurity FrameworksMultiple frameworks help you design the construction of a plan to provide security to your loads in the cloud. Well Architected Framework NIST CyberSecurity Framework Center for Internet Security (CIS) AWS Foundations Cloud Adoption FrameworkHow to prioritize“With so many services, security controls, and recommendations… How do I prioritize? Where do I start?” All Quick Win recommendations can be implemented in less than a week.Evolutive pathPhase 1. Quick WinsQuick Wins are the first thing to focus on, controls that you could implement in an organization within a maximum of one or two weeks, and will significantly improve your security standpoint. Level Recommendation Security governance - Assign Security Contacts - Select the region(s) Security assurance - Automate alignment with best practices using AWS Security Hub Identity and Access management - Multi-Factor Authentication - Avoid using Root and audit it - Access and role analysis with IAM Access Analyzer Thread detection - Thread Detection with Amazon GuardDuty and review your findings - Audit API calls with AWS CloudTrail- Remediate security findings found by AWS Trusted Advisor - Billing alarms for anomaly detection Vulnerability management   Infrastructure protection - Limit Security Groups Data prevention - Amazon S3 Block Public Access - Analyze data security posture with Amazon Macie Application security - AWS WAF with managed rules Incident response - Act on Amazon GuardDuty findings Link to the updated content and more information on each recommendationPhase 2. FoundationalThe controls and recommendations may take some more effort to implement but are very important. Level Recommendation Security governance - Identify security and regulatory requirements - Identify the most sensitive data (crown jewels) - Cloud Security Training Plan Security assurance - Configuration monitoring with AWS Config Identity and Access management - Centralized user repository – Organization Policies (SCPs) Threat detection - Investigate most Amazon GuardDuty findings Vulnerability management - Manage vulnerabilities in your infrastructure and perform pentesting - Manage vulnerabilities in your application Infrastructure protection - Manage your instances with Fleet Manager - Network segmentation (Public/Private Networks - VPCs) - Multi-account management with AWS Control Tower Data protection - Data Encryption (AWS KMS) - Backups - Discover sensitive data with Amazon Macie Application security - Involve security teams in development - No secrets in your code AWS Secrets Manager Incident response - Define Incident response playbooks- Redundancy using multiple Availability Zones Link to the updated content and more information on each recommendationPhase 3. EfficientThere are some controls and recommendations that allow us to manage security in an efficient way. Level Recommendation Security governance - Perform thread modelling Security assurance - Create your reports for compliance (such as PCI-DSS) Identity and Access - Privilege review (Least Privilege) - Tagging strategy - Customer IAM: security of your customers Threat detection - Integration with SIEM/SOAR - Network Flows analysis (VPC Flow Logs) Vulnerability management - Security champion in development Infrastructure protection - Image Generation Pipeline - Anti-Malware/EDR - Outbound Traffic Control - Use abstract services (Serverless) Data protection - Encryption in transit Application security - WAF with custom rules - Shield Advanced Advanced DDoS Mitigation Incident response - Automate critical and most frequently run Playbooks - Automate deviation correction in configurations -Using infrastructure as Code (CloudFormation, CDK) Link to the updated content and more information on each recommendationPhase 4. OptimizedAnd finally, there are those controls and recommendations that allow you to optimize in a continuous improvement cycle the security posture every day. It will be characterized by security controls that are often seen in more mature organizations, in terms of security or large organizations with very demanding requirements. Level Recommendation Security governance - Forming a Chaos Engineering team (Resilience) - Sharing Security work and responsibility Security assurance   Identity and Access management - Context-based access control - IAM Policy Generation Pipeline Threat detection - Amazon Fraud Detector - Integration with additional Intelligence Feeds Vulnerability management   Infrastructure protection - Process standardization with Service Catalog Data protection   Application security - DevSecOps - Forming a Red Team (Attacker’s Point of View) Incident response - Automate most playbooks - Amazon Detective: Root cause analysis - Forming a Blue Team (Incident Response) - Multi-region disaster recovery automation Link to the updated content and more information on each recommendationComplete Maturity LevelThis is the complete maturity model with all the phases through all the epics. I recommend you access to the original page to view the linkable table in the original site.Top 10 recommendations These are the top 10 most important cloud security tips that Stephen Schmidt, Chief Information Security Officer for AWS, laid out at AWS re:Invent 2019.The original article was written in the AWS blog here Configure account contacts Use multi-factor authentication (MFA) is the best way to protect accounts from inappropriate access No hard-coding secrets use AWS Secrets Manager if you are using java or python you can use CodeGuru Reviewer to detect secrets in the code Limit security groups use AWS Config and AWS Firewall Manager to programmatically ensure that the virtual private cloud (VPC) security group configuration is what you intended Intentional data policies design your approach to data classification Centralize CloudTrail logs AWS recommends that you write logs in an AWS account designated for logging (Log Archive). The permissions on the bucket should prevent the deletion of the logs, and they should also be encrypted at rest. Review how to use AWS to visualize AWS CloudTrail logs Validate IAM roles Use AWS IAM Access Analyzer Take action on findings Turn on AWS Security Hub, Amazon GuardDuty, and AWS Identity and Access Management Access Analyzer You also need to take action when you see findings Rotate keys Be involved in the dev cycle “raise the security culture of your organization.” " }, { "title": "Getting Started with AWS Security", "url": "/posts/getting-started-with-aws-security/", "categories": "Security", "tags": "getting-started, security", "date": "2022-05-31 19:33:00 +0200", "snippet": "TLDRYou have probably read many times that in AWS security is the TOP priority, and as you know there are many resources on the internet. I want to share with you in this article the **security bas...", "content": "TLDRYou have probably read many times that in AWS security is the TOP priority, and as you know there are many resources on the internet. I want to share with you in this article the **security basics** to improve your AWS solutions by focusing on these 2 resources you _have to_ know*: Recommendations and best practices: Security Pillar in AWS Well-Architected Framework AWS Security checklist If you’re looking to dive deeper into the broader range of learning materials available on security, including digital courses, blogs, whitepapers, and more, AWS recommends you the Ramp-Up GuideSecurity Pillar in AWS Well-Architected FrameworkYou should start here. This is the official link. I am sure you are familiar with the Well-Architected Framework and the Security Pillar… but have you read the whole thing? I will try to compile the main points for you. The Security Pillar provides guidance to help you apply best practices, current recommendations in the design, delivery, and maintenance of secure AWS workloads. By adopting this practices you can build architectures that protect your data and systems, control access, and respond automatically to security events.Security in the cloud is composed of six areas: Foundations Identity and access management Detection (logging and monitoring) Infrastructure protection Data protection Incident response1. Security Foundations1.1. Design PrinciplesThe security pillar of the Well-Architected Framework captures a set of design principles that turn the security areas into practical guidance that can help you strengthen your workload security.Where the security epics frame the overall security strategy, these Well-Architected principles describe what you should start doing: Implement a strong identity foundation Implement the principle of least privilege Enforce separation of duties (with appropriate authorization) Centralize identity management Aim to eliminate reliance on long-term static credentials Enable traceability Monitor, alert, and audit actions and changes to your environment in real time Integrate log and metric collection with systems to automatically investigate and take action Apply security at all layers Apply a defense in depth approach with multiple security controls Apply to all layers (for example, edge of network, VPC, load balancing, every instance and compute service, operating system, application, and code) Automate security best practices Automated software-based security mechanisms (improve your ability to securely scale more rapidly and cost-effectively) Create secure architectures, including the implementation of controls that are defined and managed as code in version-controlled templates Protect data in transit and at rest Classify your data into sensitivity levels and use mechanisms, such as encryption, tokenization, and access control where appropriate Keep people away from data Use mechanisms and tools to reduce or eliminate the need for direct access or manual processing of data Prepare for security events Prepare for an incident by having incident management and investigation policy and processes that align to your organizational requirements Run incident response simulations and use tools with automation to increase your speed for detection, investigation, and recovery 1.2. Shared Responsibility Security and Compliance is a shared responsibility between AWS and the customer.More detailed information here1.3. AWS Account Management and SeparationBest practices for account management and separation: Separate workloads using accounts Secure AWS account: not use the root user keep the contact information up to date Use AWS Organizations to: Manage accounts centrally: automates AWS account creation and management, and control of those accounts after they are created Set controls centrally: allows you to use service control policies (SCPs) to apply permission guardrails at the organization, organizational unit, or account level, which apply to all AWS Identity and Access Management (IAM) users and role Configure services and resources centrally: helps you configure AWS services that apply to all of your accounts (CloudTrail, AWS Config) 2. Identity and Access Management Identity and Access Management (IAM) helps customers integrate AWS into their identity management lifecycle, and sources of authentication and authorization.The best practices for these capabilities fall into two main areas. Identity Management Permissions Management2.1. Identity ManagementThere are two types of identities you will need to manage: Human identities: administrators, developers, operators, and consumers of your applications, … Machine identities: workload applications, operational tools, components, …The following are the best practices related to the identities: Rely on a centralized identity provider: This makes it easier to manage access across multiple applications and services, because you are creating, managing, and revoking access from a single location Federation with individual AWS Account: you can use centralized identities for AWS with a SAML 2.0-based provider with AWS IAM For federation to multiple accounts in your AWS Organization, you can configure your identity source in AWS Single Sign-On (AWS SSO) For managing end-users or consumers of your workloads, such as a mobile app, you can use Amazon Cognito Leverage user groups and attributes: Place users with common security requirements in groups defined by your identity provider, and put mechanisms in place to ensure that user attributes are correct and updated Use strong sign-in mechanisms Use temporary credentials Audit and rotate credentials periodically Store and use secrets securely: For credentials that are not IAM-related and cannot take advantage of temporary credentials, such as database logins, use a service that is designed to handle the management of secrets, such as AWS Secrets Manager2.2. Permission managementManage permissions to control access to human and machine identities that require access to AWS and your workloads. Permissions control who can access what, and under what conditions.How to grant access to different types of resources: Identity-based policies in IAM (managed or inline): These policies let you specify what that identity can do (its permissions) In most cases, you should create your own customer-managed policies following the principle of least privilege Resource-based policies are attached to a resource. These policies grant permission to a principal that can be in the same account as the resource or in another account Permissions boundaries: use a managed policy to set the maximum permissions that an administrator can set This enables you to delegate the ability to create and manage permissions to developers, such as the creation of an IAM role, but limit the permissions they can grant so that they cannot escalate their permission using what they have created Attribute-based access control (ABAC): enables you to grant permissions based on tags (attributes) Tags can be attached to IAM principals (users or roles) and to AWS resources Using IAM policies, administrators can create a reusable policy that applies permissions based on the attributes of the IAM principal Organizations service control policies (SCP): define the maximum permissions for account members of an organization or organizational unit (OU). Limit permission but do not grant it Session policies: advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. These policies limit permissions but do not grant permissionsThe following are the best practices related to permission management: Grant least privilege access Define permission guardrails for your organization: You should use AWS Organizations to establish common permission guardrails that restrict access to all identities in your organization. Here are examples of service control policies (SCPs) defined by AWS that you can apply to your organization. Analyze public and cross-account access: In AWS, you can grant access to resources in another account. You grant direct cross-account access using policies attached to resources or by allowing identity to assume an IAM role in another account. IAM Access Analyzer identify all access paths to a resource from outside of its account. It reviews resource policies continuously, and reports findings of public and cross-account access to make it easy for you to analyze potentially broad access. Share resources securely: AWS recommends sharing resources using AWS Resource Access Manager (AWS RAM) because enables you to easily and securely share AWS resources within your AWS Organization and Organizational Units Reduce permissions continuously: Maybe in the getting started of a project you chose to grant broad access, but later you should evaluate access continuously and restrict access to only the permissions required and achieve least privilege Establish emergency access process: AWS recommends having a process that allows emergency access to your workload, in particular your AWS accounts, in the unlikely event of an automated process or pipeline issue3. DetectionDetective Control provides guidance to help identify potential security incidents within the AWS environment. Detection consists of two parts: Configure Investigate3.1. Configure Configure services and application logging A foundational practice is to establish a set of detection mechanisms at the account level. This base set of mechanisms is aimed at recording and detecting a wide range of actions on all resources in your account. AWS CloudTrail provides the event history of your AWS account activity AWS Config monitors and records your AWS resource configurations and allows you to automate the evaluation and remediation against desired configurations Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads AWS Security Hub provides a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services and optional third-party products to give you a comprehensive view of security alerts and compliance status. Many core AWS services provide service-level logging features. For example, Amazon VPC provides VPC Flow Logs Amazon CloudWatch Logs can be used to store and analyze logs for EC2 instances and application-based logging that doesn’t originate from AWS services (you will need an agent), and use CloudWatch Logs Insights to process them in real-time or dive into analysis. Analyze logs, findings, and metrics centrally: A best practice is to deeply integrate the flow of security events and findings into a notification and workflow system. GuardDuty and Security Hub provides aggregation, deduplication, and analysis mechanisms for log records that are also made available to you via other AWS services. 3.2. Investigate Implement actionable security events: For each detective mechanism you have, you should also have a process, in the form of a runbook or playbook, to investigate Automate response to events: In AWS, investigating events of interest and information on potentially unexpected changes in an automated workflow can be achieved using Amazon EventBridge Amazon GuardDuty also allows you to route events to a workflow system for those building incident response systems (Step Functions), to a central Security Account, or to a bucket for further analysis. Detecting change and routing this information to the correct workflow can also be accomplished using AWS Config Rules and Conformance Packs. Conformance packs are a collection of Config Rules and remediation actions you deploy as a single entity authored as a YAML template. A sample conformance pack template is available for the Well-Architected Security Pillar 4. Infrastructure Protection Infrastructure protection ensures that systems and resources within your workloads are protected against unintended and unauthorized access, and other potential vulnerabilities. You need to be familiar with Regions, Availability Zones, AWS Local Zones, and AWS Outposts.4.1. Protecting NetworksWhen you follow the principle of applying security at all layers, you employ a Zero Trust approach (application components don’t trust any other). Create network layers: Components that share reachability requirements can be segmented into layers formed by subnets. Control traffic at all layers: You should examine the connectivity requirements of each component. In a VPC (region level), the subnets are in an Availability Zone with Network ACLs and route tables associated, and inside of subnets, you include the use of security groups (stateful inspection firewall). Some AWS services require components to access the internet for making API calls, where AWS API endpoints are located. Other AWS services use VPC endpoints within your Amazon VPCs. Many AWS services, including Amazon S3 and Amazon DynamoDB, support VPC endpoints, and this technology has been generalized in AWS PrivateLink. AWS recommends you to use this approach to access AWS services, third-party services, and your own services hosted in other VPCs securely because all network traffic on AWS PrivateLink stays on the global AWS backbone and never traverses the internet. Connectivity can only be initiated by the consumer of the service, and not by the provider of the service. Implement inspection and protection: Inspect and filter your traffic at each layer. You can inspect your VPC configurations for potential unintended access using VPC Network Access Analyzer. For components transacting over HTTP-based protocols, a web application firewall, AWS WAF, can help protect from common attacks. AWS WAF lets you monitor and block HTTP(s) requests that match your configurable rules that are forwarded to an Amazon API Gateway API, Amazon CloudFront, or an Application Load Balancer. For managing AWS WAF, AWS Shield Advanced protection, and Amazon VPC security groups across AWS Organizations, you can use AWS Firewall Manager. It allows you to centrally configure and manage firewall rules across your accounts and applications, making it easier to scale enforcement of common rules. It also enables you to rapidly respond to attacks, using AWS Shield Advanced, or solutions that can automatically block unwanted requests to your web applications. Firewall Manager also works with AWS Network Firewall, a managed service that uses a rules engine to give you fine-grained control over both stateful and stateless network traffic. Automate network protection: Automate protection mechanisms to provide a self-defending network based on threat intelligence and anomaly detection. For example, intrusion detection and prevention tools can adapt to current threats and reduce their impact. A web application firewall is an example of where you can automate network protection, for example, by using the AWS WAF Security Automations solution (https://github.com/awslabs/aws-waf-security-automations ) to automatically block requests originating from IP addresses associated with known threat actors. 4.2. Protecting ComputeCompute resources include EC2 instances, containers, AWS Lambda functions, database services, IoT devices, and more. Each of these compute resource types requires different approaches to secure them. However, they do share common strategies that you need to consider: Perform vulnerability management: Frequently scan and patch for vulnerabilities in your code, dependencies, and in your infrastructure to help protect against new threats. Automate the creation of infrastructure with CloudFormation and create secure-by-default infrastructure templates verified with CloudFormation Guard For patch management, use AWS System Manager Patch Manager Reduce attack surface: Reduce your exposure to unintended access by hardening operating systems and minimizing the components, libraries, and externally consumable services in use. Reduce unused components In EC2 you can create your own AMIs simplifying the process with EC2 Image Builder. When using containers implement ECR Image Scanning Using third-party static code analysis tools, you can identify common security issues. You can use Amazon CodeGuru for supported languages. Dependency-checking tools can also be used to determine whether libraries your code links against are the latest versions, are free of CVEs, and have licensing conditions that meet your software policy requirements. Using Amazon Inspector, you can perform configuration assessments against your instances for known common vulnerabilities and exposures (CVEs), assess against security benchmarks and automate the notification of defects Enable people to perform actions at a distance: Removing the ability for interactive access reduces the risk of human error, and the potential for manual configuration or management. For example, use a change management workflow to manage EC2 instances using tools such as AWS Systems Manager instead of allowing direct access, or via a bastion host. AWS CloudFormation stacks build from pipelines and can automate your infrastructure deployment and management tasks without using the AWS Management Console or APIs directly. Implement managed services: Implement services that manage resources, such as Amazon RDS, AWS Lambda, and Amazon ECS, to reduce your security maintenance tasks as part of the shared responsibility model. This means you have more free time to focus on securing your application Validate software integrity: Implement mechanisms (e.g. code signing) to validate that the software, code, and libraries used in the workload are from trusted sources and have not been tampered with. You can use AWS Signer Automate compute protection: Automate your protective compute mechanisms including vulnerability management, reduction in attack surface, and management of resources. The automation will help you invest time in securing other aspects of your workload, and reduce the risk of human error.5. Data protection Before architecting any workload, foundational practices that influence security should be in place: data classification provides a way to categorize organizational data based on criticality and sensitivity in order to help you determine appropriate protection and retention controls encryption protects data by way of rendering it unintelligible to unauthorized access These methods are important because they support objectives such as preventing mishandling or complying with regulatory obligations.5.1. Data Classification Identify the data within your workload: You need to understand the type and classification of data your workload is processing, the associated business processes, the data owner, applicable legal and compliance requirements, where it’s stored, and the resulting controls that are needed to be enforced. Define data protection controls: By using resource tags, separate AWS accounts per sensitivity, IAM policies, Organizations SCPs, AWS KMS, and AWS CloudHSM, you can define and implement your policies for data classification and protection with encryption. Define data lifecycle management: Your defined lifecycle strategy should be based on sensitivity level as well as legal and organizational requirements. Aspects including the duration for which you retain data, data destruction processes, data access management, data transformation, and data sharing should be considered. Automate identification and classification: Automating the identification and classification of data can help you implement the correct controls. Using automation for this instead of direct access from a person reduces the risk of human error and exposure. You should evaluate using a tool, such as Amazon Macie, that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.5.2. Protecting data at restData at rest represents any data that you persist in non-volatile storage for any duration in your workload. This includes block storage, object storage, databases, archives, IoT devices, and any other storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized access when encryption and appropriate access controls are implemented.Encryption and tokenization are two important but distinct data protection schemes. Tokenization is a process that allows you to define a token to represent an otherwise sensitive piece of information. Encryption is a way of transforming content in a manner that makes it unreadable without a secret key necessary to decrypt the content back into plaintext.Best practices: Implement secure key management: By defining an encryption approach that includes the storage, rotation, and access control of keys, you can help provide protection for your content against unauthorized users and against unnecessary exposure to authorized users. AWS KMS helps you manage encryption keys and integrates with many AWS services. This service provides durable, secure, and redundant storage for your AWS KMS keys. AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys in the AWS Cloud. Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. You can use AWS Managed Config Rules to check automatically that you are using encryption, for example, for EBS volumes, RDS instances, and S3 buckets. Enforce access control: Different controls including access (using least privilege), backups (see Reliability whitepaper), isolation, and versioning can all help protect your data at rest. Access to your data should be audited using detective mechanisms like CloudTrail and service level log You should inventory what data is publicly accessible, and plan for how you can reduce the amount of data available over time. Amazon S3 Glacier Vault Lock and S3 Object Lock are capabilities providing mandatory access control—once a vault policy is locked with the compliance option, not even the root user can change it until the lock expires Audit the use of encryption keys: Ensure that you understand and audit the use of encryption keys to validate that the access control mechanisms on the keys are appropriately implemented. For example, any AWS service using an AWS KMS key logs each use in AWS CloudTrail. You can then query AWS CloudTrail, by using a tool such as Amazon CloudWatch Insights, to ensure that all uses of your keys are valid. Use mechanisms to keep people away from data: Keep all users away from directly accessing sensitive data and systems under normal operational circumstances. For example, use a change management workflow to manage EC2 instances using tools instead of allowing direct access or a bastion host. This can be achieved using AWS Systems Manager Automation, which uses automation documents that contain steps you use to perform tasks. These documents can be stored in source control, peer-reviewed before running, and tested thoroughly to minimize risk compared to shell access. Business users could have a dashboard instead of direct access to a data store to run queries. Where CI/CD pipelines are not used, determine which controls and processes are required to adequately provide a normally disabled break-glass access mechanism. Automate data at rest protection: Use automated tools to validate and enforce data at rest controls continuously, for example, verify that there are only encrypted storage resources. You can automate validation that all EBS volumes are encrypted using AWS Config Rules. AWS Security Hub can also verify several different controls through automated checks against security standards. Additionally, your AWS Config Rules can automatically remediate non-compliant resources. 5.3. Protecting data in transitData in transit is any data that is sent from one system to another. This includes communication between resources within your workload as well as communication between other services and your end-users. By providing the appropriate level of protection for your data in transit, you protect the confidentiality and integrity of your workload’s data.Best practices: Implement secure key and certificate management: Store encryption keys and certificates securely and rotate them at appropriate time intervals with strict access control. The best way to accomplish this is to use a managed service, such as AWS Certificate Manager (ACM). It lets you easily provision, manage, and deploy public and private Transport Layer Security (TLS) certificates for use with AWS services and your internal connected resources. Enforce encryption in transit: AWS services provides HTTPS endpoints using TLS for communication, thus providing encryption in transit when communicating with the AWS APIs. Insecure protocols, such as HTTP, can be audited and blocked in a VPC through the use of security groups. HTTP requests can also be automatically redirected to HTTPS in Amazon CloudFront or on an Application Load Balancer. Additionally, you can use VPN connectivity into your VPC from an external network to facilitate the encryption of traffic. Third-party solutions are available in the AWS Marketplace if you have special requirements. Authenticate network communications: Using network protocols (TLS/IPsec) that support authentication allows for trust to be established between the parties adding encryption to reduce the risk of communications being altered or intercepted. Automate detection of unintended data access: Use tools such as Amazon GuardDuty automatically detects suspicious activity or attempts to move data outside of defined boundaries. Amazon VPC Flow Logs to capture network traffic information can be used with Amazon EventBridge to trigger the detection of abnormal connections–both successful and denied. S3 Access Analyzer can help assess what data is accessible to who in your S3 buckets. Secure data from between VPC or on-premises locations: You can use AWS PrivateLink to create a secure and private network connection between Amazon Virtual Private Cloud (Amazon VPC) or on-premises connectivity to services hosted in AWS. You can access AWS services, third-party services, and services in other AWS accounts as if they were on your private network. With AWS PrivateLink, you can access services across accounts with overlapping IP CIDRs without needing an Internet Gateway or NAT. You also do not have to configure firewall rules, path definitions, or route tables. Traffic stays on the Amazon backbone and doesn’t traverse the internet, therefore your data is protected. 6. Incident response Incident Response helps customers define and execute a response to security incidents.6.1. Design Goals of Cloud Response Establish response objectives: Some common goals include containing and mitigating the issue, recovering the affected resources, and preserving data for forensics, and attribution. Document plans: Create plans to help you respond to, communicate during, and recover from an incident. Respond using the cloud Know what you have and what you need: Preserve logs, snapshots, and other evidence by copying them to a centralized security cloud account. Use redeployment mechanisms: when possible, and make your response mechanisms safe to execute more than once and in environments in an unknown state. Automate where possible: As you see issues or incidents repeat and build mechanisms that programmatically triage and respond to common situations. Use human responses for unique, new, and sensitive incidents. Choose scalable solutions: reduce the time between detection and response. Learn and improve your process: When you identify gaps in your process, tools, or people, and implement plans to fix them. Simulations are safe methods to find gaps and improve processes.In AWS, there are several different approaches you can use when addressing incident response. Educate your security operations and incident response staff about cloud technologies and how your organization intends to use them. Development Skills: programming, source control, version control, CI/CD processes AWS Services: security services Application Awareness The best way to learn is hands-on, through running incident response game days Prepare your incident response team to detect and respond to incidents in the cloud, enable detective capabilities, and ensure appropriate access to the necessary tools and cloud services. Additionally, prepare the necessary runbooks, both manual and automated, to ensure reliable and consistent responses. Work with other teams to establish expected baseline operations, and use that knowledge to identify deviations from those normal operations. Simulate both expected and unexpected security events within your cloud environment to understand the effectiveness of your preparation. Iterate on the outcome of your simulation to improve the scale of your response posture, reduce time to value, and further reduce risk.AWS Security checklist This is a whitepaper of AWS that provides customer recommendations that align with the Well-Architected Framework Security Pillar. It is available here1. Identity and Access Management Secure your AWS Account Use AWS Organizations Use the root user with MFA Configure account contacts Rely on centralized identity provider Centralize identities using either AWS Single Sign-On or a third-party provider to avoid routinely creating IAM users or using long-term access keys—this approach makes it easier to manage multiple AWS accounts and federated applications Use multiple AWS accounts Use of Service Control Policies to implement guardrails AWS Control Tower can help you easily set up and govern a multi-account AWS environment Store and use secrets securely Use AWS Secrets Manager if you cannot use temporary credentials 2. Detection Enable foundational services for all AWS accounts AWS CloudTrail to log API activity Amazon GuardDuty for continuous monitoring AWS Security Hub for a comprehensive view of your security posture Configure service and application-level logging In addition to your application logs, enable logging at the service level, such as Amazon VPC Flow Logs and Amazon S3, CloudTrail, and Elastic Load Balancer access logging, to gain visibility into events Configure logs to flow to a central account, and protect them from manipulation or deletion Configure monitoring and alerts, and investigate events Enable AWS Config to track the history of resources Enable Config Managed Rules to automatically alert or remediate undesired changes Configure alerts for all your sources of logs and events, from AWS CloudTrail to Amazon GuardDuty and your application logs, for high-priority events and investigate 3. Infrastructure Protection Patch your operating system, applications, and code Use AWS Systems Manager Patch Manager to automate the patching process of all systems and code for which you are responsible, including your OS, applications, and code dependencies Implement distributed denial-of-service (DDoS) protection for your internet-facing resources Use Amazon Cloudfront, AWS WAF and AWS Shield to provide layer 7 and layer 3/layer 4 DDoS protection Control access using VPC Security Groups and subnet layers Use security groups for controlling inbound and outbound traffic, and automatically apply rules for both security groups and WAFs using AWS Firewall Manager Group different resources into different subnets to create routing layers, for example, database resources do not need a route to the internet 4. Data protection Protect data at rest Use AWS Key Management Service (KMS) to protect data at rest across a wide range of AWS services and your applications Enable default encryption for Amazon EBS volumes, and Amazon S3 buckets Encrypt data in transit Enable encryption for all network traffic, including Transport Layer Security (TLS) for web-based network infrastructure you control using AWS Certificate Manager to manage and provision certificates Use mechanisms to keep people away from data Keep all users away from directly accessing sensitive data and systems. For example, provide an Amazon QuickSight dashboard to business users instead of direct access to a database, and perform actions at a distance using AWS Systems Manager automation documents and Run Command 5. Incident response Ensure you have an incident response (IR) plan Begin your IR plan by building runbooks to respond to unexpected events in your workload Make sure that someone is notified to take action on critical findings Begin with GuardDuty findings. Turn on GuardDuty and ensure that someone with the ability to take action receives the notifications. Automatically creating trouble tickets is the best way to ensure that GuardDuty findings are integrated with your operational processes Practice responding to events Simulate and practice incident response by running regular game days, incorporating the lessons learned into your incident management plans, and continuously improving them " }, { "title": "How to create serverless applications with CDK and SAM", "url": "/posts/how-to-create-serverless-applications-with-cdk-and-sam/", "categories": "Serverless", "tags": "how-to, cdk, sam, iac, serverless, comparative", "date": "2022-04-25 20:45:00 +0200", "snippet": "TLDR A serverless application is more than just a Lambda Function. It is a combination of Lambda functions, event sources, APIs, databases, and other resources that work together to perform tasks....", "content": "TLDR A serverless application is more than just a Lambda Function. It is a combination of Lambda functions, event sources, APIs, databases, and other resources that work together to perform tasks.Creating serverless resources from the AWS Console is quick and easy, but as you know you should only use it for testing purposes when you are learning how it works. After that, and thinking about how to use the resources in a real project, you will need: IaC: to create your resources in a way that allows you to recreate them easily Version control: to track all the code modifications CI/CD: to automate the release process Does anyone use CloudFormation or Terraform to manage their serverless resources? Possibly but come on, there is a better way! To manage your serverless resources, there are much better options: SAM (Serverless Application Model) / Serverless Framework: declarative option with templates. Specific frameworks for serverless applications CDK / Pulumi: add a level of abstraction and allow you to define the infrastructure with modern programming languages In this article, we will review the approach to combining both CDK + SAM. By the way, CDK + SAM is my preferred approach: you get the best of the 2 options!CDK vs SAMIn the following articles, you will find the basics of CDK and SAM. CDK basics: How to create infrastructure with CDK SAM basics: How to create serverless applications with SAMWhat do CDK and SAM have in common?Both… Are open-source, Apache-licensed software development frameworks Provide Infrastructure as Code (IaC) Use AWS CloudFormation behind the scenes to deploy resources Provide a CLI to build and deploy applications Are well integrated with AWS build pipelines Support component reuseWhat are the main differences between CDK and SAM?   CDK SAM To declare resources Uses familiar programming languages Uses JSON or YAML Dynamic references Native language capabilities Pseudo parameters and logical functions Testing Not supported natively (you could use SAM) Supported (also debug) IaC resources All Focus on serverless Complexity Very low High, verbose configuration Maintainability Higher Lower Demo: CDK + SAMFrom Jan 6, 2022, AWS announced the general availability of AWS SAM CLI support for local testing of AWS CDK applications. It means that you can use SAM over your CDK project to test your resources!So… we will use a new CDK project to show the CDK + SAM.The source code is available here. This repository has several CDK projects but first, we will use the v1-simplePrepare to testWith CDK, when you run cdk synth, it will synthesize a stack defined in your app into a CloudFormation template in a json file in the cdk.out folder.However, SAM uses a yaml template, template.yaml or template.yml, in the root folder. Also, to test locally, you will need this file created or you will receive an error &gt; sam local invokeError: Template file not found at /.../aws-cdk-simple-webservice/template.yml Then, we have to run cdk synth and store the result in template.yml file. use –no-staging to disable the copy of assets which allows local debugging via the SAM CLI to reference the original source filescdk synth --no-staging &gt; template.yml You have to use --no-staging because it is required for SAM CLI to local debug the source files.Testing Lambda FunctionsYou have now a template.yml file and can run the SAM command to test your lambda function.&gt; sam local invokeInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece Version: $LATEST2022-04-22T21:00:38.952Z 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece INFO request: {}END RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797eceREPORT RequestId: 03d4ef7d-47b4-4ad2-a491-d0e5fc797ece Init Duration: 0.39 ms Duration: 205.98 ms Billed Duration: 206 ms Memory Size: 512 MB Max Memory Used: 512 MB{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda!\"}%The Lambda returns the following body: You have connected with the Lambda!Testing Lambda Functions with input dataIf your Lambda Functions need input data, you can generate it from SAM CLI with the command generate-eventsam local generate-event [OPTIONS] COMMAND [ARGS]...You can use this command to generate sample payloads from different event sources such as S3, API Gateway, SNS, and so on. These payloads contain the information that the event sources send to your Lambda functions.Or you can add the input data to the option -e of the command invoke&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"706Z 992499c1-83c4-408d-966b-2e13f5955cbc INFO input: {{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda!\"}END RequestId: 992499c1-83c4-408d-966b-2e13f5955cbcREPORT RequestId: 992499c1-83c4-408d-966b-2e13f5955cbc Init Duration: 0.89 ms Duration: 244.32 ms Billed Duration: 245 ms Memory Size: 512 MB Max Memory Used: 512 MBTesting API GatewayYou have to run sam local start-api&gt; sam local start-apiMounting simplest-lambda at http://127.0.0.1:3000$default [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-23 00:03:58 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) You can access http://127.0.0.1:3000/ to connect with your Lambda FunctionIf you review your previous console, it will be updated when you accessed your API Gateway:&gt; sam local start-apidefault [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-23 00:03:58 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)Invoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351 Version: $LATEST \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Ge \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng} \"isBase64Encoded\": falsehost\"01 +0000\",-11ba1c012bf1\",END RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351REPORT RequestId: 8ed4a0a8-18bc-43af-b759-ad0668784351 Init Duration: 0.51 ms Duration: 230.55 ms Billed Duration: 231 ms Memory Size: 512 MB Max Memory Used: 512 MB2022-04-23 00:11:05 127.0.0.1 - - [23/Apr/2022 00:11:05] \"GET / HTTP/1.1\" 200 -Invoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v1-simple/functions/simplest-example as /var/task:ro,delegated inside runtime containerSTART RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7 Version: $LATEST \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Ge} \"isBase64Encoded\": falseco\"t\"01 +0000\",-11ba1c012bf1\",g+xml,image/*,*/*;q=0.8\",END RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7REPORT RequestId: 5759a74a-40b5-4a7e-8362-eec719ae44a7 Init Duration: 0.50 ms Duration: 238.45 ms Billed Duration: 239 ms Memory Size: 512 MB Max Memory Used: 512 MB2022-04-23 00:11:06 127.0.0.1 - - [23/Apr/2022 00:11:06] \"GET /favicon.ico HTTP/1.1\" 200 -Bonus: Testing DynamoDBOk, testing a mocked Lambda Function is the “hello world” example and not very useful, but what about a Lambda Function that connects to a DynamoDB table?We will update our Lambda Function to store the data in a DynamoDB table, so we are using the v2-dynamodb example in the repository. This code is based on the pattern defined in the web cdkpatterns as the simple webserviceCase 1: Testing cloud DynamoDBWhen you try to locally test a Lambda Function that stores data in a DynamoDB table, it will automatically attempt to connect to the DynamoDB service of your AWS Account.So, to test your Account DynamoDB tables, you have to do nothing.&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v2-dynamodb/functions/dynamodb-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"492Z 69d093d2-083c-46e7-a318-636ed94d7e47 INFO request: {2022-04-23T06:38:56.797Z 69d093d2-083c-46e7-a318-636ed94d7e47 ERROR Invoke Error {\"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\",\"code\":\"ResourceNotFoundException\",\"message\":\"Requested resource not found\",\"time\":\"2022-04-23T06:38:56.787Z\",\"requestId\":\"RCJDUN8DRCPPOS3SR3034ETK8VVV4KQNSO5AEMVJF66Q9ASUAAJG\",\"statusCode\":400,\"retryable\":false,\"retryDelay\":49.42319019990148,\"stack\":[\"ResourceNotFoundException: Requested resource not found\",\" at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:52:27)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:686:14)\",\" at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\" at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\" at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:688:12)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:116:18)\"]}{\"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\",\"trace\":[\"ResourceNotFoundException: Requested resource not found\",\" at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/json.js:52:27)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\",\" at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:686:14)\",\" at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\" at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\" at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\",\" at Request.&lt;anonymous&gt; (/var/runtime/node_modules/aws-sdk/lib/request.js:688:12)\",\" at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequentialEND RequestId: 69d093d2-083c-46e7-a318-636ed94d7e47REPORT RequestId: 69d093d2-083c-46e7-a318-636ed94d7e47 Init Duration: 0.98 ms Duration: 928.39 ms Billed Duration: 929 ms Memory Size: 512 MB Max Memory Used: 512 MB_executor.js:116:18)\"]}% If you don’t deploy your CDK project before attempting to test it, you will get the following ERROR: \"errorType\":\"ResourceNotFoundException\",\"errorMessage\":\"Requested resource not found\"Of course, you can set your AWS account in your sam CLI using the profile command. If you do not specify it, the default value will be applied.&gt; sam local invoke -e test/events/simple-event.json profile testInvoking index.handler (nodejs14.x)...Case 2: Testing local DynamoDBYou may want to test your Lambda function locally instead of connecting to your DynamoDB account, so do the following: Download DynamoDB docker image Run the DynamoDB docker image Set up DynamoDB: create tables, insert data, and test it Change your Lambda Function codeDownload the DynamoDB docker imageThe first step is to download the DynamoDB Docker image.&gt; docker pull amazon/dynamodb-localUsing default tag: latestlatest: Pulling from amazon/dynamodb-local3a461b3ae562: Pull complete14d349bd5978: Pull complete3e361eec6409: Pull completeDigest: sha256:07e740ad576acdcfdc48676f9a153a93a8e35436ea36942d4c14939caeca8851Status: Downloaded newer image for amazon/dynamodb-local:latestdocker.io/amazon/dynamodb-local:latestRun the DynamoDB Docker image locallyNow we have to run the locally downloaded docker image. This terminal tab will be kept running and you will have to open another one.&gt; docker run -p 8000:8000 amazon/dynamodb-localInitializing DynamoDB Local with the following configuration:Port: 8000InMemory: trueDbPath: nullSharedDb: falseshouldDelayTransientStatuses: falseCorsParams: * This command will not persist data in the local DynamoDB.Create a local DynamoDB tableWe are going to create a table with the name hits, with a partitionKey with the name path and the String type.&gt; aws dynamodb create-table --table-name hits --attribute-definitions AttributeName=path,AttributeType=S --key-schema AttributeName=path,KeyType=HASH --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 --endpoint-url http://localhost:8000{ \"TableDescription\": { \"TableArn\": \"arn:aws:dynamodb:ddblocal:000000000000:table/hits\", \"AttributeDefinitions\": [ { \"AttributeName\": \"path\", \"AttributeType\": \"S\" } ], \"ProvisionedThroughput\": { \"NumberOfDecreasesToday\": 0, \"WriteCapacityUnits\": 1, \"LastIncreaseDateTime\": 0.0, \"ReadCapacityUnits\": 1, \"LastDecreaseDateTime\": 0.0 }, \"TableSizeBytes\": 0, \"TableName\": \"hits\", \"TableStatus\": \"ACTIVE\", \"KeySchema\": [ { \"KeyType\": \"HASH\", \"AttributeName\": \"path\" } ], \"ItemCount\": 0, \"CreationDateTime\": 1650668228.617 }}Add values to our local DynamoDB tableWe will add 2 elements: path: “/test” path: “/hello”aws dynamodb put-item --table-name hits --item '{ \"path\": {\"S\": \"/test\"} }' --return-consumed-capacity TOTAL --endpoint-url http://localhost:8000aws dynamodb put-item --table-name hits --item '{ \"path\": {\"S\": \"/hello\"} }' --return-consumed-capacity TOTAL --endpoint-url http://localhost:8000Scan your table locallyWe check that our table has the created elements:&gt; aws dynamodb scan --table-name hits --endpoint-url http://localhost:8000{ \"Count\": 2, \"Items\": [ { \"path\": { \"S\": \"/test\" } }, { \"path\": { \"S\": \"/hello\" } } ], \"ScannedCount\": 2, \"ConsumedCapacity\": null}Change Lambda Function codeWe need to update our Lambda Function code to tell DynamoDB to read from our local DynamoDB service: You have to use your specific docker endpointif (process.env.AWS_SAM_LOCAL) { // mac dynamo.endpoint = new AWS.Endpoint(\"http://docker.for.mac.localhost:8000/\"); // windows // dynamo.endpoint = new AWS.Endpoint(\"http://docker.for.windows.localhost:8000/\"); // linux // dynamo.endpoint = new AWS.Endpoint(\"http://127.0.0.1:8000\");}Test DynamoDB locallyIn summary, we have: the DynamoDB service locally running a table (hits) 2 elements path=/test path=/hello Now we are going to test our Lambda Function which will insert data into our local DynamoDB table.&gt; sam local invoke -e test/events/simple-event.jsonInvoking index.handler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.46.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/cdk/aws-cdk-simple-webservice/v2-dynamodb/functions/dynamodb-example as /var/task:ro,delegated inside runtime container} \"rawPath\": \"/test\"036Z eaf85e61-e9a2-4b49-9953-d247f9794fb8 INFO request: {2022-04-22T23:54:20.130Z eaf85e61-e9a2-4b49-9953-d247f9794fb8 INFO inserted counter for /testEND RequestId: eaf85e61-e9a2-4b49-9953-d247f9794fb8REPORT RequestId: eaf85e61-e9a2-4b49-9953-d247f9794fb8 Init Duration: 0.48 ms Duration: 697.40 ms Billed Duration: 698 ms Memory Size: 512 MB Max Memory Used: 512 MB{\"statusCode\":200,\"headers\":{\"Content-Type\":\"text/html\"},\"body\":\"You have connected with the Lambda and store the data in the DynamoDB table!\"}If we scan the table again, we can review that in “/test” element will be a new hits column and 2 values:&gt; aws dynamodb scan --table-name hits --endpoint-url http://localhost:8000{ \"Count\": 2, \"Items\": [ { \"path\": { \"S\": \"/test\" }, \"hits\": { \"N\": \"2\" } }, { \"path\": { \"S\": \"/hello\" } } ], \"ScannedCount\": 2, \"ConsumedCapacity\": null}If you run your function more times, the value of hits will be updated.And, of course, you can also test it from API Gateway:&gt; sam local start-apiMounting dynamodb-lambda at http://127.0.0.1:3000$default [X-AMAZON-APIGATEWAY-ANY-METHOD]You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template2022-04-25 19:53:01 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)" }, { "title": "How to add CI/CD to my SAM project", "url": "/posts/how-to-add-ci-cd-to-my-sam-project/", "categories": "DevOps", "tags": "how-to, sam, iac, cicd, codepipeline, codebuild, github", "date": "2022-04-10 10:53:00 +0200", "snippet": "Introduction This is the second article of SAM. In this other article I had explained how to create serverless applications with SAM, and there I explained all the basic SAM information, so you ma...", "content": "Introduction This is the second article of SAM. In this other article I had explained how to create serverless applications with SAM, and there I explained all the basic SAM information, so you may need to review it before this.Here we will add CI/CD to our SAM application through the pipeline integration of the AWS SAM CLI. As we want to add automation to our deployment process and integrate it with the AWS ecosystem, we will use the AWS Developer tools.This is the SAM project code on GitHub that we will use in the article. In the commit history you can find the evolution of the application through the steps explained.Add CI/CD to a SAM projectWe want to create a CI/CD pipeline to implement continuous deployment (we want that when we push new code, the pipeline deploys our resources automatically). From AWS doc: AWS SAM provides a set of default pipeline templates for multiple CI/CD systems that encapsulate AWS’s deployment best practices. These default pipeline templates use standard JSON/YAML pipeline configuration formats, and the built-in best practices help perform multi-account and multi-region deployments and verify that pipelines cannot make unintended changes to infrastructure.We want to create a new pipeline in the AWS CodePipeline resource using the SAM templates.To generate a starter pipeline configuration for AWS CodePipeline, we have to perform the following tasks in this order: Create infrastructure resources Generate the pipeline configuration Commit your pipeline configuration to the Git repository Deploy your pipeline Connect your Git repository with your CI/CD system After you’ve generated the starter pipeline configuration and committed it to your Git repository, whenever someone commits a code change to that repository your pipeline will be triggered to run automatically.Step 1: Create infrastructure resourcesPipelines that use AWS SAM require certain AWS resources, like an IAM user and roles with necessary permissions, an Amazon S3 bucket, and optionally an Amazon ECR repository. You must have a set of infrastructure resources for each deployment stage of the pipeline.For each stage we need (dev, test, prod…), we have to run sam pipeline bootstrap, and it will create a CloudFormation stack with the name aws-sam-cli-managed-${stage}-pipeline-resources, which will create the necessary resources that SAM needs.We are going to create only one stage with a name test:&gt; sam pipeline bootstrapsam pipeline bootstrap generates the required AWS infrastructure resources to connectto your CI/CD system. This step must be run for each deployment stage in your pipeline,prior to running the sam pipeline init command.We will ask for [1] stage definition, [2] account details, and[3] references to existing resources in order to bootstrap these pipeline resources.[1] Stage definitionEnter a configuration name for this stage. This will be referenced later when you use the sam pipeline init command:Stage configuration name: &gt; test[2] Account detailsThe following AWS credential sources are available to use.To know more about configuration AWS credentials, visit the link below:&lt;https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html&gt;1 - Environment variables (not available)2 - default (named profile)3 - localstack (named profile)q - Quit and configure AWS credentialsSelect a credential source to associate with this stage: &gt; 2Associated account xxxxxxxxxxxx with configuration test.Enter the region in which you want these resources to be created [eu-west-1]:Enter the pipeline IAM user ARN if you have previously created one, or we will create one for you []:[3] Reference application build resourcesEnter the pipeline execution role ARN if you have previously created one, or we will create one for you []: &gt;Enter the CloudFormation execution role ARN if you have previously created one, or we will create one for you []: &gt;Please enter the artifact bucket ARN for your Lambda function. If you do not have a bucket, we will create one for you []: &gt;Does your application contain any IMAGE type Lambda functions? [y/N]: &gt;[4] SummaryBelow is the summary of the answers:1 - Account: xxxxxxxxxxxx2 - Stage configuration name: test3 - Region: eu-west-14 - Pipeline user: [to be created]5 - Pipeline execution role: [to be created]6 - CloudFormation execution role: [to be created]7 - Artifacts bucket: [to be created]8 - ECR image repository: [skipped]Press enter to confirm the values above, or select an item to edit the value: &gt;This will create the following required resources for the 'test' configuration:- Pipeline IAM user- Pipeline execution role- CloudFormation execution role- Artifact bucketShould we proceed with the creation? [y/N]: &gt; YCreating the required resources...Successfully created!The following resources were created in your account:- Pipeline IAM user- Pipeline execution role- CloudFormation execution role- Artifact bucketPipeline IAM user credential:AWS_ACCESS_KEY_ID: xxxxxxxxxxAWS_SECRET_ACCESS_KEY: xxxxxxxxxxView the definition in .aws-sam/pipeline/pipelineconfig.toml,run sam pipeline bootstrap to generate another set of resources, or proceed tosam pipeline init to create your pipeline configuration file.Before running sam pipeline init, we recommend first setting up AWS credentialsin your CI/CD account. Read more about how to do so with your provider in&lt;https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-generating-example-ci-cd-others.html&gt;.A new stack is created for our new stage test. If you want more than 1 stage you should repeat the sam pipeline bootstrap for the new stage (prod?). In this example I only want 1 stage to simplify.In our SAM project now we have 1 new file containing our stage information:Step 2: Generate the pipeline configurationTo generate the pipeline configuration, run the command sam pipeline init:&gt; sam pipeline initsam pipeline init generates a pipeline configuration file that your CI/CD systemcan use to deploy serverless applications using AWS SAM.We will guide you through the process to bootstrap resources for each stage,then walk through the details necessary for creating the pipeline config file.Please ensure you are in the root folder of your SAM application before you begin.Select a pipeline template to get started: 1 - AWS Quick Start Pipeline Templates 2 - Custom Pipeline Template LocationChoice: &gt; 1Cloning from https://github.com/aws/aws-sam-cli-pipeline-init-templates.git (process may take a moment)Select CI/CD system 1 - Jenkins 2 - GitLab CI/CD 3 - GitHub Actions 4 - Bitbucket Pipelines 5 - AWS CodePipelineChoice: &gt; 5You are using the 2-stage pipeline template. _________ _________| | | || Stage 1 |-&gt;| Stage 2 ||_________| |_________|Checking for existing stages...Only 1 stage(s) were detected, fewer than what the template requires: 2.To set up stage(s), please quit the process using Ctrl+C and use one of the following commands:sam pipeline init --bootstrap To be guided through the stage and config file creation process.sam pipeline bootstrap To specify details for an individual stage.To reference stage resources bootstrapped in a different account, press enter to proceed []:What is the Git provider? 1 - Bitbucket 2 - CodeCommit 3 - GitHub 4 - GitHubEnterpriseServerChoice []: &gt; 3What is the full repository id (Example: some-user/my-repo)?: alazaroc/aws-sam-appWhat is the Git branch used for production deployments? [main]:What is the template file path? [template.yaml]:We use the stage configuration name to automatically retrieve the bootstrapped resources created when you ran `sam pipeline bootstrap`.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testSelect an index or enter the stage 1's configuration name (as provided during the bootstrapping): &gt; 1What is the sam application stack name for stage 1? [sam-app]: &gt;Stage 1 configured successfully, configuring stage 2.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testSelect an index or enter the stage 2's configuration name (as provided during the bootstrapping):I have to stop the execution here.In the console log below, the following is displayed: You are using the 2-stage pipeline template Only 1 stage(s) were detected, fewer than what the template requires: 2. In any real project you should have at least two stages, so you could use the default AWS template.I don't want to create two stages in my CI/CD pipeline, I am testing a simple SAM project, and I only want ONE. Unfortunately, you can’t do that with the AWS Quick Start Pipeline Templates so I forked the main AWS project and I created a custom template with only ONE stage. This is my forked project: https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.git I had to put my custom template in the root folder because otherwise, the AWS SAM CLI doesn’t work.In the next execution, I will choose option 2 Custom Pipeline Template Location, and add it to my updated forked repository to create only one stage in the CodePipeline.&gt; sam pipeline initsam pipeline init generates a pipeline configuration file that your CI/CD systemcan use to deploy serverless applications using AWS SAM.We will guide you through the process to bootstrap resources for each stage,then walk through the details necessary for creating the pipeline config file.Please ensure you are in the root folder of your SAM application before you begin.Select a pipeline template to get started: 1 - AWS Quick Start Pipeline Templates 2 - Custom Pipeline Template LocationChoice: &gt; 2Template Git location: &gt; https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.gitCloning from https://github.com/alazaroc/aws-sam-cli-pipeline-init-templates.git (process may take a moment)You are using the 1-stage pipeline template. _________| || Stage 1 ||_________|Checking for existing stages...What is the Git provider? 1 - Bitbucket 2 - CodeCommit 3 - GitHub 4 - GitHubEnterpriseServerChoice []: &gt; 3What is the full repository id (Example: some-user/my-repo)?: &gt; alazaroc/aws-sam-appWhat is the Git branch used for production deployments? [main]: &gt;What is the template file path? [template.yaml]: &gt;We use the stage name to automatically retrieve the bootstrapped resources created when you ran `sam pipeline bootstrap`.Here are the stage configuration names detected in .aws-sam/pipeline/pipelineconfig.toml: 1 - testWhat is the name of stage 1 (as provided during the bootstrapping)?Select an index or enter the stage name: &gt; 1What is the sam application stack name for stage 1? [sam-app]: &gt;Stage 1 configured successfully (you only have one stage).To deploy this template and connect to the main git branch, run this against the leading account:`sam deploy -t codepipeline.yaml --stack-name &lt;stack-name&gt; --capabilities=CAPABILITY_IAM`.SUMMARYWe will generate a pipeline config file based on the following information: What is the Git provider?: GitHub What is the full repository id (Example: some-user/my-repo)?: alazaroc/aws-sam-app What is the Git branch used for production deployments?: main What is the template file path?: template.yaml What is the name of stage 1 (as provided during the bootstrapping)?Select an index or enter the stage name: 1 What is the sam application stack name for stage 1?: sam-app What is the pipeline execution role ARN for stage 1?: arn:aws:iam::xxxxxxxxxxxx:role/aws-sam-cli-managed-test-pip-PipelineExecutionRole-1RT9YMZ60U7L8 What is the CloudFormation execution role ARN for stage 1?: arn:aws:iam::xxxxxxxxxxxx:role/aws-sam-cli-managed-test-CloudFormationExecutionR-FA52519RHRDD What is the S3 bucket name for artifacts for stage 1?: aws-sam-cli-managed-test-pipeline-artifactsbucket-jf6hixn3rx29 What is the ECR repository URI for stage 1?: What is the AWS region for stage 1?: eu-west-1Successfully created the pipeline configuration file(s): - codepipeline.yaml - assume-role.sh - pipeline/buildspec_unit_test.yml - pipeline/buildspec_build_package.yml - pipeline/buildspec_integration_test.yml - pipeline/buildspec_feature.yml - pipeline/buildspec_deploy.ymlNow we have the new files in our project that CodePipeline will use to deploy our code:Step 3: Commit your pipeline configuration to GitThis step is necessary to ensure your CI/CD system is aware of your pipeline configuration and will run when changes are committed.Step 4: Deploy your pipelineFor AWS CodePipeline you have to deploy the pipeline running sam deploy -t codepipeline.yaml --stack-name &lt;pipeline-stack-name&gt; --capabilities=CAPABILITY_IAM --region &lt;region-X&gt; Don’t set the same stack name as your SAM application because doing so will overwrite your application’s stack (and delete your application resources).&gt; sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM Deploying with following values =============================== Stack name : sam-app-pipeline Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-19wv6mek3hxyw Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/f421d3ae8f9c85b69c81f641d104e1eb.template, skipping uploadWaiting for changeset to be created..CloudFormation stack changeset-------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-------------------------------------------------------------------------------------------------------------------------+ Add CodeBuildProjectBuildAndPack AWS::CodeBuild::Project N/A+ Add CodeBuildProjectDeploy AWS::CodeBuild::Project N/A+ Add CodeBuildServiceRole AWS::IAM::Role N/A+ Add CodePipelineExecutionRole AWS::IAM::Role N/A+ Add CodeStarConnection AWS::CodeStarConnections::Co N/A+ Add PipelineArtifactsBucketPolic AWS::S3::BucketPolicy N/A+ Add PipelineArtifactsBucket AWS::S3::Bucket N/A+ Add PipelineArtifactsLoggingBuck AWS::S3::BucketPolicy N/A+ Add PipelineArtifactsLoggingBuck AWS::S3::Bucket N/A+ Add PipelineStackCloudFormationE AWS::IAM::Role N/A+ Add Pipeline AWS::CodePipeline::Pipeline N/A-------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649184605/5c7e3379-74d0-486e-a387-99837b6ab74bPreviewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-05 20:50:13 - Waiting for stack create/update to completeCloudFormation events from stack operations-------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-------------------------------------------------------------------------------------------------------------------------CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::IAM::Role PipelineStackCloudFormationE -CREATE_IN_PROGRESS AWS::CodeStarConnections::Co CodeStarConnection -CREATE_IN_PROGRESS AWS::IAM::Role PipelineStackCloudFormationE Resource creation InitiatedCREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsLoggingBuck Resource creation InitiatedCREATE_COMPLETE AWS::CodeStarConnections::Co CodeStarConnection -CREATE_IN_PROGRESS AWS::CodeStarConnections::Co CodeStarConnection Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role PipelineStackCloudFormationE -CREATE_COMPLETE AWS::S3::Bucket PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsBucket -CREATE_IN_PROGRESS AWS::S3::Bucket PipelineArtifactsBucket Resource creation InitiatedCREATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsLoggingBuck Resource creation InitiatedCREATE_COMPLETE AWS::S3::Bucket PipelineArtifactsBucket -CREATE_IN_PROGRESS AWS::IAM::Role CodeBuildServiceRole -CREATE_IN_PROGRESS AWS::IAM::Role CodeBuildServiceRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role CodeBuildServiceRole -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectBuildAndPack -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectDeploy -CREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectDeploy Resource creation InitiatedCREATE_IN_PROGRESS AWS::CodeBuild::Project CodeBuildProjectBuildAndPack Resource creation InitiatedCREATE_COMPLETE AWS::CodeBuild::Project CodeBuildProjectDeploy -CREATE_COMPLETE AWS::CodeBuild::Project CodeBuildProjectBuildAndPack -CREATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionRole -CREATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role CodePipelineExecutionRole -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsBucketPolic -CREATE_IN_PROGRESS AWS::S3::BucketPolicy PipelineArtifactsBucketPolic Resource creation InitiatedCREATE_IN_PROGRESS AWS::CodePipeline::Pipeline Pipeline -CREATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsBucketPolic -CREATE_IN_PROGRESS AWS::CodePipeline::Pipeline Pipeline Resource creation InitiatedCREATE_COMPLETE AWS::CodePipeline::Pipeline Pipeline -CREATE_COMPLETE AWS::CloudFormation::Stack sam-app-pipeline --------------------------------------------------------------------------------------------------------------------------CloudFormation outputs from deployed stack---------------------------------------------------------------------------------------------------------------------------Outputs---------------------------------------------------------------------------------------------------------------------------Key CodeStarConnectionArnDescription The Arn of AWS CodeStar Connection used to connect to external code repositories.Value arn:aws:codestar-connections:eu-west-1:xxxxxxxxxxxx:connection/81d48b76-1ae7-4d69-b6eb-11eea6acf94a---------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app-pipeline in eu-west-1A new stack has been created to deploy our AWS CodePipeline: But the first execution has failed We can see in the CodePipeline flow all steps created: Source: integrated with GitHub as we indicated before UpdatePipeline: the pipeline can update itself BuildAndPackage: the SAM application is built, packaged, and uploaded (with AWS CodeBuild service) DeployTest: the SAM application is deployed (with AWS CodeBuild service)The cause of the error was that the connection between GitHub and AWS must be confirmed after being created:Step 5: Connect your Git repository with your CI/CD system If you are using GitHub or Bitbucket, after running the sam deploy command, you need to complete the pending connection in the Settings/Connection section of the Developer Tools. In addition, you could store a copy of the CodeStarConnectionArn from the output of the sam deploy command, because you will need it if you want to use AWS CodePipeline with another branch than main.By accessing the Settings/Connections in the Developer Tools, you can validate that the connection is pending approval:After activating it, we run the pipeline again (by clicking on the Release change button) and now the pipeline ends correctly.Update CI/CD steps to the SAM projectNote that now we have a pipeline that first checks for changes in the pipeline itself and then checks the code and deploys the resources.Step 1: Update steps in the pipeline automatically With this pipeline configuration (with UpdatePipeline step) all the changes that we make in the pipeline will be updated automatically.We want to test the automatic update of the pipeline if we make some changes.So, let’s change the pipeline steps to: delete the UpdatePipeline step add the UnitTest stepTo do this, we have to update the codepipeline.yaml file with the necessary changes: comment all the UpdatePipeline step code uncomment all the UnitTest step codeAnd after that, commit the changes and push them to the repository:When we push the changes, the UpdatePipeline step executes an update on the sam-app-pipeline stack (in the CloudFormation service) and it will update the pipeline definition.As we expected, the pipeline has updated itself and now we have UnitTest step but not UpdatePipeline.Step 2: Update steps in the pipeline manually If you remove the UpdatePipeline step, when you push a change to the repository the pipeline won’t be updated, so you have to run manually the update of the pipeline.To update the pipeline we have to run again the command sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM&gt; sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM Deploying with following values =============================== Stack name : sam-app-pipeline Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================Uploading to sam-app/23360d9a8d229e9ea24f2e1a53ea8b00.template 15967 / 15967 (100.00%)Waiting for changeset to be created..CloudFormation stack changeset-------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-------------------------------------------------------------------------------------------------* Modify CodeBuildProjectBuildA AWS::CodeBuild::Projec Conditional* Modify CodeBuildProjectDeploy AWS::CodeBuild::Projec Conditional* Modify CodeBuildServiceRole AWS::IAM::Role False* Modify CodePipelineExecutionR AWS::IAM::Role False* Modify CodeStarConnection AWS::CodeStarConnectio False* Modify PipelineArtifactsBucke AWS::S3::BucketPolicy False* Modify PipelineArtifactsBucke AWS::S3::Bucket False* Modify PipelineArtifactsLoggi AWS::S3::BucketPolicy False* Modify PipelineArtifactsLoggi AWS::S3::Bucket False* Modify PipelineStackCloudForm AWS::IAM::Role False* Modify Pipeline AWS::CodePipeline::Pip False- Delete CodeBuildProjectUnitTe AWS::CodeBuild::Projec N/A-------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649503303/4d23550d-27da-458d-ba18-f3be6db7ca54Previewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-09 13:22:01 - Waiting for stack create/update to completeCloudFormation events from stack operations-------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-------------------------------------------------------------------------------------------------UPDATE_COMPLETE AWS::S3::Bucket PipelineArtifactsLoggi -UPDATE_COMPLETE AWS::IAM::Role PipelineStackCloudForm -UPDATE_COMPLETE AWS::CodeStarConnectio CodeStarConnection -UPDATE_COMPLETE AWS::S3::Bucket PipelineArtifactsBucke -UPDATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsLoggi -UPDATE_COMPLETE AWS::IAM::Role CodeBuildServiceRole -UPDATE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectBuildA -UPDATE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectDeploy -UPDATE_IN_PROGRESS AWS::IAM::Role CodePipelineExecutionR -UPDATE_COMPLETE AWS::IAM::Role CodePipelineExecutionR -UPDATE_COMPLETE AWS::S3::BucketPolicy PipelineArtifactsBucke -UPDATE_IN_PROGRESS AWS::CodePipeline::Pip Pipeline -UPDATE_COMPLETE AWS::CodePipeline::Pip Pipeline -UPDATE_COMPLETE_CLEANU AWS::CloudFormation::S sam-app-pipeline -DELETE_IN_PROGRESS AWS::CodeBuild::Projec CodeBuildProjectUnitTe -UPDATE_COMPLETE AWS::CloudFormation::S sam-app-pipeline -DELETE_COMPLETE AWS::CodeBuild::Projec CodeBuildProjectUnitTe --------------------------------------------------------------------------------------------------CloudFormation outputs from deployed stack-------------------------------------------------------------------------------------------------Outputs-------------------------------------------------------------------------------------------------Key CodeStarConnectionArnDescription The Arn of AWS CodeStar Connection used to connect to external coderepositories.Value arn:aws:codestar-connections:eu-west-1:xxxxxxxxxxxx:connection/cc4ce462-f77f-43ab-b63c-8012ef6a467e-------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app-pipeline in eu-west-1And the pipeline will be updated:Summary of the current stateAfter all the changes that we made: Now, we have 3 steps in our CodePipeline: Source: integrated with GitHub as we indicated before BuildAndPackage: the SAM application is built, packaged, and uploaded (with AWS CodeBuild service) DeployTest: the SAM application is deployed (with AWS CodeBuild service) To summarize the current state: If we change the SAM application and update the code in our GitHub repository, the pipeline will update the resources in our AWS account. If we want to update the pipeline itself, we have to update the specific pipeline files in our SAM application, and then run manually the command sam deploy -t codepipeline.yaml --stack-name sam-app-pipeline --capabilities=CAPABILITY_IAM. Clean upWe have created 4 stacks in CloudFormation related to SAM: sam-app: application code sam-app-pipeline: codepipeline aws-sam-cli-managed-test-pipeline-resources: test stage resources aws-sam-cli-managed-default: general SAM resources If you want to use SAM in the future you could keep the aws-sam-cli-managed-default stack.You have several ways to delete your resources: AWS CloudFormation service AWS CLI WS SAM CLI If you execute the command sam delete, it only will delete the main stack (sam-app) but not the CI/CD pipeline or the stage resources stack." }, { "title": "How to create serverless applications with SAM", "url": "/posts/how-to-create-serverless-applications-with-sam/", "categories": "Serverless", "tags": "how-to, sam, iac, serverless, github", "date": "2022-04-09 15:07:00 +0200", "snippet": "IntroductionThe AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.A serverless application is more than just a Lambda F...", "content": "IntroductionThe AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS.A serverless application is more than just a Lambda Function. It is a combination of Lambda functions, event sources, APIs, databases, and other resources that work together to perform tasks.SAM is an extension of AWS CloudFormation but SAM is streamlined and specifically designed for Serverless resources. SAM is the specific IaC solution of AWS for defining and deploying Lambda applications without the need to ever touch the AWS Console.Benefits of SAM Local Testing and Debugging With the aws sam cli you can execute and test your serverless applications on your local (by mounting a docker image and running the code) Extension of AWS Cloud Formation You get reliability on the deployment capabilities You are also able to use in the SAM YAML template all of the resources that are available in CloudFormation Single Deployment Configuration You can easily manage all your necessary resources in one single place that belongs to the same stack Built-in best practices You can define and deploy your infrastructure as config you can enforce code reviews you can enable safe deployments through CodeDeploy you can enable tracing by using AWS X-Ray Deep integration with development tools AWS Serverless Application Repository: discover new applications AWS Cloud9 IDE: For authoring, testing, and debugging CodeBuild, CodeDeploy, and CodePipeline: To build a deployment pipeline AWS CodeStar: To get started with a project structure, code repository, and a CI/CD pipeline that’s automatically configured for you BasicsTo understand the code structure of the SAM projects, five files are particularly important: src/handlers/file.js: src folder contains the code for the application’s Lambda Function. __tests__/unit/handlers/file.test.js: test folder contains the unit tests for the application code. events/file.json: events folder contains the invocation events that you can use to invoke the function. template.yaml: This file contains the AWS SAM template that defines your application’s AWS resources. package.json: This file of NodeJS contains the application dependencies and is used for the sam build. If you are using Python language instead of NodeJS, the file will be requirements.txt. The location of Lambda Functions or Unit Tests could change! In some examples, a folder with the name of the lambda is created and inside is located the lambda function and the unit tests.Example of Lambda FunctionThis is the template.yaml content for a Lambda Function:# The AWSTemplateFormatVersion identifies the capabilities of the template# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/format-version-structure.htmlAWSTemplateFormatVersion: 2010-09-09Description: &gt;- sam-app# Transform section specifies one or more macros that AWS CloudFormation uses to process your template# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-section-structure.htmlTransform:- AWS::Serverless-2016-10-31# Resources declares the AWS resources that you want to include in the stack# https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.htmlResources: # Each Lambda function is defined by properties: # https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction # This is a Lambda function config associated with the source code: hello-from-lambda.js helloFromLambdaFunction: Type: AWS::Serverless::Function Properties: Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler Runtime: nodejs14.x Architectures: - x86_64 MemorySize: 128 Timeout: 100 Description: A Lambda function that returns a static string. Policies: # Give Lambda basic execution Permission to the helloFromLambda - AWSLambdaBasicExecutionRoleAdd an API GatewayTo add an API (Amazon API Gateway) to your Lambda Function you will have to update inside the properties by adding Events as follows:Resources: HelloWorldFunction: Properties: ... Events: HelloWorld: Type: Api Properties: Path: / Method: getAnd you have to create in events folder the json definition of the method:{ \"httpMethod\": \"GET\"}Add an scheduled eventInclude a scheduled event is pretty similar to add an API.Resources: HelloWorldFunction: Properties: ... Events: CloudWatchEvent: Type: Schedule Properties: Schedule: cron(0 * * * ? *)And you have to create in events folder the json definition of the rule:{ \"id\": \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\", \"detail-type\": \"Scheduled Event\", \"source\": \"aws.events\", \"account\": \"\", \"time\": \"1970-01-01T00:00:00Z\", \"region\": \"us-west-2\", \"resources\": [ \"arn:aws:events:us-west-2:xxxxxxxxxxxx:rule/ExampleRule\" ], \"detail\": {}}More examples I recommend that you run aws sam init and ty to create different projects from the templates.More information about template anatomy herePrerequisites AWS CLI how to install it how configure it AWS SAM CLI (here)SAM applicationTo keep it simple, we will create a SAM application from a quick start template using the standalone function but you could try a different template if you wish. The code example is available here. If you want to see the step by step you can check the commit history, where you can find the evolution of the application through the steps explained in the following lines.These are all the steps that I want to show you in this article: Step 1: Download a sample SAM application Step 2 (Optional): Test your application locally Step 3 (Optional): Unit test Step 4: Build your application Step 5: Deploy manually your application with the CLI Step 6 (Optional): AWS SAM Accelerate (Preview) - SyncStep 1: Download a sample SAM applicationThe first step is to create our application through a quick start template: Standalone function.To create a new application from a template we run the sam init command.&gt; sam initWhich template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template LocationChoice: &gt; 1Choose an AWS Quick Start application template 1 - Hello World Example 2 - Multi-step workflow 3 - Serverless API 4 - Scheduled task 5 - Standalone function 6 - Data processing 7 - Infrastructure event management 8 - Machine LearningTemplate: &gt; 5Which runtime would you like to use? 1 - dotnetcore3.1 2 - nodejs14.x 3 - nodejs12.xRuntime: &gt; 2Based on your selections, the only Package type available is Zip.We will proceed to selecting the Package type as Zip.Based on your selections, the only dependency manager available is npm.We will proceed copying the template using npm.Project name [sam-app]: &gt; sam-appCloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) ----------------------- Generating application: ----------------------- Name: sam-app Runtime: nodejs14.x Architectures: x86_64 Dependency Manager: npm Application Template: quick-start-from-scratch Output Directory: . Next steps can be found in the README file at ./sam-app/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-app &amp;&amp; sam pipeline init --bootstrap [*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch Note that at the end of the command line messages, Commands you can use next appears where other SAM CLI commands are suggested as the next steps to execute.This is the basic application that has been created (only with one lambda function for easy understanding):Note that we have 4 of the 5 files that we reviewed before: src/handlers/hello-from-lambda.js _test_/unit/handlers/hello-from-lambda.test.js template.yaml package.jsonWe don’t have the folder events because we only create one simple Lambda Function with no event integrationsStep 2 (Optional): Test your application locallyThe AWS SAM CLI provides the sam local command to run your application using Docker containers that simulate the execution environment of Lambda.Invoke your Lambda function running sam local invoke:&gt; sam local invokeInvoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.43.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app as /var/task:ro,delegated inside runtime containerSTART RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305ef Version: $LATEST2022-04-08T18:55:39.230Z c8b494ab-a1db-48b7-8f50-f9a93e1305ef INFO Hello from Lambda!END RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305efREPORT RequestId: c8b494ab-a1db-48b7-8f50-f9a93e1305ef Init Duration: 1.55 ms Duration: 363.29 ms Billed Duration: 364 ms Memory Size: 128 MB Max Memory Used: 128 MB\"Hello from Lambda!\"We received the response \"Hello from Lambda!\" and more useful information (Duration, Billed Duration, Memory Size, or Max Memory Used).If you have more than one Lambda Function, you must add the name which appears in the template.yaml file.&gt; sam local invoke \"helloFromLambdaFunction\"Invoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs14.x)Skip pulling image and use local one: public.ecr.aws/sam/emulation-nodejs14.x:rapid-1.43.0-x86_64.Mounting /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app as /var/task:ro,delegated inside runtime containerSTART RequestId: dc41d97d-0867-4516-94d9-d1830192565e Version: $LATEST2022-04-08T18:56:44.834Z dc41d97d-0867-4516-94d9-d1830192565e INFO Hello from Lambda!\"Hello from Lambda!\"END RequestId: dc41d97d-0867-4516-94d9-d1830192565eREPORT RequestId: dc41d97d-0867-4516-94d9-d1830192565e Init Duration: 0.49 ms Duration: 224.97 ms Billed Duration: 225 ms Memory Size: 128 MB Max Memory Used: 128 MB You also can test an API locally if your SAM project includes it. You should run sam local start-api command, which starts up a local endpoint that replicates your REST API endpoint.Step 3 (Optional): Unit testTests are defined in the __tests__ folder in this project. Use npm to install the Jest test framework and run unit tests.&gt; npm install...&gt; npm run testreplaced-by-user-input@0.0.1 testjestjest-haste-map: Haste module naming collision: replaced-by-user-input The following files share their name; please adjust your hasteImpl: * &lt;rootDir&gt;/package.json * &lt;rootDir&gt;/.aws-sam/build/helloFromLambdaFunction/package.json PASS __tests__/unit/handlers/hello-from-lambda.test.js ● Console console.info Hello from Lambda! at Object.helloFromLambdaHandler (src/handlers/hello-from-lambda.js:9:13) PASS .aws-sam/build/helloFromLambdaFunction/__tests__/unit/handlers/hello-from-lambda.test.js ● Console console.info Hello from Lambda! at Object.helloFromLambdaHandler (.aws-sam/build/helloFromLambdaFunction/src/handlers/hello-from-lambda.js:9:13)Test Suites: 2 passed, 2 totalTests: 2 passed, 2 totalSnapshots: 0 totalTime: 1.854 sRan all test suites.Step 4: Build your applicationThe sam build command builds any dependencies that your application has, and copies your application source code to folders under .aws-sam/build to be zipped and uploaded to Lambda.&gt; sam buildBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpBuild SucceededBuilt Artifacts : .aws-sam/buildBuilt Template : .aws-sam/build/template.yamlCommands you can use next=========================[*] Invoke Function: sam local invoke[*] Test Function in the Cloud: sam sync --stack-name {stack-name} --watch[*] Deploy: sam deploy --guidedThese are the new files of our SAM project:Step 5: Deploy manually your application with the CLINow we want to deploy our application. We will do it now manually from CLI, although in this other article I will do it with a pipeline (automatically). Remember that AWS SAM uses AWS CloudFormation as the underlying deployment mechanism.As we don’t have a configuration file containing all the values, we are going to create one. We run the sam deploy -- guided command which will search as a first step if a samconfig.toml file exists and if not the AWS SAM CLI will ask us about the necessary information to deploy our application. The sam deploy command will package and upload the application artifacts to the S3 bucket, and deploys the application using AWS CloudFormation&gt; sam deploy --guidedConfiguring SAM deploy====================== Looking for config file [samconfig.toml] : Not found Setting default arguments for 'sam deploy' ========================================= Stack Name [sam-app]: AWS Region [eu-west-1]: #Shows you resources changes to be deployed and require a 'Y' to initiate deploy Confirm changes before deploy [y/N]: &gt; Y #SAM needs permission to be able to create roles to connect to the resources in your template Allow SAM CLI IAM role creation [Y/n]: &gt; #Preserves the state of previously provisioned resources when an operation fails Disable rollback [y/N]: &gt; Save arguments to configuration file [Y/n]: &gt; SAM configuration file [samconfig.toml]: &gt; SAM configuration environment [default]: &gt; Looking for resources needed for deployment: Creating the required resources... Successfully created! Managed S3 bucket: aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 A different default S3 bucket can be set in samconfig.toml Saved arguments to config file Running 'sam deploy' for future deployments will use the parameters saved above. The above parameters can be changed by modifying samconfig.toml Learn more about samconfig.toml syntax at https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-config.htmlUploading to sam-app/c606de95995c9e6d65f310f130ccc787 5777 / 5777 (100.00%) Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================Uploading to sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template 659 / 659 (100.00%)Waiting for changeset to be created..CloudFormation stack changeset-----------------------------------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-----------------------------------------------------------------------------------------------------------------------------------------------------+ Add helloFromLambdaFunctionRole AWS::IAM::Role N/A+ Add helloFromLambdaFunction AWS::Lambda::Function N/A-----------------------------------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649445168/3355bdea-6549-4a2b-b486-94befe703b4dPreviewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-08 21:13:08 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------CREATE_IN_PROGRESS AWS::IAM::Role helloFromLambdaFunctionRole -CREATE_IN_PROGRESS AWS::IAM::Role helloFromLambdaFunctionRole Resource creation InitiatedCREATE_COMPLETE AWS::IAM::Role helloFromLambdaFunctionRole -CREATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -CREATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction Resource creation InitiatedCREATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -CREATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app in eu-west-1Remember that the executed command will create the samconfig.toml file in our project to save the deployment configuration and be able to repeat it without configuration.From now on, to deploy our SAM project we just need to run the sam deploy command, so we run it but if we have no changes, the deployment will fail:&gt; sam deployFile with same data already exists at sam-app/c606de95995c9e6d65f310f130ccc787, skipping upload Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template, skipping uploadWaiting for changeset to be created..Error: No changes to deploy. Stack sam-app is up to dateStep 6 (Optional): AWS SAM Accelerate (Preview) - SyncWe already have deployed our application in the cloud and you may want to synchronize the changes, i.e. deploy the changes in real-time when we save the changes (without running the deploy command). The sync command deploys your local changes to the AWS Cloud. Use sync to build, package, and deploy changes to your development environment as you iterate on your application. As a best practice, run sam sync after you finish iterating on your application to sync changes to your AWS CloudFormation stack. Be careful if you use this functionality. First it is in preview and also as you will see in the next lines in the console: “The SAM CLI will use the AWS Lambda, Amazon API Gateway, and AWS StepFunctions APIs to upload your code without performing a CloudFormation deployment. This will cause drift in your CloudFormation stack.” The sync command should only be used against a development stack.&gt; sam sync --stack-name sam-app --watchManaged S3 bucket: aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0Default capabilities applied: ('CAPABILITY_NAMED_IAM', 'CAPABILITY_AUTO_EXPAND')To override with customized capabilities, use --capabilities flag or set it in samconfig.tomlThis feature is currently in beta. Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/.The SAM CLI will use the AWS Lambda, Amazon API Gateway, and AWS StepFunctions APIs to upload your code withoutperforming a CloudFormation deployment. This will cause drift in your CloudFormation stack.**The sync command should only be used against a development stack**.Confirm that you are synchronizing a development stack and want to turn on beta features.Enter Y to proceed with the command, or enter N to cancel: [y/N]: &gt; YExperimental features are enabled for this session.Visit the docs page to learn more about the AWS Beta terms https://aws.amazon.com/service-terms/.Queued infra sync. Wating for in progress code syncs to complete...Starting infra sync.Manifest file is changed (new hash: c448eb733590e1cea85b58f147c47f01) or dependency folder (.aws-sam/deps/c14e7b11-e157-4aea-a19b-97b33b39cef5) is missing for c14e7b11-e157-4aea-a19b-97b33b39cef5, downloading dependencies and copying/building sourceBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:NpmInstallRunning NodejsNpmBuilder:CleanUpClean up action: .aws-sam/deps/c14e7b11-e157-4aea-a19b-97b33b39cef5 does not exist and will be skipped.Running NodejsNpmBuilder:MoveDependenciesRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpRunning NodejsNpmBuilder:LockfileCleanUpBuild SucceededSuccessfully packaged artifacts and wrote output template to file /var/folders/wq/bz6xngtx5h3f5gf8py3kf28c0000gn/T/tmp6076iix4.Execute the following command to deploy the packaged templatesam deploy --template-file /var/folders/wq/bz6xngtx5h3f5gf8py3kf28c0000gn/T/tmp6076iix4 --stack-name &lt;YOUR STACK NAME&gt; Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_NAMED_IAM\", \"CAPABILITY_AUTO_EXPAND\"] Parameter overrides : {} Signing Profiles : nullInitiating deployment=====================2022-04-08 21:34:02 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------UPDATE_IN_PROGRESS AWS::CloudFormation::Stack sam-app Transformation succeededCREATE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -CREATE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac Resource creation InitiatedCREATE_COMPLETE AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -UPDATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE_CLEANUP_IN_PROGRESS AWS::CloudFormation::Stack sam-app -UPDATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Stack update succeeded. Sync infra completed.{'StackId': 'arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:stack/sam-app/e17d9780-b76f-11ec-a123-02591afab591', 'ResponseMetadata': {'RequestId': 'c726d9ed-ba77-473c-9155-9e05cc7ee0c7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c726d9ed-ba77-473c-9155-9e05cc7ee0c7', 'content-type': 'text/xml', 'content-length': '377', 'date': 'Fri, 08 Apr 2022 19:34:02 GMT'}, 'RetryAttempts': 0}}Infra sync completed.The console still listens for changes and if we change our lambda code and save it:The console will be updated automatically as follow:Syncing Lambda Function helloFromLambdaFunction...Manifest is not changed for c14e7b11-e157-4aea-a19b-97b33b39cef5, running incremental buildBuilding codeuri: /Users/alazaroc/Documents/MyProjects/github/aws/sam/sam-app runtime: nodejs14.x metadata: {} architecture: x86_64 functions: ['helloFromLambdaFunction']download_dependencies is False and dependencies_dir is None. Copying the source files into the artifacts directory.Running NodejsNpmBuilder:NpmPackRunning NodejsNpmBuilder:CopyNpmrcAndLockfileRunning NodejsNpmBuilder:CopySourceRunning NodejsNpmBuilder:CleanUpNpmrcRunning NodejsNpmBuilder:LockfileCleanUpRunning NodejsNpmBuilder:LockfileCleanUpFinished syncing Lambda Function helloFromLambdaFunction.When you stop it (control + C) in the console it will appear:Shutting down sync watch...Sync watch stopped. When you executed the sync command, a nested stack associated with your main stack (sam-app) was created: And when the console stops being synchronized, this nested stack is NOT deleted.How to remove the nested stack created with the sync command?You have to run the sam deploy command again:&gt; sam deployFile with same data already exists at sam-app/c606de95995c9e6d65f310f130ccc787, skipping upload Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Disable rollback : False Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-bymcog0ibyy0 Capabilities : [\"CAPABILITY_IAM\"] Parameter overrides : {} Signing Profiles : {}Initiating deployment=====================File with same data already exists at sam-app/c3a5d8cf01d6af391d2863628b67fbbe.template, skipping uploadWaiting for changeset to be created..CloudFormation stack changeset-----------------------------------------------------------------------------------------------------------------------------------------------------Operation LogicalResourceId ResourceType Replacement-----------------------------------------------------------------------------------------------------------------------------------------------------* Modify helloFromLambdaFunction AWS::Lambda::Function False- Delete AwsSamAutoDependencyLayerNestedStac AWS::CloudFormation::Stack N/A-----------------------------------------------------------------------------------------------------------------------------------------------------Changeset created successfully. arn:aws:cloudformation:eu-west-1:xxxxxxxxxxxx:changeSet/samcli-deploy1649447607/790db35a-524f-48c5-af5c-1239e1b8fe92Previewing CloudFormation changeset before deployment======================================================Deploy this changeset? [y/N]: &gt; Y2022-04-08 21:53:42 - Waiting for stack create/update to completeCloudFormation events from stack operations-----------------------------------------------------------------------------------------------------------------------------------------------------ResourceStatus ResourceType LogicalResourceId ResourceStatusReason-----------------------------------------------------------------------------------------------------------------------------------------------------UPDATE_IN_PROGRESS AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE AWS::Lambda::Function helloFromLambdaFunction -UPDATE_COMPLETE_CLEANUP_IN_PROGRESS AWS::CloudFormation::Stack sam-app -DELETE_IN_PROGRESS AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -DELETE_COMPLETE AWS::CloudFormation::Stack AwsSamAutoDependencyLayerNestedStac -UPDATE_COMPLETE AWS::CloudFormation::Stack sam-app ------------------------------------------------------------------------------------------------------------------------------------------------------Successfully created/updated stack - sam-app in eu-west-1Step 7: Clean upWe only have one stack in our AWS Account.To delete it, you can run the sam delete command which deletes the main stack (sam-app).Next steps If you need more information about SAM I recommend you to visit the AWS documentation here Next post: How to add CI/CD to my SAM project Comment this post" }, { "title": "How to add CI/CD to my CDK project", "url": "/posts/how-to-add-ci-cd-to-my-cdk-project/", "categories": "DevOps", "tags": "how-to, iac, cdk, cicd, codepipeline, codebuild", "date": "2022-03-26 02:03:00 +0100", "snippet": "TLDRI already have a CDK project on GitHub here, but to deploy it I have to run the CDK Toolkit command cdk deploy from my local machine.I want to add automation to my deployment process and integr...", "content": "TLDRI already have a CDK project on GitHub here, but to deploy it I have to run the CDK Toolkit command cdk deploy from my local machine.I want to add automation to my deployment process and integrate it with the AWS ecosystem… so I will use the AWS Developer tools to do it.I will show you 2 different approaches: Create the pipeline with the AWS Console: Why? Helps you understand the low level of the solution and how AWS services work Create the pipeline with IaC (CDK): Why? You should always try to automate everything. In this case, I will create the pipeline that will allow us to deploy the CDK code automatically.I want to implement the simplest solution, with the KISS principle in mind, and for this reason, my architecture diagram is as follows:Explanation: In a CDK deployment, I don’t need to run the cdk synth command and manage the generated artifacts, so the simplest solution is to run the cdk deploy command directly. If you need more information about it, I wrote a related post: How to create infrastructure with CDK.However, upon investigation the AWS recommendation to deploy a CI/CD pipeline of CDK projects is something similar to the following:I will explain it in detail in this post. I have preferred to include all of information in the same post, although I could easily split it in 2 or 3, and this post have a lot of content and images.IntroductionBefore I start showing you how to add the CI/CD I will introduce you to the basic concepts involved:SDLC (Software Development Lifecycle) is a process for planning, creating, testing, and deploying an information system - WikipediaDepending on where you look, there will be a different number of phases in the SDLC process.For this article we will explain what means CI/CD over 4 phases of the software release process: source, build, test and production (deployment):CI/CD refers to Continuous Integration and Continuous Delivery and it introduces automation and monitoring to the complete SDLC.Continuous integration (CI) is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run The key goals of CI are to find and address bugs more quickly, improve software quality, and reduce the time it takes to validate and release new software updates Continuous integration focuses on smaller commits and smaller code changes to integrateContinuous delivery (CD) is a software development practice where code changes are automatically built, tested, and prepared for production release Benefits Automate the software release process Improve developer productivity Improve code quality Deliver updates faster The point of continuous delivery is not to apply every change to production immediately but to ensure that every change is ready to go to productionContinuous deployment (CD), revisions are deployed to a production environment automatically without explicit approval from a developer, making the entire software release process automated. So yes, the “CD” in “CI/CD” means 2 different things: Continuos Delivery (prepare deployment to prod) and Continuos Deployment (deploy automatically in prod) CodePipeline for CDK with AWS ConsoleFirst, we will create the solution with the AWS Console because it helps to understand how the services involved work.As we want to create a new Pipeline, we must access to CodePipeline service and click on Create pipeline.Step 1 in CodePipeline is to choose the pipeline settings. We have to create a new service role (or use an existing one).Step 2 is to add the source of the stage choosing the source provider. I have my code repository on GitHub so I choose GitHub (version 2) but you could choose a different one. There are 2 options for GitHub source provider: version 1 (not recommended) which uses OAuth apps to access your GitHub repository version 2 (recommended) which uses a connection with GitHub Apps to access your repository If you choose GitHub version 2, the next step is to create a new connection to GitHub and if you don’t have any GitHub App created you need to create a new one, so you should click on Install new app. You have to choose whether to create the connection for all repositories or only th selected ones, and click Install. The GitHub connection is ready to use and you will be redirected to Step 2 of the creation of the CodePipeline.Now you can choose your repository and your branch and click on Next.Step 3 is to add the build stage, and you should select AWS CodeBuild because we want to use this service to add custom commands.After that, select the region, a project name, and a single build and click Next. With this specific configuration, it will fail, do you know why?Step 4 is to add the deploy stage. Here are all the available options but we skip this step because we don’t need it.Now the CodePipeline is ready to be created and a review page is displayed. Confirm and create the CodePipeline.It is done. We have created the CodePipeline and added 2 stages: Source BuildFirst execution…As you can see the execution had failed!Do you know what caused the error? Let’s investigate it…If you click on the execution ID link, you will be redirected to the pipeline execution summary and you will see the error message Project cannot be found in CodeBuild.Also, you can click on the AWS CodeBuild action name and you will be redirected to the CodeBuild service… where you will receive the same error information: “Resource not available”. Yes, there is no CodeBuild project created in the pipeline, we just add a name of a created CodeBuild resource (and this resource doesn’t exist because nobody has created it).To fix it, you need to edit the Pipeline and edit the Build stage to create a new CodeBuild project.A new window will be opened, the Build stage will be editable and you need to click on the Create project button.You can create the build project by choosing the following: Operating System: Amazon Linux 2 Runtime(s): Standard Image: the more updated image New role name Buildspec: Insert build commands and click to Switch to editor and add the following: version: 0.2phases: install: commands: - npm install - npm install -g typescript - npm install -g aws-cdk build: commands: - npm ci - npm run build - cdk deploy Add a CloudWatch log, choosing as Group name /aws/codebuild/blog-backend-infrastructureWhen you have finished filling in all fields, click Continue to CodePipeline. Again, with this configuration the execution will fail. Do you know why?You can now go back to the CodePipeline and force the execution again by clicking on Release change.And, as expected, it fails again.This time we will review the CloudWatch logs generated for this run to look for errors.You can see that the CodeBuild role is trying to assume the CDK role to perform the CDK commands, and of course, we didn’t specify any permissions to the new role so it can’t assume any roles. How CDK deploy works: Behind the scenes, when the cdk deploy command is executed, CDK is using the CDK roles created in the bootstrap process to perform some actions: perform a lookup, upload files and deploy the template uploaded in the S3 into the CloudFormation service.Therefore, you need to update the CodeBuild role to add the assumed permission to CDK roles. To do this, create new permission (new inline policy).You must add the Action sts:AssumeRole and the Resources of the 4 CDK roles created in the bootstrap.When is created, you can review that the new permission has been added to the CodeBuild role.If you come back to the CodePipeline service and you execute it again, it will succeed! Now, if you make any changes in your repository, the pipeline will be automatically executed and your infrastructure will be updated executing the cdk deploy command of the CDK Toolkit inside of the CodeBuild service.If you want, you can check the logs in the CloudWatch service to verify that the execution of the cdk deploy command went as we expected:Improve: Use a Buildspec file inside the codeYou have done a lot of manual work, and the first improvement you can automate is the definition of the build process itself.You need to update, in the CodeBuild project, the buildspec configuration of the project, select Use a buildspec file and click to Update buildspec.Now when the pipeline runs it will look for the buildspec file inside the code (in the root folder). You have configured the CodeBuild service but you don’t have the buildspec.yml file added to your code yet.Next, you must add the buildspec.yml file with the same content you provided in the online editor to update the build commands in the code. More information about buildspec fileIn the following image, you can see the VSCode IDE and the new buildspec.yml file with the same content as before.Test it: automatic execution of the pipeline when a commit is doneIf you commit the new file to your repository (buildspec.yml)The pipeline runs automatically as expectedCodePipeline for CDK with IaCNow that we have deployed the CodePipeline with the AWS Console, we will do the same with Infrastructure as Code with CDK.To do this, I will add a CodePipeline resource to my CDK project for the blog. I am using the GitHub v2 connection because it is the recommended way, and it requires first to use the AWS Console to authenticate to the source control provider, and then use the connection ARN in your pipeline definition. In other words, if you use GitHub v2 you need create the connection manually!Selfmutation property I want to show you first how “selfmutation” works in CDK pipelines because this it is important to know.This is the code to add the CodePipeline resource with 2 stages: Source with GitHub v2 (with a connection) Build phase (cdk synth) CDK pipelines will generate CodeBuild projects for each ShellStep you useconst codePipelineName = `blog-backend-infrastructure-cdk`;const pipeline = new CodePipeline(this, codePipelineName, { pipelineName: codePipelineName, synth: new ShellStep('Synth', { // input: CodePipelineSource.gitHub('alazaroc/aws-cdk-pipeline', 'main'), input: CodePipelineSource.connection( 'alazaroc/blog-backend-infrastructure', 'main', { connectionArn: 'arn:aws:codestar-connections:eu-west-1:xxxxxxxx:connection/fb936fb8-a047-43d5-90bd-xxxxxxxxxx', // Created using the AWS console * });', }, ), commands: ['npm ci', 'npm run build', 'npx cdk synth'], }),}); You must deploy the pipeline manually once. After that, the pipeline will be kept up to date from the source code repository.If you run the cdk deploy command, as you can see the pipeline is created and automatically runs. But wait a minute, we have three stages? A new one SelfMutate appears. Well, let’s wait to finish… PipelineNotFoundException? What happened here? We waited for the pipeline to finish executing and the pipeline no longer exists!Let’s do some research on SelfMutate. The CDK documentation says: Whether the pipeline will update itself This needs to be set to true to allow the pipeline to reconfigure itself when assets or stages are being added to it, and true is the recommended setting. You can temporarily set this to false while you are iterating on the pipeline itself and prefer to deploy changes using cdk deploy.We haven’t added this property to our code and the default value applied is true, so the pipeline has updated itself and, as the code is NOT committed in the source code, the pipeline has been removed.We have previously executed the deploy command but not a previous commit. I didn’t commit my CodePipeline code to make the result “more dramatic” (pipeline deleted automatically). But if you commit the code, nothing will happen. You will have one more stage to allow the pipeline to autoconfigure if there any changes, and at each run this will be checked.I want to show you what will happen if you set the selfmutate property to false (and the code is not committed).This is the change needed in the CDK CodePipeline resource code, adding this line:selfMutation: false,And if you run the cdk deploy command again, the pipeline will be updated:Now, there are only 2 stages in the CodePipeline, the Source and the Build, the 2 that we have configured and work perfectly.CDK DeployWe will change the code of our CodePipeline service so that instead of executing a cdk synth command, it will execute the cdk deploy command.Also, to avoid the assumed role error we saw in the AWS Console example (in this same post), we will add the IAM permissions necessary to the CodeDeploy role to run the deploy command. To customize the CodeBuild project, change ShellStep by CodeBuildStep. This class has more properties to customize it:const codePipelineName = `blog-backend-infrastructure-cdk`; const pipeline = new CodePipeline(scope, codePipelineName, { pipelineName: codePipelineName, // synth: new ShellStep('Deploy', { synth: new CodeBuildStep('Deploy', { input: CodePipelineSource.connection( 'alazaroc/aws-cdk-pipeline', 'main', { connectionArn: 'arn:aws:codestar-connections:eu-west-1:xxxxxx:connection/4d6c1902-bda7-43fb-8508-xxxxxx', }, ), commands: ['npm ci', 'npm run build', 'npx cdk deploy --require-approval never'], rolePolicyStatements: [ new aws_iam.PolicyStatement({ actions: ['sts:AssumeRole'], resources: ['*'], conditions: { StringEquals: { 'iam:ResourceTag/aws-cdk:bootstrap-role': [ 'lookup', 'image-publishing', 'file-publishing', 'deploy', ], }, }, }), ], }), selfMutation: false, });When you deploy it will create the CodePipeline project and execute the 2 steps defined: Source Build (cdk deploy) We changed the cdk synth by cdk deploy and also added the necessary permissions.And since we have added the appropriate permissions, it doesn’t fail.Recommended deployment of CodePipeline to CDK projects This approach is a little different.I will use this other example of CodePipeline for CDK, to make it easier to understand. Also, I will go step by step to understand perfectly how it works.This is the final diagram of what we will build (with the CDK code): If you want to create the Pipeline of the CDK project you will need to include at least two stacks: one for the pipeline and one or more for the infrastructure that will be deployed with the pipeline.Application deployment begins by defining MyPipelineAppStage, a subclass of Stage that contains the stacks that make up a single copy of my stack (MyIaCStack).export class MyPipelineAppStage extends Stage { constructor(scope: Construct, id: string, props?: StageProps) { super(scope, id, props); const iaCStack = new MyIaCStack(this, 'iac-example-stack', { description: 'Stack created with codepipeline in the example aws-cdk-pipeline', }); }}Now, we define MyIaCStack, which contains all the AWS resources that will be created in a different stack than Pipeline:export class MyIaCStack extends Stack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props); // Add resources new aws_s3.Bucket(this, 'MyFirstBucket', { enforceSSL: false, }); }}Finally, we create the main stack, MyPipelineStack, which will add MyPipelineAppStage as a stage within the CodePipeline resource.export class MyPipelineStack extends Stack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props); const codePipelineName = `test-iac-with-cdk`; const pipeline = new CodePipeline(this, codePipelineName, { pipelineName: codePipelineName, synth: new ShellStep('Synth', { // input: CodePipelineSource.gitHub('alazaroc/aws-cdk-pipeline', 'main'), input: CodePipelineSource.connection( 'alazaroc/aws-cdk-pipeline', 'main', { connectionArn: getMyGitHubConnectionFromSsmParameterStore(this), // Created using the AWS console * });', }, ), commands: ['npm ci', 'npm run build', 'npx cdk synth'], }), }); pipeline.addStage( new MyPipelineAppStage(this, 'Deploy', { // env: { account: \"111111111111\", region: \"eu-west-1\" } }), ); }}We have to commit all the above changes, and after that deploy it.It will create: the main stack with the CodePipeline, MyPipelineStack, the Deploy-iacStackFirst, the pipeline stack is created, and then, when the CodePipeline is executed, the second stack which contains all the other resources is created.That is all, we have automation in our deployment process!So as you can see, using CDK’s CodePipeline constructor, the following is created:" }, { "title": "How to create infrastructure with CDK", "url": "/posts/how-to-create-infrastructure-with-cdk/", "categories": "IaC", "tags": "how-to, iac, cdk, cloudformation, github", "date": "2022-03-16 19:28:00 +0100", "snippet": "TLDRI will explain the basics of CDK in practice: how CDK works and how to deploy a CDK project from scratch.In addition, I will share the source code of my CDK project used to create the infrastru...", "content": "TLDRI will explain the basics of CDK in practice: how CDK works and how to deploy a CDK project from scratch.In addition, I will share the source code of my CDK project used to create the infrastructure of my blog.IntroductionThis section contains: What is CDK How CDK works PrerequisitesWhat is CDKCDK is an open-source software development framework to define your cloud application resources using familiar programming languages (TypeScript, JavaScript, Python, Java, C#/.Net, and Go)AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. To me, with a developer background, CloudFormation is complex and CDK fills the gap because it allows me to use a programming language to create the infrastructure easily, it’s wonderful.How CDK worksTo interact with CDK apps you will need the AWS CDK Toolkit (command-line tool).In a nutshell: Add code: Add the desired AWS resources in the app code with your preferred programming language. Transform the code into a CloudFormation template: Run the cdk synth command from the AWS CDK Toolkit to generate the CloudFormation template from the app code. Deploy the infrastructure: Run the cdk deploy command from the AWS CDK Toolkit to create a new stack on the CloudFormation service, which will deploy the AWS resources to the configured AWS account.AWS CDK Toolkit commands you need to know: Command: Function cdk init: Creates a new CDK project in the current directory from a specified template cdk bootstrap: Deploys the CDK Toolkit stack. It must be executed once per environment (account and region) to allow cdk to create the resources it needs to run cdk synthesize / cdk synth: Synthesizes and prints the CloudFormation template for the specified stack(s) cdk diff: Compares the specified local stack with the deployed stack cdk deploy: Deploys the specified stack(s) cdk destroy: Destroys the specified stack(s) Prerequisites AWS CLI how to install it how configure it Node.js official website IDE for your programming language VSCode Others AWS CDK Toolkit npm install -g aws-cdk cdk version cdk bootstrap You must execute it once per environment (account and region) to allow CDK to create the resources it needs to run How to create infrastructure with CDKThis section contains: How to create and deploy a basic CDK application Clean up Make changes to the default CDK application and deploy itHow to create and deploy a basic CDK applicationI have chosen TypeScript as my programming language.TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale.The typescript sources need to be compiled into JavaScript.What is the fastest way to create a new CDK project and deploy it in your AWS account?# Create an empty foldermkdir cdk-basic-example &amp;&amp; cd cdk-basic-example# Creates an example of CDK Application with some constructscdk init sample-app --language typescript# Deploy and generate the resources into your AWS accountcdk deploy --require-approval neverAnd that’s all, we have deployed one topic and one queue in our AWS Account… NOTE: What about the generation of the CloudFormation template in the synth phase? We have executed only the deploy command… When you execute cdk deploy behind the scenes also… is executed cdk synth to generate the CloudFormation template (so you could want to avoid execute cdk synth before cdk deploy) and our assets code and the CloudFormation template are deployed to the S3 bucket provisioned when cdk bootstrap was executed Clean upLet’s destroy the stack. I know this section maybe should be at the end, but where’s the fun in that? We are playing and we need to try different things. If you try to do something different and on your own, you will learn faster!# Delete the CloudFormation stack (so it will delete all resources related)cdk destroy --forceYou have to know how to destroy a stack, so remember to do it at the end if you are playing with cdk…Make changes to the default CDK application and deploy itPerhaps we want to check what is to be deployed before we deploy it?It makes sense to me.# Synthesizes the CloudFormation template for the specified stack(s)# In the console, the template will be printed in yaml format# In the \"cdk.out\" folder, the template will be in json formatcdk synthWhat if we compare the local code with the stack deployed in the AWS account?cdk diff NOTE: cdk diff needs to connect to the AWS Account to check the CloudFormation stack against your local resources.We can see all the new resources that will be created: [+] AWS::SQS::Queue [+] AWS::SQS::QueuePolicy [+] AWS::SNS::Subscription [+] AWS::SNS::Topic Remember that we have destroyed the stack in the previous step so all resources are detected as new.We deploy it again:cdk deploy --require-approval neverAnd we run the diff command again to see the differences between the local code and the deployed stack:cdk diffNow let’s update the CDK code to generate some differences. First of all, we have to open the project with our IDE (you can also do it with a notepad but…)This file contains the 2 AWS resources of my example, a queue (red) and a topic (yellow). I could add a new service but for simplicity, I will remove the topic (lines 15 to 17) and run the cdk diff again.cdk diffWe can see that we have deleted the topic in the code, and when we run the diff command CDK finds the changes and shows them to us.And that’s all, keep practicing and learning! Remember to destroy your stack when you are done playingBonus: My CDK blog codeThe source code is available hereIf you review it and think it can be improved, please let me know.Next steps If you need more information about CDK I recommend you to visit the AWS documentation here Next post: How to add CI/CD to my CDK project Comment this post" }, { "title": "How to deploy a web with Amplify Hosting", "url": "/posts/how-to-deploy-a-web-with-amplify-hosting/", "categories": "DevOps", "tags": "how-to, amplify, github, devops", "date": "2022-03-15 21:13:00 +0100", "snippet": " Updated April 2023. I have updated this article to add more information about AWS Amplify.TLDRThis is a practical use case where I will explain step by step how I deployed my blog using Amplify H...", "content": " Updated April 2023. I have updated this article to add more information about AWS Amplify.TLDRThis is a practical use case where I will explain step by step how I deployed my blog using Amplify Hosting. I will use the AWS Console to do it.The source code of my blog (web) is available hereIntroduction AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: Amplify Hosting and Amplify Studio. Amplify Hosting provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. Amplify Studio is a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend, and then connect the two together. AWS Amplify is the fastest and easiest way to develop and deploy reliable and scalable mobile/web applications on AWSHow to deploy a web with AmplifyWe need to have our code ready to be deployed in a supported repository. Supported repositories: GitHub, Bitbucket, GitLab and AWS CodeCommit. Another option is to deploy manually with drag and drop, Amazon S3 or any URL.In AWS Console, enter to AWS Amplify service and choose Amplify Hostinga) If you don’t have an Amplify app, this screen appears and you have to click on Get Startedand then choose Amplify Hostingb) If you already have an Amplify resource click on New app and Host web appConfigure Amplify Hosting:The first step to setting up Amplify Hosting is to connect your repository. In my case, I chose GitHub Add repository branch: choose your repository and your branch Configure build settings: Advanced settings are optional, allow you to reference your build image, add environment variables and override default installed packages Review the configuration and click Save and deploy Now Amplify must be provisioned, built, deployed and verified At this moment an email notification will be sent to your email (at least in the GitHub case) When finished, you will be able to access the new URL generated by Amplify Additional configurationIn the following lines, we will review how to apply additional configuration to add more value to our website.How to associate my web with my domain nameWe can also set up our Domain Name to our deployed website with Amplify Hosting easily.To register the domain name I used Amazon Route53. Use Route53 is not the cheapest option (e.g. I paid $12 to register the new domain with Route53 instead of $1 the first year with GoDaddy), but it’s worth it (to me)In the side menu, click Domain management, and then click Add domain.Choose your Domain and click on Save. As I register my domain with Route53, it appears in the text field when I click on it.Now you can choose the branch, the subdomains and check for automatic redirect from HTTP to HTTPS, and click Save.It may take several minutes to complete. First, you need to create the SSL certificate, then configure the SSL and finally activate it.If we access the main page we can see that the URL has changed.We can now access it with our domain name:That’s it, quick and easy!How to deploy different versions of the applicationConfiguring AWS Amplify Hosting to deploy several versions of the website is very easy, you just need different branches in the repository and configure them in the service as follows:How to preview changes before applying them to productionUsing the preview feature you can preview changes before merging a pull request.To do that you have to: Create a new branch Enable previews and associate with your branch Open a pull request –&gt; A new version of the application will be deployedHow to protect any environmentIf you want to protect any environment to don’t be accessible by everyone, you can use the feature of Access Control. With this option, you can create a simple user/password to restrict access to your web:There are only 2 options for the option Access settings: Publicly available Restricted - password requiredWhen you try to access now to the web you have to log in yo view the content:" }, { "title": "How I decided on the technology behind the blog", "url": "/posts/the-technology-behind-this-blog/", "categories": "General", "tags": "amplify, cdk, github, serverless", "date": "2022-03-02 21:49:00 +0100", "snippet": " Last updated: I have migrated my web from Amplify web to Terraform + S3 + CloudFront + AWS Certificate Manager + Developer ToolsTLDRMy technology approach is: Use AWS resources when possible. On...", "content": " Last updated: I have migrated my web from Amplify web to Terraform + S3 + CloudFront + AWS Certificate Manager + Developer ToolsTLDRMy technology approach is: Use AWS resources when possible. One of the reasons for creating this blog is to practice with AWS… and as a first step, the blog itself must use AWS resources. However, as a good rule, there is an exception GitHub is used as code repository because I want to share my code easily in a public way Serverless architecture I applied the separation of concerns design principle to frontend and backend: Frontend: Static website generated with Jekyll and deployed with AWS Amplify Code here Backend: AWS resources deployed with CDK using the TypeScript language Code here This is the architecture of my blog: Current Version [2]: Previous Version [1]: FrontendI have included the following in this section: Decision 1: Technology to create the blog Decision 2: Technology to deploy the blog Demo: How to deploy itI could add much more details but I want to keep it short.Technology to create the blogFirst of all, I needed to choose how to create the blog, and nowadays there are a lot of options to do it in a serverless way: Single Page Application (SPA): React, Angular, Vue.js, Ionic, Ember Server-Side Rendering (SSR): Express.js, Next.js, Nux.js, Gatsby.js Static Site Generator (SSG): Gatsby, Next.js, Nuxt.js, Hugo, Jekyll, Hexo Progressive Web Apps (PWA): React, Angular, Vue.js, Preact, PWABuilderSince I wanted to keep it simple, I used a static site generator. I must admit I hesitated with Hugo, Jekyll, and Hexo, all three options were good for me and although I liked Hugo for its fast build times and execution performance, I finally decided on Jekyl just because the theme I used (Chirpy) I liked more visually than the other options and I didn’t want to have to customize it too much. I am a big fan of the KISS design principle!Technology to deploy the blogAfter choosing Jekyll as my static site generator, I needed to know how to deploy it on AWS and, of course, there are many options to do it: EC2 + RDS (i.e. traditional blog with WordPress / Ghost + Gatsby / …) LightSail (by the way, an interesting article comparing LightSail with EC2) Container solutions (ECS/EKS) AWS ElasticBeanstalk S3 AWS AmplifyVersion 1But like I want it serverless and simple, in the v1 (until March 5, 2023) I leveraged AWS Amplify to help me with this point. What is AWS Amplify? (Explained by AWS) AWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack applications on AWS. Amplify provides two services: Amplify Hosting and Amplify Studio. Amplify Hosting provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment. Amplify Studio is a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use Studio to build your frontend UI with a set of ready-to-use UI components, create an app backend, and then connect the two together. AWS Amplify is the fastest and easiest way to develop and deploy reliable and scalable mobile/web applications on AWSI used Amplify Hosting for the following reasons (there are more, but the following are important to me): Serverless (uses S3 and CloudFront behind the scenes) Integrates with my existing code in GitHub Supports Jekyll (my static site generator) Manage the CI/CD of my application Easy way to connect my application with my custom domain Instant cache invalidations in new versions Integrated with Amazon CloudWatchAs you can see, this solution is awesome if you want that AWS manage for you the CI/CD, web, cache, the certificate of your domain…Version 2 [Current version]Version 1, using Amplify Hosting, was too “automagic” for me and I was here to practice/play and show you the results… so I migrated my web to a custom solution to have more control and more services to play with, a lot of fun!Creates the infrastructure using Terraform with the following AWS services: S3 bucket used as website CloudFront distribution in front of the S3 bucket Lambda Edge to use it in CloudFront, to transform all requests (required for Jekyll web) AWS ACM: Certificate generation of my custom domain (playingaws.com) Developer Tools: to deploy the Infrastructure as Code with TerraformHow to deploy itVersion 1I wrote it in this post: How to deploy a web with amplify hostingAnd I complemented it with this one: How to add CI/CD to my CDK projectVersion 2 [Current version]Infrastructure as Code created with Terraform and Developer Tools to deploy and automate the IaCBackendI have included the following in this section: Decision 1: What resources should I create? Decision 2: Technology to deploy infrastructure Demo: How to deploy infrastructureWhat resources should I create?No backend is necessary for a blog. Simple blogs that only have content don’t need anything more than static pages. However, if you want more functionality like forms, email subscriptions, or comments you will need to use external plugins (to store the data somewhere else, not on AWS) or create your solutions.I don’t want to use external plugins if I can do it “just the same” myself in AWS and practice/play with new services in the processAfter creating my empty blog I thought that it would be a good idea to implement the following: Forms (contact form) Email Subscription (to receive blog updates) Add comments to each post (register it and show it)Now, I have a basic implementation of these points but I will improve it in the future. The backend could be integrated with the frontend with Amplify Studio, but I am not interested in doing it that way. I want separation of concerns and manage both independently.Forms Used here: Contact form External option easy to integrate: Google Forms. Custom AWS solution: flowchart LR A(Contact form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(SES) D --&gt; E(My email) Email Subscription Used here: Mail subscription External option easy to integrate: Mailchimp Custom AWS solution: flowchart LR A(Email subscription form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(DynamoDB) At this moment, I only store the subscription information and if I want to send emails I have to do it manually. However, in the future, I will automate it and I will add the option to unsubscribe in the email sentComments (updated January 27, 2023) It is used at the end of each post In the first year of my blog, the solution was as follows: flowchart LR A(Email subscription form) --&gt; B(API Gateway) B --&gt; C(Lambda) C --&gt; D(DynamoDB) This solution was a custom AWS solution to use more AWS services and although I received some comments and store them in the database, I didn’t implement the system to display them on the blog. However, now I am using the giscus plugin flowchart LR A(Email subscription form) --&gt; B(GitHub repository) B --&gt; C(GitHub discussion) Technology to deploy infrastructure Now, I am using CDK (Cloud Development Kit) and Terraform.Version 1Initially, in version 1, I used: CDK with TypeScript programming language to deploy backend infrastructure using this GitHub repository: https://github.com/alazaroc/blog-backend-infrastructure. Amplify Hosting to create for me the frontend infrastructure to deploy the Jekyll web located in this GitHub repository: https://github.com/alazaroc/blog-web.Honestly, I did not evaluate other options, since I knew which one to choose and I wanted to be as simple as possible. What is CDK? (Explained by AWS) CDK is an open-source software development framework to define your cloud application resources using familiar programming languages. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation, but also is available (in alpha phase) a CDK for Terraform cdktf and a CDK for Kubernetes cdk8s. To find all of these CDKs in one place, check out Construct Hub, a place to discover and share construct libraries published by the open-source community, AWS, and partners. To me, with a developer background, CloudFormation is complex and CDK fills the gap because it allows me to use a programming language to create the infrastructure easily, it’s wonderful.Version 2 [Current version]However now, I have migrated the frontend infrastructure to Terraform.How to deploy infrastructureI wrote it in these posts: How to create infrastructure with CDK How to deploy a serverless website with TerraformPrice estimation of the blogI just created the AWS Account, so I will use the free tier.Price information by services used: Route53 AWS Amplify API Gateway DynamoDB Lambda I will update it when my free tier expires AWS resource Action Free Tier (per month) Estimation price Route53 Domain Registration $1 for domain $1 Route53 Hosted Zone $0.50 per Hosted Zone for the first 25 $0.50 AWS Amplify Build &amp; Deploy $0 for 1000 build minutes $0 AWS Amplify Hosting 5 GB stored $0 AWS Amplify Hosting $0 for 15 GB served $0 API Gateway API calls for REST API $0 for 1 million $0 DynamoDB On-demand Data Storage on Standard table $0 for 25 GB $0 DynamoDB On-demand Data transfer out to the internet $0 for 100 GB $0 Lambda Requests per month $0 for 1 million $0 Lambda Compute time $0 for 400,000 GB-seconds $0 CDK &amp; CloudFormation Free to use   $0 TOTAL 1.5$ per month. Taxes are NOT included. The domain name purchase on Route53 is annual and is paid in the month of purchase, but for simplicity, I split it into each month. Also, the invoice category is NOT Route53 but “Registrar”, “Global Region”, and “Amazon Registrar DomainRegistration”. Use Route53 for Register Domain is not the cheapest option. I paid $12 instead of around $1 for the first year with GoDaddy, but it’s worth it (to me)Next steps about blog technologyI have many next steps identified, but I’ll put here the ones related to the content of this post. Update comments form –&gt; March 17, 2022 –&gt; A form was available and comments were recorded in a database Show comments in the posts –&gt; January 27, 2023 –&gt; giscus plugin has been integrated into my web Migrate AWS Amplify Web to S3 + CloudFront + AWS Certificate Manager + Developer Tools –&gt; March 5, 2023 - [ ] Automate email subscription" } ]
